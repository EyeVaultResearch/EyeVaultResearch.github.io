<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>EyeVault</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <link rel="stylesheet" href="static/css/override.css">

  <script src="https://code.jquery.com/jquery-3.6.4.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <!--overrides-->
  <link rel="stylesheet" href="static/css/override.css">
  <script src="static/js/sortingtable.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EyeVault</h1>
            <h2 class="title is-2 publication-title">Opening Eyes to Inclusion: An Insightful Analysis of Eye Tracking Datasets</h2>
            <img src="static/images/eyelogo.png">
            <!-- Paper authors -->
            <!--<div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span>
              </div>-->

                  <!--<div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Conferance name and year</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>-->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="#" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!--<span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!--<span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            Several calls have encouraged researchers to foster research ethics, reproducibility, and transparency in their empirical research. One method of achieving this is publishing datasets to promote open-source practices, and encouraging data reuse. Surprisingly, inclusivity in eye tracking data collection has been largely overlooked in the community. Most published models and prototypes lean heavily on existing datasets, claiming broad generalizability but do not consider the need for diverse data sources. This paper critically evaluates all eye tracking datasets published or utilized in ETRA proceedings from ETRA 2000 to 2023, including adjunct materials. Our findings reveal that few published datasets include crucial information about factors impacting data generalizability, like sample size, ethnicity, age range, eye color, and face coverage.  We discuss several implications and suggest guidelines for more comprehensive data reporting. This paper aims to guide the eye tracking community toward a more diverse and inclusive approach to collecting eye tracking data.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Table -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column ">
        <h2 class="title is-3">EyeVault Dataset</h2>
        <div class="content has-text-justified">
          
          <!--begin table-->
          <table id ="sortable-table">
            <thead>
            <tr>
            <th>Dataset name</th>
            <th>Dataset link</th>
            <th>Dataset Publication link</th>
            <th>Dataset BibTeX</th>
            <th>Dataset publication year</th>
            <th>Dataset available?</th>
            <th>Dataset published at ETRA?</th>
            <th>ETRA Paper Type</th>
            <th>Paper Title</th>
            <th>Year published</th>
            <th>Paper link</th>
            <th>Paper BibTeX</th>
            <th>Data privacy Mentioned</th>
            <th>Hardware</th>
            <th>static vs. Mobile</th>
            <th>screen based, VR or wearable, RGB</th>
            <th>frequency</th>
            <th>screen resoultion or camera resolution</th>
            <th>Visual angle</th>
            <th>eye tracker accuracy</th>
            <th>Data type collected</th>
            <th>Data type published</th>
            <th>Data analysed</th>
            <th>Data Cleaning</th>
            <th>Data size</th>
            <th>Camera recording the environment?</th>
            <th>Lab vs. Wild vs remote study</th>
            <th>Calibration needed</th>
            <th>Chinrest and other constraints</th>
            <th>Task</th>
            <th>Task duration</th>
            <th>Distance to stimuli</th>
            <th>Standing, sitting, walking</th>
            <th>Study time in the day</th>
            <th>screen luminance </th>
            <th>Light control</th>
            <th>N=?</th>
            <th colspan="3">Gender</th>
            <th>Compensation mentioned ?</th>
            <th>Background</th>
            <th>Major</th>
            <th>Age</th>
            <th>Average age</th>
            <th>Gaze correction</th>
            <th>Ethnicity/nationality/race</th>
            <th>Eye tracking experience</th>
            <th>Reading direction</th>
            <th>Dominant hand</th>
            <th>Dominent eye</th>
            <th>letter contrast sensitivity</th>
            <th>Retinal defect</th>
            <th>Eye Lid</th>
            <th>Eye diseases reporting</th>
            <th>Face coverage</th>
            <th>other demographics</th>
            <th>Makeup/fake lashes etc</th>
            <th>Pupil size</th>
            <th>IPDs</th>
            <th>inclusion and exclusion critria </th>
              </tr>
            </thead>
            <tbody>
            <tr>
            <td>3D_gaze_dataset</td>
            <td><a href="https://github.com/elmadjian2/3D_gaze_dataset">https://github.com/elmadjian2/3D_gaze_dataset</a></td>
            <td><a href="https://doi.org/10.1145/3206343.3206351">https://doi.org/10.1145/3206343.3206351</a></td>
            <td>@inproceedings{10.1145/3206343.3206351,<br>  author = {Elmadjian, Carlos and Shukla, Pushkar and Tula, Antonio Diaz and Morimoto, Carlos H.},<br>  title = {3D Gaze Estimation in the Scene Volume with a Head-Mounted Eye Tracker},<br>  year = {2018},<br>  isbn = {9781450357906},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3206343.3206351},<br>  doi = {10.1145/3206343.3206351},<br>  booktitle = {Proceedings of the Workshop on Communication by Gaze Interaction},<br>  articleno = {3},<br>  numpages = {9},<br>  keywords = {calibration, 3D dataset, gaze estimation, head-mounted eye tracking},<br>  location = {Warsaw, Poland},<br>  series = {COGAIN '18}<br>}</td>
            <td>2018</td>
            <td>yes</td>
            <td>Yes</td>
            <td>COGAIN Workshop</td>
            <td>3D gaze estimation in the scene volume with a head-mounted eye tracker</td>
            <td>2018</td>
            <td><a href="https://doi.org/10.1145/3206343.3206351">https://doi.org/10.1145/3206343.3206351</a></td>
            <td>@inproceedings{10.1145/3206343.3206351,<br>  author = {Elmadjian, Carlos and Shukla, Pushkar and Tula, Antonio Diaz and Morimoto, Carlos H.},<br>  title = {3D Gaze Estimation in the Scene Volume with a Head-Mounted Eye Tracker},<br>  year = {2018},<br>  isbn = {9781450357906},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3206343.3206351},<br>  doi = {10.1145/3206343.3206351},<br>  booktitle = {Proceedings of the Workshop on Communication by Gaze Interaction},<br>  articleno = {3},<br>  numpages = {9},<br>  keywords = {calibration, head-mounted eye tracking, gaze estimation, 3D dataset},<br>  location = {Warsaw, Poland},<br>  series = {COGAIN '18}<br>  }</td>
            <td> </td>
            <td>Pupil binocular eye tracker  coupled with an<br>  Intel Realsense R200 camera</td>
            <td>-</td>
            <td>wearable</td>
            <td>30 HZ for the eye tracker </td>
            <td>480p for the camera</td>
            <td>-</td>
            <td>-</td>
            <td>Pupil images for both eyes</td>
            <td>Pupil images for both eyes</td>
            <td>Pupil images for both eyes</td>
            <td> </td>
            <td>-</td>
            <td>No</td>
            <td>lab</td>
            <td>yes</td>
            <td>-</td>
            <td> follow a green target</td>
            <td>-</td>
            <td>2.75 m from the<br>  wall with teh projected stimuli</td>
            <td>standng could be extarcted from the setup images</td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>11</td>
            <td>5 women</td>
            <td>6 men</td>
            <td>-</td>
            <td>no</td>
            <td>-</td>
            <td>-</td>
            <td>22-35</td>
            <td>-</td>
            <td>no</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>55Rides</td>
            <td><a href="https://www.hci.uni-tuebingen.de/research/Applications/Driving/55rides.html">https://www.hci.uni-tuebingen.de/research/Applications/Driving/55rides.html</a></td>
            <td><a href="https://doi.org/10.1145/3448018.3457993">https://doi.org/10.1145/3448018.3457993</a></td>
            <td>@inproceedings{10.1145/3448018.3457993,<br>  author = {K\"{u}bler, Thomas C and Fuhl, Wolfgang and Wagner, Elena and Kasneci, Enkelejda},<br>  title = {55 Rides: Attention Annotated Head and Gaze Data during Naturalistic Driving},<br>  year = {2021},<br>  isbn = {9781450383455},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3448018.3457993},<br>  doi = {10.1145/3448018.3457993},<br>  booktitle = {ACM Symposium on Eye Tracking Research and Applications},<br>  articleno = {17},<br>  numpages = {8},<br>  keywords = {gaze detection, driver attention, datasets, neural networks},<br>  location = {Virtual Event, Germany},<br>  series = {ETRA '21 Short Papers}<br>  }</td>
            <td>2021</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Short</td>
            <td>55 Rides: attention annotated head and gaze data during naturalistic driving</td>
            <td>2021</td>
            <td><a href="https://doi.org/10.1145/3448018.3457993">https://doi.org/10.1145/3448018.3457993</a></td>
            <td>@inproceedings{10.1145/3448018.3457993,<br>  author = {K\"{u}bler, Thomas C and Fuhl, Wolfgang and Wagner, Elena and Kasneci, Enkelejda},<br>  title = {55 Rides: Attention Annotated Head and Gaze Data during Naturalistic Driving},<br>  year = {2021},<br>  isbn = {9781450383455},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi-org.libproxy.abertay.ac.uk/10.1145/3448018.3457993},<br>  doi = {10.1145/3448018.3457993},<br>  booktitle = {ACM Symposium on Eye Tracking Research and Applications},<br>  articleno = {17},<br>  numpages = {8},<br>  keywords = {gaze detection, driver attention, neural networks, datasets},<br>  location = {Virtual Event, Germany},<br>  series = {ETRA '21 Short Papers}<br>  }</td>
            <td>yes</td>
            <td>NIR-cameras</td>
            <td>in the car</td>
            <td>camera static location</td>
            <td>-</td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>Attention level</td>
            <td>head movement, eye movement, eye opening angle</td>
            <td> </td>
            <td>28 hours of data</td>
            <td>yes, but not published due to legal restrctions</td>
            <td>wild</td>
            <td>-</td>
            <td>-</td>
            <td>drive</td>
            <td>30 minutes</td>
            <td>not mentiobed bur arround 35 as its arround the car radio area</td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td>no</td>
            <td>4</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>no</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>Visual Analytics Methodology for Eye Movement Studies</td>
            <td><a href="http://geoanalytics.net/and/papers/vast2012em/data/">http://geoanalytics.net/and/papers/vast2012em/data/</a></td>
            <td><a href="https://ieeexplore.ieee.org/document/6065011">https://ieeexplore.ieee.org/document/6065011</a></td>
            <td>@ARTICLE{6065011,<br>  author={Burch, Michael and Konevtsova, Natalia and Heinrich, Julian and Hoeferlin, Markus and Weiskopf, Daniel},<br>  journal={IEEE Transactions on Visualization and Computer Graphics}, <br>  title={Evaluation of Traditional, Orthogonal, and Radial Tree Diagrams by an Eye Tracking Study}, <br>  year={2011},<br>  volume={17},<br>  number={12},<br>  pages={2440-2448},<br>  doi={10.1109/TVCG.2011.193}}<br>  </td>
            <td>2012</td>
            <td>yes</td>
            <td>No</td>
            <td>Full Paper</td>
            <td>A Dynamic Graph Visualization Perspective on Eye Movement Data</td>
            <td>2014</td>
            <td><a href="https://dl.acm.org/doi/10.1145/2578153.2578175">https://dl.acm.org/doi/10.1145/2578153.2578175</a></td>
            <td>@inproceedings{10.1145/2578153.2578175,<br> author = {Burch, Michael and Beck, Fabian and Raschke, Michael and Blascheck, Tanja and Weiskopf, Daniel},<br> title = {A Dynamic Graph Visualization Perspective on Eye Movement Data},<br> year = {2014},<br> isbn = {9781450327510},<br> publisher = {Association for Computing Machinery},<br> address = {New York, NY, USA},<br> url = {https://doi.org/10.1145/2578153.2578175},<br> doi = {10.1145/2578153.2578175},<br> booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},<br> pages = {151\u2013158},<br> numpages = {8},<br> keywords = {spatio-temporal data, eye tracking, dynamic graph visualization},<br> location = {Safety Harbor, Florida},<br> series = {ETRA '14}<br> }</td>
            <td> </td>
            <td>Tobii T60 XL</td>
            <td>-</td>
            <td>screen based</td>
            <td> 30 fps</td>
            <td>screen 1920 \u00d7 1200 pixels</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>fixation duration, and location</td>
            <td>saccades, heat maps, gaze plots, and areas of interest</td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>lab</td>
            <td>yes</td>
            <td>-</td>
            <td>participants were given different tree<br>  diagrams and asked to find the least common ancestor of three or<br>  more marked leaf nodes</td>
            <td>41 to 76 minutes</td>
            <td>65 cms</td>
            <td>standing could be extracted from the setup picture</td>
            <td> </td>
            <td> </td>
            <td>yes</td>
            <td>38</td>
            <td>10 female</td>
            <td>28 males</td>
            <td>-</td>
            <td>yes</td>
            <td>students and 5 graduates</td>
            <td>17 from CS and the rest form engineering</td>
            <td>19 - 54</td>
            <td>25.6</td>
            <td>13 with glasses and 5 with contact lenses</td>
            <td>-</td>
            <td>-</td>
            <td>left to right</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>An Eye Tracking Dataset for Point of Gaze Detection</td>
            <td><a href="http://heracleia.uta.edu/~mcmurrough/eyetracking/">broken link: http://heracleia.uta.edu/~mcmurrough/eyetracking/</a></td>
            <td><a href="https://dl.acm.org/doi/10.1145/2168556.2168622">https://dl.acm.org/doi/10.1145/2168556.2168622</a></td>
            <td>@inproceedings{10.1145/2168556.2168622,<br>  author = {McMurrough, Christopher D. and Metsis, Vangelis and Rich, Jonathan and Makedon, Fillia},<br>  title = {An Eye Tracking Dataset for Point of Gaze Detection},<br>  year = {2012},<br>  isbn = {9781450312219},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/2168556.2168622},<br>  doi = {10.1145/2168556.2168622},<br>  booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},<br>  pages = {305\u2013308},<br>  numpages = {4},<br>  keywords = {dataset, eye tracking, point of gaze detection},<br>  location = {Santa Barbara, California},<br>  series = {ETRA '12}<br>  }</td>
            <td>2012</td>
            <td>link is broken</td>
            <td>Yes</td>
            <td>Full Paper</td>
            <td>An eye tracking dataset for point of gaze detection</td>
            <td>2012</td>
            <td><a href="https://dl.acm.org/doi/10.1145/2168556.2168622">https://dl.acm.org/doi/10.1145/2168556.2168622</a></td>
            <td>@inproceedings{10.1145/2168556.2168622,<br> author = {McMurrough, Christopher D. and Metsis, Vangelis and Rich, Jonathan and Makedon, Fillia},<br> title = {An Eye Tracking Dataset for Point of Gaze Detection},<br> year = {2012},<br> isbn = {9781450312219},<br> publisher = {Association for Computing Machinery},<br> address = {New York, NY, USA},<br> url = {https://doi.org/10.1145/2168556.2168622},<br> doi = {10.1145/2168556.2168622},<br> booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},<br> pages = {305\u2013308},<br> numpages = {4},<br> keywords = {dataset, point of gaze detection, eye tracking},<br> location = {Santa Barbara, California},<br> series = {ETRA '12}<br> }</td>
            <td> </td>
            <td><a href="http://www.asleyetracking.com/">http://www.asleyetracking.com/</a></td>
            <td>static</td>
            <td>wearable</td>
            <td>29.97 Hz</td>
            <td>video: 768 x 480 pixels</td>
            <td>-</td>
            <td>-</td>
            <td>eye and head position in 3D</td>
            <td>eye motion videos and eye and ehad position in 3D</td>
            <td>-</td>
            <td> </td>
            <td>-</td>
            <td>yes but not published</td>
            <td>lab</td>
            <td>yes</td>
            <td>-</td>
            <td>two sessions, follow a tagret on the screen in 9 targets and in 16 targets</td>
            <td>-</td>
            <td>not mintioned but arround 45cm from the pictures</td>
            <td>sitting could be extracted from the setup pic</td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>20</td>
            <td>2 women</td>
            <td>18 men</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>no</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>Analysis of User Behavior in Interfaces with Recommended<br>  Items: An Eye-tracking Study<br>  </td>
            <td>not published</td>
            <td><a href="https://ceur-ws.org/Vol-2225/paper5.pdf">https://ceur-ws.org/Vol-2225/paper5.pdf</a></td>
            <td>@inproceedings{gaspar2018analysis,<br>  title={Analysis of User Behavior in Interfaces with Recommended Items: An Eye-tracking Study.},<br>  author={Gaspar, Peter and Kompan, Michal and Simko, Jakub and Bielikova, Maria},<br>  booktitle={IntRS@ RecSys},<br>  pages={32--36},<br>  year={2018}<br>  }</td>
            <td>2018</td>
            <td>no</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Eye Tracking as a Source of Implicit Feedback in Recommender<br>  Systems: A Preliminary Analysis<br>  </td>
            <td>2023</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3588015.3589511">https://dl.acm.org/doi/10.1145/3588015.3589511</a></td>
            <td> </td>
            <td> </td>
            <td>Tobii X2-60 eye-trackers</td>
            <td>static</td>
            <td>screen based</td>
            <td>60 hz </td>
            <td>screen: 1920 \u00d7 1200 pixels</td>
            <td>-</td>
            <td>-</td>
            <td>raw data</td>
            <td>not published</td>
            <td>fixations</td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>lab</td>
            <td>yes</td>
            <td>-</td>
            <td>select a movie, 24 times our of 192 movies</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>64 reduced to 56 after data cleaning</td>
            <td>19 females</td>
            <td>45 males</td>
            <td>-</td>
            <td>-</td>
            <td>highschool </td>
            <td>-</td>
            <td>15-27</td>
            <td>- </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>eyesum</td>
            <td><a href="https://notredame.app.box.com/s/bhyoqle1i90vuz75pu4ufch3rpz1lpij">https://notredame.app.box.com/s/bhyoqle1i90vuz75pu4ufch3rpz1lpij</a></td>
            <td><a href="https://dl.acm.org/doi/pdf/10.1145/2568225.2568247">https://dl.acm.org/doi/pdf/10.1145/2568225.2568247</a></td>
            <td>@inproceedings{10.1145/2568225.2568247,<br>  author = {Rodeghero, Paige and McMillan, Collin and McBurney, Paul W. and Bosch, Nigel and D'Mello, Sidney},<br>  title = {Improving Automated Source Code Summarization via an Eye-Tracking Study of Programmers},<br>  year = {2014},<br>  isbn = {9781450327565},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/2568225.2568247},<br>  doi = {10.1145/2568225.2568247},<br>  booktitle = {Proceedings of the 36th International Conference on Software Engineering},<br>  pages = {390\u2013401},<br>  numpages = {12},<br>  keywords = {source code summaries, program comprehension},<br>  location = {Hyderabad, India},<br>  series = {ICSE 2014}<br>  }</td>
            <td>2014</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>Towards Modeling Human Attention from Eye Movements for Neural Source Code Summarization</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3591136">https://doi.org/10.1145/3591136</a></td>
            <td>@article{bansal2023towards,<br>  title={Towards Modeling Human Attention from Eye Movements for Neural Source Code Summarization},<br>  author={Bansal, Aakash and Sharif, Bonita and McMillan, Collin},<br>  journal={Proceedings of the ACM on Human-Computer Interaction},<br>  volume={7},<br>  number={ETRA},<br>  pages={1--19},<br>  year={2023},<br>  publisher={ACM New York, NY, USA}<br>  }</td>
            <td> </td>
            <td>Tobii TX300</td>
            <td>static</td>
            <td>screen based</td>
            <td>120 hz</td>
            <td>eye tracker resolution: 1920x1080 a</td>
            <td>-</td>
            <td>-</td>
            <td> gaze time, fixations,<br>  and regressions.</td>
            <td>fixations and gaze point</td>
            <td>total eye gaze time</td>
            <td> </td>
            <td>130 data points x 9</td>
            <td>-</td>
            <td>lab</td>
            <td>-</td>
            <td>-</td>
            <td>reading source code of a Java method, and subsequently writing a short summary<br>  for each method</td>
            <td>1 hour over all study duration</td>
            <td>- </td>
            <td>sitting coule be extracted from the task</td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>10</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>profdessional programmers</td>
            <td>- </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>MET</td>
            <td><a href="https://osf.io/x2e6j/">https://osf.io/x2e6j/</a></td>
            <td><a href="https://dl.acm.org/doi/10.1145/3588015.3590128">https://dl.acm.org/doi/10.1145/3588015.3590128</a></td>
            <td>@article{10.1145/3472622,<br>  author = {Siegfried, R\'{e}my and Odobez, Jean-Marc},<br>  title = {Robust Unsupervised Gaze Calibration Using Conversation and Manipulation Attention Priors},<br>  year = {2022},<br>  issue_date = {January 2022},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  volume = {18},<br>  number = {1},<br>  issn = {1551-6857},<br>  url = {https://doi.org/10.1145/3472622},<br>  doi = {10.1145/3472622},<br>  journal = {ACM Trans. Multimedia Comput. Commun. Appl.},<br>  month = {jan},<br>  articleno = {20},<br>  numpages = {27},<br>  keywords = {conversation, visual focus of attention, unsupervised calibration, manipulation, remote sensor, Gaze estimation, RGB-D camera, online calibration}<br>  }</td>
            <td>2023</td>
            <td>yes</td>
            <td>Yes</td>
            <td>adjunct</td>
            <td>A Dataset of Underrepresented Languages<br>  in Eye Tracking Research</td>
            <td>2023</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3588015.3590128">https://dl.acm.org/doi/10.1145/3588015.3590128</a></td>
            <td> </td>
            <td> </td>
            <td>EyeLink 1000 Plus</td>
            <td>static</td>
            <td>screen based</td>
            <td>1000 Hz</td>
            <td>screen resolution was set<br>  to 1920x1080</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>yes</td>
            <td>-</td>
            <td>An English language trial and a trial for each language<br>  that they know (except for three participants who did not do the<br>  English trial). The participant is asked to read 10 sentences in each<br>  trial, before answering a set of 4 comprehension questions</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>40</td>
            <td>21 female</td>
            <td>18 male</td>
            <td>1 non binary</td>
            <td>yes</td>
            <td>students one prof</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>KTH-Idiap Group-Interviewing Corpus</td>
            <td><a href="https://www.idiap.ch/en/dataset/gaze_vfoa_conversation_and_manipulation/index_html">https://www.idiap.ch/en/dataset/gaze_vfoa_conversation_and_manipulation/index_html</a></td>
            <td><a href="https://dl.acm.org/doi/pdf/10.1145/3472622">https://dl.acm.org/doi/pdf/10.1145/3472622</a></td>
            <td>@article{10.1145/3472622,<br>  author = {Siegfried, R\'{e}my and Odobez, Jean-Marc},<br>  title = {Robust Unsupervised Gaze Calibration Using Conversation and Manipulation Attention Priors},<br>  year = {2022},<br>  issue_date = {January 2022},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  volume = {18},<br>  number = {1},<br>  issn = {1551-6857},<br>  url = {https://doi.org/10.1145/3472622},<br>  doi = {10.1145/3472622},<br>  journal = {ACM Trans. Multimedia Comput. Commun. Appl.},<br>  month = {jan},<br>  articleno = {20},<br>  numpages = {27},<br>  keywords = {conversation, visual focus of attention, unsupervised calibration, manipulation, remote sensor, Gaze estimation, RGB-D camera, online calibration}<br>  }</td>
            <td>2021</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>A Deep Learning Approach for Robust Head Pose Independent Eye Movements Recognition from Videos</td>
            <td>2019</td>
            <td><a href="https://doi.org/10.1145/3314111.3319844">https://doi.org/10.1145/3314111.3319844</a></td>
            <td>@inproceedings{siegfried2019deep,<br>  title={A deep learning approach for robust head pose independent eye movements recognition from videos},<br>  author={Siegfried, R{\'e}my and Yu, Yu and Odobez, Jean-Marc},<br>  booktitle={Proceedings of the 11th ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  pages={1--5},<br>  year={2019}<br>  }</td>
            <td>no</td>
            <td>GoPro</td>
            <td>static</td>
            <td>scrrenbased</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>face images</td>
            <td>head pose, gaze, speaking status, people/object positions</td>
            <td>head pose, gaze, speaking status, people/object positions</td>
            <td>-</td>
            <td>five 1-hour four-party meetings i</td>
            <td>yes</td>
            <td>lab</td>
            <td>-</td>
            <td>no</td>
            <td>natural conversation</td>
            <td>1 hour</td>
            <td>-</td>
            <td>sitting</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>20</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Dgaze</td>
            <td><a href="http://zhiminghu.net/hu20_dgaze.html">http://zhiminghu.net/hu20_dgaze.html</a></td>
            <td><a href="https://ieeexplore.ieee.org/document/8998375">https://ieeexplore.ieee.org/document/8998375</a></td>
            <td>@article{hu20_dgaze,<br>  title = {DGaze: CNN-Based Gaze Prediction in Dynamic Scenes},<br>  author = {Hu, Zhiming and Li, Sheng and Zhang, Congyi and Yi, Kangrui and Wang, Guoping and Manocha, Dinesh},<br>  journal = {IEEE Transactions on Visualization and Computer Graphics},<br>  volume = {26},<br>  number = {5},<br>  pages = {1902--1911},<br>  year = {2020},<br>  doi = {10.1109/TVCG.2020.2973473}<br>  }</td>
            <td>2019</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>A Deep Learning Architecture for Egocentric Time-to-Saccade Prediction using Weibull Mixture-Models and Historic Priors</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588408">https://doi.org/10.1145/3588015.3588408</a></td>
            <td>@inproceedings{rolff2023deep,<br>  title={A Deep Learning Architecture for Egocentric Time-to-Saccade Prediction using Weibull Mixture-Models and Historic Priors},<br>  author={Rolff, Tim and Schmidt, Susanne and Steinicke, Frank and Frintrop, Simone},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--8},<br>  year={2023}<br>  }</td>
            <td> </td>
            <td>HTC Vive</td>
            <td>static</td>
            <td>VR</td>
            <td>100 Hz</td>
            <td>The CPU and<br>  GPU of our platform are an Intel(R) Core(TM) i7-8700 @ 3.20GHz<br>  and an NVIDIA GeForce RTX 2080 Ti, respectively</td>
            <td>-</td>
            <td>0.5 angle</td>
            <td>raw data</td>
            <td>raw data for head, and eye + videos + fixations + saliency maps</td>
            <td>angular distance between the ground truth and the predicted gaze<br>  position</td>
            <td> </td>
            <td>86 pieces of data from 43 users<br>  and each piece of data contains about 18,000 gaze positions, 18,000<br>  object positions, 36,000 head velocities, and 10,800 frames of scene<br>  screenshots.</td>
            <td>VR</td>
            <td>lab</td>
            <td>yes</td>
            <td>-</td>
            <td>explores 2 scenes<br>  (randomly chosen from our 5 scenes)</td>
            <td>6 minutes</td>
            <td>-</td>
            <td>sitting could be extracted from the setup</td>
            <td> </td>
            <td> </td>
            <td>some scenes were outdoors and some were indoors</td>
            <td>43</td>
            <td>18 females</td>
            <td>25 males</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>18-32</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Ego4D</td>
            <td><a href="https://ego4d-data.org/">https://ego4d-data.org/</a></td>
            <td><a href="https://arxiv.org/abs/2110.07058">https://arxiv.org/abs/2110.07058</a></td>
            <td>@misc{grauman2022ego4d,<br>    title={Ego4D: Around the World in 3,000 Hours of Egocentric Video}, <br>    author={Kristen Grauman and Andrew Westbury and Eugene Byrne and Zachary Chavis and Antonino Furnari and Rohit Girdhar and Jackson Hamburger and Hao Jiang and Miao Liu and Xingyu Liu and Miguel Martin and Tushar Nagarajan and Ilija Radosavovic and Santhosh Kumar Ramakrishnan and Fiona Ryan and Jayant Sharma and Michael Wray and Mengmeng Xu and Eric Zhongcong Xu and Chen Zhao and Siddhant Bansal and Dhruv Batra and Vincent Cartillier and Sean Crane and Tien Do and Morrie Doulaty and Akshay Erapalli and Christoph Feichtenhofer and Adriano Fragomeni and Qichen Fu and Abrham Gebreselasie and Cristina Gonzalez and James Hillis and Xuhua Huang and Yifei Huang and Wenqi Jia and Weslie Khoo and Jachym Kolar and Satwik Kottur and Anurag Kumar and Federico Landini and Chao Li and Yanghao Li and Zhenqiang Li and Karttikeya Mangalam and Raghava Modhugu and Jonathan Munro and Tullie Murrell and Takumi Nishiyasu and Will Price and Paola Ruiz Puentes and Merey Ramazanova and Leda Sari and Kiran Somasundaram and Audrey Southerland and Yusuke Sugano and Ruijie Tao and Minh Vo and Yuchen Wang and Xindi Wu and Takuma Yagi and Ziwei Zhao and Yunyi Zhu and Pablo Arbelaez and David Crandall and Dima Damen and Giovanni Maria Farinella and Christian Fuegen and Bernard Ghanem and Vamsi Krishna Ithapu and C. V. Jawahar and Hanbyul Joo and Kris Kitani and Haizhou Li and Richard Newcombe and Aude Oliva and Hyun Soo Park and James M. Rehg and Yoichi Sato and Jianbo Shi and Mike Zheng Shou and Antonio Torralba and Lorenzo Torresani and Mingfei Yan and Jitendra Malik},<br>    year={2022},<br>    eprint={2110.07058},<br>    archivePrefix={arXiv},<br>    primaryClass={cs.CV}<br>  }</td>
            <td>2021</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Short</td>
            <td>A Deep Learning Architecture for Egocentric Time-to-Saccade Prediction using Weibull Mixture-Models and Historic Priors</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588408">https://doi.org/10.1145/3588015.3588408</a></td>
            <td>@inproceedings{rolff2023deep,<br>  title={A Deep Learning Architecture for Egocentric Time-to-Saccade Prediction using Weibull Mixture-Models and Historic Priors},<br>  author={Rolff, Tim and Schmidt, Susanne and Steinicke, Frank and Frintrop, Simone},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--8},<br>  year={2023}<br>  }</td>
            <td> </td>
            <td>zShade<br>  1080p camera glasses, iVue Rincon 1080 camera glasses,<br>  ORDRO EP-6, and Pupil Labs Invisible camera and gaze<br>  tracking glasses</td>
            <td>mobile</td>
            <td>werable</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>raw data</td>
            <td>raw data + videos</td>
            <td>-</td>
            <td> </td>
            <td>103 hours</td>
            <td>yes</td>
            <td>wild</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td>-</td>
            <td>66</td>
            <td>42 females</td>
            <td>23 males</td>
            <td>1 non binary</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>46 were 20-29 years old, 14 were 30-39 years old, 1<br>  was 40-49, 2 were 50-59, 1 was 60-69, and 2 were 70-79).</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Ego4D</td>
            <td><a href="https://ego4d-data.org/">https://ego4d-data.org/</a></td>
            <td><a href="https://arxiv.org/abs/2110.07058">https://arxiv.org/abs/2110.07058</a></td>
            <td>@misc{grauman2022ego4d,<br>    title={Ego4D: Around the World in 3,000 Hours of Egocentric Video}, <br>    author={Kristen Grauman and Andrew Westbury and Eugene Byrne and Zachary Chavis and Antonino Furnari and Rohit Girdhar and Jackson Hamburger and Hao Jiang and Miao Liu and Xingyu Liu and Miguel Martin and Tushar Nagarajan and Ilija Radosavovic and Santhosh Kumar Ramakrishnan and Fiona Ryan and Jayant Sharma and Michael Wray and Mengmeng Xu and Eric Zhongcong Xu and Chen Zhao and Siddhant Bansal and Dhruv Batra and Vincent Cartillier and Sean Crane and Tien Do and Morrie Doulaty and Akshay Erapalli and Christoph Feichtenhofer and Adriano Fragomeni and Qichen Fu and Abrham Gebreselasie and Cristina Gonzalez and James Hillis and Xuhua Huang and Yifei Huang and Wenqi Jia and Weslie Khoo and Jachym Kolar and Satwik Kottur and Anurag Kumar and Federico Landini and Chao Li and Yanghao Li and Zhenqiang Li and Karttikeya Mangalam and Raghava Modhugu and Jonathan Munro and Tullie Murrell and Takumi Nishiyasu and Will Price and Paola Ruiz Puentes and Merey Ramazanova and Leda Sari and Kiran Somasundaram and Audrey Southerland and Yusuke Sugano and Ruijie Tao and Minh Vo and Yuchen Wang and Xindi Wu and Takuma Yagi and Ziwei Zhao and Yunyi Zhu and Pablo Arbelaez and David Crandall and Dima Damen and Giovanni Maria Farinella and Christian Fuegen and Bernard Ghanem and Vamsi Krishna Ithapu and C. V. Jawahar and Hanbyul Joo and Kris Kitani and Haizhou Li and Richard Newcombe and Aude Oliva and Hyun Soo Park and James M. Rehg and Yoichi Sato and Jianbo Shi and Mike Zheng Shou and Antonio Torralba and Lorenzo Torresani and Mingfei Yan and Jitendra Malik},<br>    year={2022},<br>    eprint={2110.07058},<br>    archivePrefix={arXiv},<br>    primaryClass={cs.CV}<br>  }</td>
            <td>2021</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Short</td>
            <td>A Deep Learning Architecture for Egocentric Time-to-Saccade Prediction using Weibull Mixture-Models and Historic Priors</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588408">https://doi.org/10.1145/3588015.3588408</a></td>
            <td>@inproceedings{rolff2023deep,<br>  title={A Deep Learning Architecture for Egocentric Time-to-Saccade Prediction using Weibull Mixture-Models and Historic Priors},<br>  author={Rolff, Tim and Schmidt, Susanne and Steinicke, Frank and Frintrop, Simone},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--8},<br>  year={2023}<br>  }</td>
            <td> </td>
            <td>ORDRO EP6 camera wand  Pupil Invisible</td>
            <td>mobile</td>
            <td>wearble</td>
            <td>-</td>
            <td>Logitech C930e Webcam</td>
            <td>-</td>
            <td>-</td>
            <td>raw data</td>
            <td>raw data + videos</td>
            <td>-</td>
            <td> </td>
            <td>43 hours</td>
            <td>yes</td>
            <td>wild</td>
            <td>-</td>
            <td>-</td>
            <td>eating, drinking, and playing social<br>  deduction games in their homes</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td>-</td>
            <td>19</td>
            <td>7 females</td>
            <td>10 males</td>
            <td>1 non binary</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>mean age of 31.6 years with 7 participants aged 20-29 years,<br>  10 participants aged 30-39 years, and 2 participants aged<br>  40-49 years.</td>
            <td>18-64</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>FixationNet</td>
            <td><a href="http://zhiminghu.net/hu21_fixationnet.html">http://zhiminghu.net/hu21_fixationnet.html</a></td>
            <td><a href="https://zhiminghu.net/hu21_fixationnet/pdf/hu21_fixationnet.pdf">https://zhiminghu.net/hu21_fixationnet/pdf/hu21_fixationnet.pdf</a></td>
            <td>@article{hu21_fixationnet,<br>  author = {Hu, Zhiming and Bulling, Andreas and Li, Sheng and Wang, Guoping},<br>  title = {FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments},<br>  journal = {IEEE Transactions on Visualization and Computer Graphics (TVCG)},<br>  year = {2021},<br>  doi = {10.1109/TVCG.2021.3067779},<br>  pages = {2681--2690},<br>  volume = {27},<br>  number = {5},<br>  url = {https://cranehzm.github.io/FixationNet.html}<br>  }</td>
            <td>2021</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>A Deep Learning Architecture for Egocentric Time-to-Saccade Prediction using Weibull Mixture-Models and Historic Priors</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588408">https://doi.org/10.1145/3588015.3588408</a></td>
            <td>@inproceedings{rolff2023deep,<br>  title={A Deep Learning Architecture for Egocentric Time-to-Saccade Prediction using Weibull Mixture-Models and Historic Priors},<br>  author={Rolff, Tim and Schmidt, Susanne and Steinicke, Frank and Frintrop, Simone},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--8},<br>  year={2023}<br>  }</td>
            <td> </td>
            <td>HTC Vive with 7invensun VR eye tracker</td>
            <td>static</td>
            <td>VR</td>
            <td>100 Hz</td>
            <td>Intel(R) Core(TM) i7-10875H @ 2.30GHz CPU and an NVIDIA<br>  GeForce RTX 2060 GPU</td>
            <td>-</td>
            <td>0.5</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>12,000 gaze positions</td>
            <td>no</td>
            <td>-</td>
            <td>yes</td>
            <td>-</td>
            <td>search task</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td>-</td>
            <td>27</td>
            <td>12 females</td>
            <td>15 males</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>17 and 32 years</td>
            <td>-</td>
            <td>normal or corrected-to-normal</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>ETRA 2019 Challenge1</td>
            <td><a href="https://etra.acm.org/2019/challenge.html">https://etra.acm.org/2019/challenge.html</a></td>
            <td><a href="https://www.jneurosci.org/content/jneuro/34/8/2956.full.pdf">https://www.jneurosci.org/content/jneuro/34/8/2956.full.pdf</a></td>
            <td>@article{mccamy2014highly,<br>  title={Highly informative natural scene regions increase microsaccade production during visual scanning},<br>  author={McCamy, Michael B and Otero-Millan, Jorge and Di Stasi, Leandro Luigi and Macknik, Stephen L and Martinez-Conde, Susana},<br>  journal={Journal of neuroscience},<br>  volume={34},<br>  number={8},<br>  pages={2956--2966},<br>  year={2014},<br>  publisher={Soc Neuroscience}<br>  }</td>
            <td>2,014</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>A MinHash approach for fast scanpath classification</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379155.3391325">https://doi.org/10.1145/3379155.3391325</a></td>
            <td>@inproceedings{geisler2020minhash,<br>  title={A MinHash approach for fast scanpath classification},<br>  author={Geisler, David and Castner, Nora and Kasneci, Gjergji and Kasneci, Enkelejda},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--9},<br>  year={2020}<br>  }</td>
            <td> </td>
            <td>EyeLink II</td>
            <td>static</td>
            <td>wearble</td>
            <td> </td>
            <td>75 Hz refresh rate 40x30cm, 1024x768px</td>
            <td>0.75 degrees wide</td>
            <td>-</td>
            <td>fixations from both eyes</td>
            <td>-</td>
            <td>fixations, saccades and microsaccades</td>
            <td>blink removal</td>
            <td>We presented 15 different images per scene<br>  condition (except for the blank scene condition, which was presented 15 times). This resulted in a total of 8  15  120 trials,<br>  presented in pseudorandom order, and run<br>  over three sessions of 40 trials each. Each trial<br>  was 45 s long.</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>chinrest</td>
            <td>8 experimental conditions (4 fixation conditions and 4 free-viewing conditions).</td>
            <td>3 sessions each for 60 minutes</td>
            <td>57 cm</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td>-</td>
            <td>8</td>
            <td>6 females</td>
            <td>2 males</td>
            <td>-</td>
            <td>yes</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>normal or corrected-to-normal vision</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>ETRA 2019 Challenge2</td>
            <td><a href="https://etra.acm.org/2019/challenge.html">https://etra.acm.org/2019/challenge.html</a></td>
            <td><a href="https://jov.arvojournals.org/article.aspx?articleid=2193271">https://jov.arvojournals.org/article.aspx?articleid=2193271</a></td>
            <td>@article{otero2008saccades,<br>  title={Saccades and microsaccades during visual fixation, exploration, and search: foundations for a common saccadic generator},<br>  author={Otero-Millan, Jorge and Troncoso, Xoana G and Macknik, Stephen L and Serrano-Pedraza, Ignacio and Martinez-Conde, Susana},<br>  journal={Journal of vision},<br>  volume={8},<br>  number={14},<br>  pages={21--21},<br>  year={2008},<br>  publisher={The Association for Research in Vision and Ophthalmology}<br>  }</td>
            <td>2,008</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>A MinHash approach for fast scanpath classification</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379155.3391325">https://doi.org/10.1145/3379155.3391325</a></td>
            <td>@inproceedings{geisler2020minhash,<br>  title={A MinHash approach for fast scanpath classification},<br>  author={Geisler, David and Castner, Nora and Kasneci, Gjergji and Kasneci, Enkelejda},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--9},<br>  year={2020}<br>  }</td>
            <td> </td>
            <td>EyeLink II</td>
            <td>static</td>
            <td>wearble</td>
            <td>-</td>
            <td>75 Hz refresh rate 40x30cm, 1024x768px</td>
            <td>0.75 degrees wide</td>
            <td>-</td>
            <td>fixations from both eyes</td>
            <td>-</td>
            <td>fixations, saccades and microsaccades</td>
            <td>-</td>
            <td>500 sample/s X 60 minutes x 3 sessions</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>chinrest</td>
            <td>8 experimental conditions (4 fixation conditions and 4 free-viewing conditions).</td>
            <td>3 sessions each for 60 minutes</td>
            <td>57 cm</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td>-</td>
            <td>8</td>
            <td>6 females</td>
            <td>2 males</td>
            <td>-</td>
            <td>yes</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>normal or corrected-to-normal vision</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Yarbus</td>
            <td><a href="http://ilab.usc.edu/borji/Resources.html">http://ilab.usc.edu/borji/Resources.html</a></td>
            <td><a href="https://jov.arvojournals.org/article.aspx?articleid=2122021#87795308">https://jov.arvojournals.org/article.aspx?articleid=2122021#87795308</a></td>
            <td>@article{borji2014defending,<br>  title={Defending Yarbus: Eye movements reveal observers' task},<br>  author={Borji, Ali and Itti, Laurent},<br>  journal={Journal of vision},<br>  volume={14},<br>  number={3},<br>  pages={29--29},<br>  year={2014},<br>  publisher={The Association for Research in Vision and Ophthalmology}<br>  }</td>
            <td>2014</td>
            <td>th elink does not contain the dataset</td>
            <td>No</td>
            <td>Long</td>
            <td>A MinHash approach for fast scanpath classification</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379155.3391325">https://doi.org/10.1145/3379155.3391325</a></td>
            <td>@inproceedings{geisler2020minhash,<br>  title={A MinHash approach for fast scanpath classification},<br>  author={Geisler, David and Castner, Nora and Kasneci, Gjergji and Kasneci, Enkelejda},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--9},<br>  year={2020}<br>  }</td>
            <td> </td>
            <td>SR Research Eyelink eye tracker</td>
            <td>static</td>
            <td>screen-based</td>
            <td>1000 Hz.</td>
            <td>42 inch monitor,stimuli presented at 60 Hz at resolution of 1920 \u00d7 1080 pixels.</td>
            <td>43\u00b0 \u00d7 25\u00b0 of visual angle.</td>
            <td>0.5</td>
            <td>fixation</td>
            <td>-</td>
            <td>spatial fixations</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>chinrest</td>
            <td>view 15 paintings</td>
            <td>each image is shown for 30 seonds then a 5 secod grey sclae image</td>
            <td>130 cm</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td>-</td>
            <td>21</td>
            <td>11 females</td>
            <td>10 males</td>
            <td>-</td>
            <td>yes but not specified</td>
            <td>students </td>
            <td>from computer sciences, neuroscience, psychology, mathematics, cognitive sciences, communication, health, biology, sociology, business, and public relations</td>
            <td>19-24</td>
            <td>mean = 22.2, SD = 2.6</td>
            <td>normal or corrected-to-normal vision</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Hollywood</td>
            <td><a href="https://www.sciencedirect.com/science/article/pii/S2352340919303440?via%3Dihub">https://www.sciencedirect.com/science/article/pii/S2352340919303440?via%3Dihub</a></td>
            <td><a href="https://osf.io/g64tk/">https://osf.io/g64tk/</a></td>
            <td>@article{costela2019free,<br>  title={A free database of eye movements watching \u201cHollywood\u201d videoclips},<br>  author={Costela, Francisco M and Woods, Russell L},<br>  journal={Data in brief},<br>  volume={25},<br>  pages={103991},<br>  year={2019},<br>  publisher={Elsevier}<br>  }</td>
            <td>2019</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Area of interest adaption using feature importance</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588422">https://doi.org/10.1145/3588015.3588422</a></td>
            <td>@inproceedings{fuhl2023area,<br>  title={Area of interest adaption using feature importance},<br>  author={Fuhl, Wolfgang and Zabel, Susanne and Harbig, Theresa and Moldt, Julia-Astrid and Wietek, Teresa Festl and Herrmann-Werner, Anne and Nieselt, Kay},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--7},<br>  year={2023}<br>  }</td>
            <td> </td>
            <td>EyeLink 1000 system</td>
            <td>static</td>
            <td>screen-based</td>
            <td>-</td>
            <td>27\u201d display (60 \u00d7 34 cm; 120 Hz; 1920 x 1080 pixels 16:9 aspect ratio) </td>
            <td>-</td>
            <td>-</td>
            <td>fixation</td>
            <td>gaze recordings (eye movements with head constrained) from 95 observers as they watched \u201cHollywood\u201d clips. The raw gaze data is stored in Matlab files that contain a structure (\u2018EyetrackRecord\u2019) with the x and y gaze coordinates, sample time, pupil size, and an array indicating which samples were missing data.<br>  <br>  (2)<br>  206 video clips (MOV format) of 30 seconds duration viewed by 76 participants (about 21 viewing hours).<br>  <br>  (3)<br>  the first 30 seconds of eleven 30-min video clips. 19 participants viewed video clips of 30 minutes duration (about 38 viewing hours).<br>  <br>  (4)<br>  subjective ratings based on the content of the 217 videos for eight different categories, fixation data, democratic center of interest (COI) location per frame, precise timing of the scene cuts within the clips. The subjective ratings are provided for the 30-s video clips and for 30-s segments of the 30-min video clips.<br>  <br>  (5)<br>  demographic information about the 95 participants</td>
            <td>fixations and saccades</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>yes, they tracked the calibration error</td>
            <td>head and cinrest</td>
            <td> 206 clips that were 30-second duration or eleven clips of 30-minutes</td>
            <td>60 minutes</td>
            <td>1 meter</td>
            <td>sitting</td>
            <td>-</td>
            <td>maximum luminance was 128.9 and 723.5 cd/m2, and the minimum luminance was 0.14 and 0.26 cd/m2 </td>
            <td>yes</td>
            <td>95</td>
            <td>43 female</td>
            <td>52 males</td>
            <td>-</td>
            <td>-</td>
            <td>High school diploma4 (5%)<br>  Some college6 (8%)<br>  Bachelor's degree23 (32%)<br>  Associate degree2 (2%)<br>  Master's degree18 (24%)<br>  Professional degree7 (9%)<br>  Doctoral degree11 (17%)</td>
            <td>-</td>
            <td>22 to 85 years</td>
            <td>56.2 years</td>
            <td>visual acuity and contrast sensitivity for a 2.5</td>
            <td>Black    5 (5%)<br>  White    87 (91%)<br>  Asian    4 (4%)<br>  Hispanic    1 (1%)<br>  Not registered    3 (3%)</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td>1.67 log units or better</td>
            <td>no</td>
            <td>-</td>
            <td>healthy</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Multimodal Eye Movement Dataset</td>
            <td><a href="https://atreus.informatik.uni-tuebingen.de/seafile/d/%208e2ab8c3fdd444e1a135/?p=%2FA%20Multimodal%20Eye%20Movement%25%2020Dataset%20and%20...&mode=list">https://atreus.informatik.uni-tuebingen.de/seafile/d/ 8e2ab8c3fdd444e1a135/?p=%2FA%20Multimodal%20Eye%20Movement% 20Dataset%20and%20...&amp;mode=list</a></td>
            <td><a href="https://doi.org/10.1145/3448018.3458004">https://doi.org/10.1145/3448018.3458004</a></td>
            <td>@inproceedings{fuhl2021multimodal,<br>  title={A multimodal eye movement dataset and a multimodal eye movement segmentation analysis},<br>  author={Fuhl, Wolfgang and Kasneci, Enkelejda},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--7},<br>  year={2021}<br>  }</td>
            <td>2021</td>
            <td>link is broken</td>
            <td>Yes</td>
            <td>Short</td>
            <td>A Multimodal Eye Movement Dataset and a Multimodal Eye Movement Segmentation Analysis</td>
            <td>2021</td>
            <td><a href="https://doi.org/10.1145/3448018.3458004">https://doi.org/10.1145/3448018.3458004</a></td>
            <td>@inproceedings{10.1145/3448018.3458004,<br>  author = {Fuhl, Wolfgang and Kasneci, Enkelejda},<br>  title = {A Multimodal Eye Movement Dataset and a Multimodal Eye Movement Segmentation Analysis},<br>  year = {2021},<br>  isbn = {9781450383455},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3448018.3458004},<br>  doi = {10.1145/3448018.3458004},<br>  booktitle = {ACM Symposium on Eye Tracking Research and Applications},<br>  articleno = {16},<br>  numpages = {7},<br>  keywords = {Eye Movements, Machine Learning, Segmentation, Real World, Driving, Classification, Data set},<br>  location = {Virtual Event, Germany},<br>  series = {ETRA '21 Short Papers}<br>  }</td>
            <td> </td>
            <td>-</td>
            <td>static in a car</td>
            <td>-</td>
            <td> 25 frames per second.</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>fixation, sacades, blinks and smooth pursuits</td>
            <td> F1: Normalized euclidean pupil center distance between two<br>  frames.<br> F2: Normalized euclidean eyelid center vector distance between two frames.<br> F3: Normalized euclidean optical vector distance between<br>  two frames.<br> F4: Eyelid opening in relation to eye width (eye corner distance).<br> F5: Normalized pupil center distance in x direction.<br> F6: Normalized pupil center distance in y direction.<br> F7: Normalized eyelid center vector distance in x direction.<br> F8: Normalized eyelid center vector distance in y direction.<br> F9: Normalized optical vector distance in x direction.<br> F10: Normalized optical vector distance in y direction.<br> F11: Normalized optical vector distance in z direction</td>
            <td>fixation, sacades, blinks and smooth pursuits</td>
            <td>Jaccard index </td>
            <td>800,000 gaze points</td>
            <td>-</td>
            <td>in the wild and in a simulator</td>
            <td>-</td>
            <td>-</td>
            <td>car ride  in the real world and in the simulator.</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>19</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>GazeCom</td>
            <td><a href="https://www.inb.uni-luebeck.de/index.php?id=515">https://www.inb.uni-luebeck.de/index.php?id=515</a></td>
            <td><a href="https://jov.arvojournals.org/article.aspx?articleid=2121333">https://jov.arvojournals.org/article.aspx?articleid=2121333</a></td>
            <td>@article{dorr2010variability,<br>  title={Variability of eye movements when viewing dynamic natural scenes},<br>  author={Dorr, Michael and Martinetz, Thomas and Gegenfurtner, Karl R and Barth, Erhardt},<br>  journal={Journal of vision},<br>  volume={10},<br>  number={10},<br>  pages={28--28},<br>  year={2010},<br>  publisher={The Association for Research in Vision and Ophthalmology}<br>  }</td>
            <td>2010</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>A Novel Gaze Event Detection Metric<br>  That Is Not Fooled by Gaze-independent Baselines</td>
            <td>2019</td>
            <td><a href="https://doi.org/10.1145/3314111.3319836">https://doi.org/10.1145/3314111.3319836</a></td>
            <td>@inproceedings{startsev2019novel,<br>  title={A novel gaze event detection metric that is not fooled by gaze-independent baselines},<br>  author={Startsev, Mikhail and G{\"o}b, Stefan and Dorr, Michael},<br>  booktitle={Proceedings of the 11th ACM symposium on eye tracking research \&amp; applications},<br>  pages={1--9},<br>  year={2019}<br>  }</td>
            <td> </td>
            <td>SR Research EyeLink II </td>
            <td>static</td>
            <td>wearble</td>
            <td>250 Hz</td>
            <td>1280 by 960 pixels</td>
            <td>1 degree</td>
            <td>0.62 deg</td>
            <td>raw gaze data</td>
            <td>gaze &lt;horizontal resolution&gt; &lt;vertical resolution&gt;<br>  geometry distance &lt;screen-subject&gt; &lt;width of stimulus on screen&gt; &lt;height of stimulus on scre</td>
            <td>saccades, fixation duration, smooth pursuits</td>
            <td>removed invalid samples and blinks,</td>
            <td>-</td>
            <td>-</td>
            <td>lab</td>
            <td>yes</td>
            <td>-</td>
            <td>watch 18 movies</td>
            <td>-</td>
            <td>45 cm</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>54</td>
            <td>46 females</td>
            <td>8 males</td>
            <td>-</td>
            <td>yes</td>
            <td>students </td>
            <td>Physcology</td>
            <td>18 to 34 years</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Visualisation Tool</td>
            <td><a href="https://web.archive.org/web/*/https:/eyevis.west.uni-koblenz.de/">https://web.archive.org/web/*/https://eyevis.west.uni-koblenz.de/</a></td>
            <td><a href="https://doi.org/10.1145/3379156.3391831">https://doi.org/10.1145/3379156.3391831</a></td>
            <td>@inproceedings{10.1145/3379156.3391831,<br>  author = {Menges, Raphael and Kramer, Sophia and Hill, Stefan and Nisslmueller, Marius and Kumar, Chandan and Staab, Steffen},<br>  title = {A Visualization Tool for Eye Tracking Data Analysis in the Web},<br>  year = {2020},<br>  isbn = {9781450371346},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3379156.3391831},<br>  doi = {10.1145/3379156.3391831},<br>  booktitle = {ACM Symposium on Eye Tracking Research and Applications},<br>  articleno = {46},<br>  numpages = {5},<br>  keywords = {Web interaction, gaze visualization, heatmap, scanpath},<br>  location = {Stuttgart, Germany},<br>  series = {ETRA '20 Short Papers}<br>  }</td>
            <td>2020</td>
            <td>link is broken</td>
            <td>Yes</td>
            <td>Short</td>
            <td>A Visualization Tool for Eye Tracking Data Analysis in the Web</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391831">https://doi.org/10.1145/3379156.3391831</a></td>
            <td>@inproceedings{menges2020visualization,<br>  title={A visualization tool for eye tracking data analysis in the web},<br>  author={Menges, Raphael and Kramer, Sophia and Hill, Stefan and Nisslmueller, Marius and Kumar, Chandan and Staab, Steffen},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td> </td>
            <td>Tobii 4C</td>
            <td>static</td>
            <td>screen-based</td>
            <td>90 Hz</td>
            <td>15.6 inches screen that rendered with 1600 \u00d7 900 pixels</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td>gender, age, eye correction,<br>  Web experience, and English skills of each participant, alongside<br>  the Web site addresses, screenshots, transitions, gaze and mouse<br>  data on the Web pages</td>
            <td> </td>
            <td> </td>
            <td>25,000 fixations,<br>  20,000 mouse movements, and 650 mouse clicks on 164 Web pages<br>  from eleven different domains.</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>navigate the Web page like they would under everyday circumstances. </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>6</td>
            <td>4 females</td>
            <td>2 males</td>
            <td>-</td>
            <td>-</td>
            <td>professionals</td>
            <td>information systems, two in psychology, one as UX-expert, and one as a scientific employee</td>
            <td>25 to 59 </td>
            <td>36</td>
            <td>-</td>
            <td>-</td>
            <td>yes</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Eye-tracking analysis for Bayesian reasning</td>
            <td><a href="https://zenodo.org/record/1155584">https://zenodo.org/record/1155584</a></td>
            <td><a href="https://github.com/manurea/eye-tracking-analysis-for-Bayesian-reasoning-/blob/V1.0.0/submittedCOGNITION.pdf">https://github.com/manurea/eye-tracking-analysis-for-Bayesian-reasoning-/blob/V1.0.0/submittedCOGNITION.pdf</a></td>
            <td>Technical report</td>
            <td>2018</td>
            <td>yes</td>
            <td>No</td>
            <td>Full Paper</td>
            <td>An investigation of the effects of n-gram length in scanpath analysis for eye-tracking research</td>
            <td>2018</td>
            <td><a href="https://doi.org/10.1145/3204493.3204527">https://doi.org/10.1145/3204493.3204527</a></td>
            <td>@inproceedings{10.1145/3204493.3204527,<br>  author = {Reani, Manuele and Peek, Niels and Jay, Caroline},<br>  title = {An Investigation of the Effects of N-Gram Length in Scanpath Analysis for Eye-Tracking Research},<br>  year = {2018},<br>  isbn = {9781450357067},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3204493.3204527},<br>  doi = {10.1145/3204493.3204527},<br>  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  articleno = {1},<br>  numpages = {8},<br>  keywords = {scanpath analysis, n-gram analysis, human reasoning, eye-tracking methodology},<br>  location = {Warsaw, Poland},<br>  series = {ETRA '18}<br>  }</td>
            <td>no</td>
            <td>Tobii eye tracker not specified</td>
            <td>static</td>
            <td>screenbased</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>Age, Gender, Degree, scanpath and foixatiopn duration</td>
            <td>dwell time, fixation frequency, fixation duration</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>lab</td>
            <td>-</td>
            <td>-</td>
            <td>estimate<br>  the probability of rain given information about barometric pressure and historical weather data in a fictional city</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>49</td>
            <td>12 females</td>
            <td>37 males</td>
            <td>-</td>
            <td>-</td>
            <td>from the uni</td>
            <td>-</td>
            <td>16-36</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>ECG</td>
            <td>-</td>
            <td><a href="https://www.nature.com/articles/srep38227">https://www.nature.com/articles/srep38227</a></td>
            <td>@article{davies2016exploring,<br>  title={Exploring the relationship between eye movements and electrocardiogram interpretation accuracy},<br>  author={Davies, Alan and Brown, Gavin and Vigo, Markel and Harper, Simon and Horseman, Laura and Splendiani, Bruno and Hill, Elspeth and Jay, Caroline},<br>  journal={Scientific reports},<br>  volume={6},<br>  number={1},<br>  pages={38227},<br>  year={2016},<br>  publisher={Nature Publishing Group UK London}<br>  }</td>
            <td>2016</td>
            <td>no</td>
            <td>No</td>
            <td> </td>
            <td>An investigation of the effects of n-gram length in scanpath analysis for eye-tracking research</td>
            <td>2018</td>
            <td><a href="https://doi.org/10.1145/3204493.3204527">https://doi.org/10.1145/3204493.3204527</a></td>
            <td>@inproceedings{10.1145/3204493.3204527,<br>  author = {Reani, Manuele and Peek, Niels and Jay, Caroline},<br>  title = {An Investigation of the Effects of N-Gram Length in Scanpath Analysis for Eye-Tracking Research},<br>  year = {2018},<br>  isbn = {9781450357067},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3204493.3204527},<br>  doi = {10.1145/3204493.3204527},<br>  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  articleno = {1},<br>  numpages = {8},<br>  keywords = {scanpath analysis, n-gram analysis, human reasoning, eye-tracking methodology},<br>  location = {Warsaw, Poland},<br>  series = {ETRA '18}<br>  }</td>
            <td>no</td>
            <td>Tobii 1750 and Tobii X2-60 eye trackers</td>
            <td>static</td>
            <td>scree-based</td>
            <td>-</td>
            <td> 17\u201d monitor</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>fixations</td>
            <td> recording quality below 70% (the threshold recommended by the manufacturer, Tobii) were not included in the analysis</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>look at an ECG of a patient and infer<br>  the underlying medical condition</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>31</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>professionals</td>
            <td>healthcare practitioners</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Art</td>
            <td>-</td>
            <td><a href="https://bop.unibe.ch/JEMR/article/view/3742">https://bop.unibe.ch/JEMR/article/view/3742</a></td>
            <td>@article{davies2017does,<br>  title={Does descriptive text change how people look at art? A novel analysis of eye-movements using data-driven Units of Interest},<br>  author={Davies, Alan and Reani, Manuele and Vigo, Markel and Harper, Simon and Gannaway, Clare and Grimes, Martin and Jay, Caroline},<br>  journal={Journal of Eye Movement Research},<br>  volume={10},<br>  number={4},<br>  year={2017},<br>  publisher={European Group for Eye Movement Research}<br>  }</td>
            <td>2017</td>
            <td>no</td>
            <td>No</td>
            <td> </td>
            <td>An investigation of the effects of n-gram length in scanpath analysis for eye-tracking research</td>
            <td>2018</td>
            <td><a href="https://doi.org/10.1145/3204493.3204527">https://doi.org/10.1145/3204493.3204527</a></td>
            <td>@inproceedings{10.1145/3204493.3204527,<br>  author = {Reani, Manuele and Peek, Niels and Jay, Caroline},<br>  title = {An Investigation of the Effects of N-Gram Length in Scanpath Analysis for Eye-Tracking Research},<br>  year = {2018},<br>  isbn = {9781450357067},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3204493.3204527},<br>  doi = {10.1145/3204493.3204527},<br>  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  articleno = {1},<br>  numpages = {8},<br>  keywords = {scanpath analysis, n-gram analysis, human reasoning, eye-tracking methodology},<br>  location = {Warsaw, Poland},<br>  series = {ETRA '18}<br>  }</td>
            <td>no</td>
            <td> Tobii  X2-60 </td>
            <td>static</td>
            <td>scree-based</td>
            <td>-</td>
            <td>1366 x 768 pixels</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>fixation duration</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>look at 8 paintings, 10 seconds per paintingt</td>
            <td>-</td>
            <td>-</td>
            <td>sitting</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>42</td>
            <td>18 females</td>
            <td>24 males</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>normal or corrected vision</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>SEED-IV</td>
            <td>http://bcmi.sjtu.edu.cn/\u223cseed</td>
            <td><a href="https://ieeexplore.ieee.org/document/8283814">https://ieeexplore.ieee.org/document/8283814</a></td>
            <td>@article{zheng2018emotionmeter,<br>  title={Emotionmeter: A multimodal framework for recognizing human emotions},<br>  author={Zheng, Wei-Long and Liu, Wei and Lu, Yifei and Lu, Bao-Liang and Cichocki, Andrzej},<br>  journal={IEEE transactions on cybernetics},<br>  volume={49},<br>  number={3},<br>  pages={1110--1122},<br>  year={2018},<br>  publisher={IEEE}<br>  }</td>
            <td>2019</td>
            <td>link broken</td>
            <td>No</td>
            <td>adjuct</td>
            <td>Analyzing Transferability of Happiness Detection via Gaze Tracking in Multimedia Applications</td>
            <td>2020</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3379157.3391655">https://dl.acm.org/doi/10.1145/3379157.3391655</a></td>
            <td> </td>
            <td>no</td>
            <td>-</td>
            <td>-</td>
            <td>wearable</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>pupil diameter, disperssion, fixation duration, blink duration, saccades</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>were asked to assess<br>  their emotions when watching the film clips with keywords of<br>  emotions (happy, sad, neutral, and fear) and ratings out of<br>  ten points (from -5 to 5) for two dimensions: 1) valence and<br>  2) arousal. The valence scale ranges from sad to happy. The<br>  arousal scale ranges from calm to excited. in three sessions each had 14 trials</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>16</td>
            <td>8 females</td>
            <td>8 males</td>
            <td>-</td>
            <td>-</td>
            <td>college students</td>
            <td>-</td>
            <td>20-24</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>right handed</td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Perception</td>
            <td><a href="http://www.ti.uni-tuebingen.de/perception">http://www.ti.uni-tuebingen.de/perception</a></td>
            <td><a href="https://dl.acm.org/doi/10.1145/2857491.2857512">https://dl.acm.org/doi/10.1145/2857491.2857512</a></td>
            <td>@inproceedings{santini2016bayesian,<br>  title={Bayesian identification of fixations, saccades, and smooth pursuits},<br>  author={Santini, Thiago and Fuhl, Wolfgang and K{\"u}bler, Thomas and Kasneci, Enkelejda},<br>  booktitle={Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  pages={163--170},<br>  year={2016}<br>  }</td>
            <td>2016</td>
            <td>link broken</td>
            <td>Yes</td>
            <td>Full Paper</td>
            <td>Bayesian Identification of Fixations, Saccades, and Smooth Pursuits</td>
            <td>2016</td>
            <td><a href="https://doi.org/10.1145/2857491.2857512">https://doi.org/10.1145/2857491.2857512</a></td>
            <td>@inproceedings{10.1145/2857491.2857512,<br>  author = {Santini, Thiago and Fuhl, Wolfgang and K\"{u}bler, Thomas and Kasneci, Enkelejda},<br>  title = {Bayesian Identification of Fixations, Saccades, and Smooth Pursuits},<br>  year = {2016},<br>  isbn = {9781450341257},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/2857491.2857512},<br>  doi = {10.1145/2857491.2857512},<br>  booktitle = {Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  pages = {163\u2013170},<br>  numpages = {8},<br>  keywords = {online, open-source, dynamic stimuli, classification, eye-tracking, probabilistic, smooth pursuit, model},<br>  location = {Charleston, South Carolina},<br>  series = {ETRA '16}<br>  }</td>
            <td>no</td>
            <td>Dikablis Pro eye<br>  tracke</td>
            <td>static</td>
            <td>wearble</td>
            <td>30 hz</td>
            <td>Width: 520 mm. Height: 320 mm. Resolution: 1920x1200 pixels</td>
            <td>15 degress, target size 1 degree</td>
            <td>-</td>
            <td>"Saccadea<br>  straight Pursuita<br>  circular Pursuitb"</td>
            <td>-</td>
            <td>Saccadea<br>  straight Pursuita<br>  circular Pursuitb</td>
            <td>unjittering</td>
            <td>18,682 fixations, 1,296 saccades, and 4,143 smooth</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>chinrest</td>
            <td>look at these stimuli 5 dots</td>
            <td>-</td>
            <td>300 mm </td>
            <td>-</td>
            <td>-</td>
            <td>Luminance: 0.08 cd/m2</td>
            <td>-</td>
            <td>6</td>
            <td>2 females</td>
            <td>4 males</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>\u00b5 = 31.50, \u03c3 = 2.59 </td>
            <td>Two of the subjects wore corrective glasses for myopia (-13 dpt and 1.5 dpt).</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Simultaneous Measurement of Program Comprehension with fMRI and Eye Tracking</td>
            <td><a href="https://github.com/brains-on-code/simultaneous-fmri-and-eyetracking">https://github.com/brains-on-code/simultaneous-fmri-and-eyetracking</a></td>
            <td><a href="https://dl.acm.org/doi/10.1145/3216723.3216726">https://dl.acm.org/doi/10.1145/3216723.3216726</a></td>
            <td>@inproceedings{10.1145/3216723.3216726,<br>  author = {Peitek, Norman and Siegmund, Janet and Parnin, Chris and Apel, Sven and Brechmann, Andr\'{e}},<br>  title = {Beyond Gaze: Preliminary Analysis of Pupil Dilation and Blink Rates in an FMRI Study of Program Comprehension},<br>  year = {2018},<br>  isbn = {9781450357920},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3216723.3216726},<br>  doi = {10.1145/3216723.3216726},,<br>  booktitle = {Proceedings of the Workshop on Eye Movements in Programming},<br>  articleno = {4},<br>  numpages = {5},<br>  keywords = {pupil dilation, blink rates, eye tracking, functional magnetic resonance imaging, program comprehension},<br>  location = {Warsaw, Poland},<br>  series = {EMIP '18}<br>  }</td>
            <td>2018</td>
            <td>yes</td>
            <td>Yes</td>
            <td>EMIP Workshop</td>
            <td>Beyond gaze: preliminary analysis of pupil dilation and blink rates in an fMRI study of program comprehension</td>
            <td>2018</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3216723.3216726">https://dl.acm.org/doi/10.1145/3216723.3216726</a></td>
            <td>@inproceedings{10.1145/3216723.3216726,<br>  author = {Peitek, Norman and Siegmund, Janet and Parnin, Chris and Apel, Sven and Brechmann, Andr\'{e}},<br>  title = {Beyond Gaze: Preliminary Analysis of Pupil Dilation and Blink Rates in an FMRI Study of Program Comprehension},<br>  year = {2018},<br>  isbn = {9781450357920},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3216723.3216726},<br>  doi = {10.1145/3216723.3216726},<br>  location = {Warsaw, Poland},<br>  series = {EMIP '18}<br>  }</td>
            <td>no</td>
            <td>EyeLink eye-tracker</td>
            <td>static</td>
            <td>screenbased</td>
            <td>1000 hz</td>
            <td>-</td>
            <td>-</td>
            <td>&lt;.05</td>
            <td>fixations, blinks, saccades</td>
            <td>-</td>
            <td>fixations, blinks, saccades</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>determine the output of code snippets<br>  with given input values</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>22</td>
            <td>2 females</td>
            <td>20 males</td>
            <td>-</td>
            <td>-</td>
            <td>students </td>
            <td>-</td>
            <td>-</td>
            <td>26.7</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>GI4E</td>
            <td><a href="https://www.unavarra.es/gi4e/databases?languageId=1">https://www.unavarra.es/gi4e/databases?languageId=1</a></td>
            <td><a href="https://dl.acm.org/doi/abs/10.1145/2501643.2501647">https://dl.acm.org/doi/abs/10.1145/2501643.2501647</a><br><a href="https://dl.acm.org/doi/abs/10.1145/2501643.2501647">  </a></td>
            <td>@article{villanueva2013hybrid,<br>  title={Hybrid method based on topography for robust detection of iris center and eye corners},<br>  author={Villanueva, Arantxa and Ponz, Victoria and Sesma-Sanchez, Laura and Ariz, Mikel and Porta, Sonia and Cabeza, Rafael},<br>  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},<br>  volume={9},<br>  number={4},<br>  pages={1--20},<br>  year={2013},<br>  publisher={ACM New York, NY, USA}<br>  }</td>
            <td>2013</td>
            <td>yes</td>
            <td>No</td>
            <td>Short Paper</td>
            <td>BORE: boosted-oriented edge optimization for robust, real time remote pupil center detection</td>
            <td>2018</td>
            <td><a href="https://dl.acm.org/doi/pdf/10.1145/3204493.3204558">https://dl.acm.org/doi/pdf/10.1145/3204493.3204558</a></td>
            <td>@inproceedings{10.1145/3204493.3204558,<br>  author = {Fuhl, Wolfgang and Eivazi, Shahram and Hosp, Benedikt and Eivazi, Anna and Rosenstiel, Wolfgang and Kasneci, Enkelejda},<br>  title = {BORE: Boosted-Oriented Edge Optimization for Robust, Real Time Remote Pupil Center Detection},<br>  year = {2018},<br>  isbn = {9781450357067},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3204493.3204558},<br>  doi = {10.1145/3204493.3204558},<br>  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  articleno = {48},<br>  numpages = {5},<br>  keywords = {pupil center, boosting, remote eye tracking, pupil detection},<br>  location = {Warsaw, Poland},<br>  series = {ETRA '18}<br>  }</td>
            <td>no</td>
            <td>webcam</td>
            <td>static</td>
            <td>screenbased</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>face images</td>
            <td>-</td>
            <td>iris detection</td>
            <td>eyebrow removal</td>
            <td>1236 RGB images</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>gazes at 12 points uniformly distributed in the screen</td>
            <td>-</td>
            <td>25 cm</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>103</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>GazeBase</td>
            <td><a href="https://www.nature.com/articles/s41597-021-00959-y">ref 25 in here :https://www.nature.com/articles/s41597-021-00959-y</a></td>
            <td><a href="https://doi.org/10.1038/s41597-021-00959-y">https://doi.org/10.1038/s41597-021-00959-y</a></td>
            <td>@article{griffith2021gazebase,<br>  title={GazeBase, a large-scale, multi-stimulus, longitudinal eye movement dataset},<br>  author={Griffith, Henry and Lohr, Dillon and Abdulin, Evgeny and Komogortsev, Oleg},<br>  journal={Scientific Data},<br>  volume={8},<br>  number={1},<br>  pages={184},<br>  year={2021},<br>  publisher={Nature Publishing Group UK London}<br>  }</td>
            <td>2021</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Bridging the Gap: Gaze Events as Interpretable Concepts to Explain Deep Neural Sequence Models</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588412">https://doi.org/10.1145/3588015.3588412</a></td>
            <td>@inproceedings{krakowczyk2023bridging,<br>  title={Bridging the Gap: Gaze Events as Interpretable Concepts to Explain Deep Neural Sequence Models},<br>  author={Krakowczyk, Daniel G and Prasse, Paul and Reich, David R and Lapuschkin, Sebastian and Scheffer, Tobias and J{\"a}ger, Lena A},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--8},<br>  year={2023}<br>  }</td>
            <td>no</td>
            <td> EyeLink 1000 eye tracker</td>
            <td>static</td>
            <td>screen-based</td>
            <td>1000 hz</td>
            <td>1680\u2009\u00d7\u20091050 pixel </td>
            <td>-</td>
            <td>-</td>
            <td>eye movements + pupil</td>
            <td>-</td>
            <td>-</td>
            <td>thresholding and A nine-point validation process</td>
            <td>12,334 monocular eye-movement </td>
            <td> </td>
            <td>-</td>
            <td>yes</td>
            <td>chin and head rest</td>
            <td>(1) fixation task, (2) horizontal saccade task, (3) random oblique saccade task, (4) reading task, (5/6) free viewing of cinematic video task, and (7) gaze-driven gaming task. Nine rounds of recording were conducted over a 37 month period,</td>
            <td>-</td>
            <td>550\u2009mm</td>
            <td>sitting</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>322</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>college students</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>JuDo1000</td>
            <td><a href="https://osf.io/5zpvk/">https://osf.io/5zpvk/</a></td>
            <td><a href="https://ieeexplore.ieee.org/document/9304900">https://ieeexplore.ieee.org/document/9304900</a></td>
            <td>@inproceedings{makowski2020biometric,<br>  title={Biometric identification and presentation-attack detection using micro-and macro-movements of the eyes},<br>  author={Makowski, Silvia and J{\"a}ger, Lena A and Prasse, Paul and Scheffer, Tobias},<br>  booktitle={2020 IEEE International Joint Conference on Biometrics (IJCB)},<br>  pages={1--10},<br>  year={2020},<br>  organization={IEEE}<br>  }</td>
            <td>2020</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Bridging the Gap: Gaze Events as Interpretable Concepts to Explain Deep Neural Sequence Models</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588412">https://doi.org/10.1145/3588015.3588412</a></td>
            <td>@inproceedings{krakowczyk2023bridging,<br>  title={Bridging the Gap: Gaze Events as Interpretable Concepts to Explain Deep Neural Sequence Models},<br>  author={Krakowczyk, Daniel G and Prasse, Paul and Reich, David R and Lapuschkin, Sebastian and Scheffer, Tobias and J{\"a}ger, Lena A},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--8},<br>  year={2023}<br>  }</td>
            <td>no</td>
            <td>Eyelink<br>  portable Duo eye tracker</td>
            <td>static</td>
            <td>screen-based</td>
            <td>1000 hz</td>
            <td>1280\u00d71024 px)</td>
            <td>0.59 cm (20 px)</td>
            <td>-</td>
            <td>gaze position</td>
            <td>gaze position</td>
            <td>gaze position</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>yes</td>
            <td>chin- and forehead rest.</td>
            <td>Sequence<br>  of five dots that are positioned randomly on a screen and<br>  displayed for between 250 and 1000 ms each.</td>
            <td>-</td>
            <td>-</td>
            <td>sitting</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>150</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>18 to 46 years</td>
            <td>24</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Potec</td>
            <td><a href="https://osf.io/dn5hp/">https://osf.io/dn5hp/</a></td>
            <td>could not find any</td>
            <td>@article{jager2021potsdam,<br>  title={Potsdam textbook corpus (potec)},<br>  author={J{\"a}ger, Lena A and Kern, Thomas and Haller, Patrick},<br>  year={2021},<br>  publisher={Open Science Framework}<br>  }</td>
            <td>2021</td>
            <td> </td>
            <td>No</td>
            <td>Short</td>
            <td>Bridging the Gap: Gaze Events as Interpretable Concepts to Explain Deep Neural Sequence Models</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588412">https://doi.org/10.1145/3588015.3588412</a></td>
            <td>@inproceedings{krakowczyk2023bridging,<br>  title={Bridging the Gap: Gaze Events as Interpretable Concepts to Explain Deep Neural Sequence Models},<br>  author={Krakowczyk, Daniel G and Prasse, Paul and Reich, David R and Lapuschkin, Sebastian and Scheffer, Tobias and J{\"a}ger, Lena A},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--8},<br>  year={2023}<br>  }</td>
            <td>no</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>1000 hz</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>eye movements</td>
            <td> </td>
            <td> </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>read a series of German short texts</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>75</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>GazeCapture</td>
            <td><a href="https://paperswithcode.com/dataset/gazecapture">https://paperswithcode.com/dataset/gazecapture</a></td>
            <td><a href="https://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf">https://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf</a></td>
            <td>@inproceedings{krafka2016eye,<br>  title={Eye tracking for everyone},<br>  author={Krafka, Kyle and Khosla, Aditya and Kellnhofer, Petr and Kannan, Harini and Bhandarkar, Suchendra and Matusik, Wojciech and Torralba, Antonio},<br>  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},<br>  pages={2176--2184},<br>  year={2016}<br>  }</td>
            <td>2016</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Calibration Error Prediction: Ensuring High-Quality Mobile Eye-Tracking</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3517031.3529634">https://doi.org/10.1145/3517031.3529634</a></td>
            <td>@inproceedings{li2022calibration,<br>  title={Calibration Error Prediction: Ensuring High-Quality Mobile Eye-Tracking},<br>  author={Li, Beibin and Snider, James C and Wang, Quan and Mehta, Sachin and Foster, Claire and Barney, Erin and Shapiro, Linda and Ventola, Pamela and Shic, Frederick},<br>  booktitle={2022 Symposium on Eye Tracking Research and Applications},<br>  pages={1--7},<br>  year={2022}<br>  }</td>
            <td>no</td>
            <td>phone front camera</td>
            <td>mobile</td>
            <td>phone</td>
            <td>-</td>
            <td>1249 subjects used<br>  iPhones while 225 used iPads, r</td>
            <td>-</td>
            <td>error of 1.34cm<br>  and 2.12cm on mobile phones and tablets,</td>
            <td>face images</td>
            <td>face images</td>
            <td>gazed prediction on screen</td>
            <td>yes</td>
            <td>2, 445, 504 frames</td>
            <td>phone camera</td>
            <td>in the wild</td>
            <td>13 points for calibration, </td>
            <td>-</td>
            <td>60 dots 2<br>  for each orientation of the device</td>
            <td>10 mins</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>it was a study factor</td>
            <td>1474</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Gaze in Wild (GiW) dataset</td>
            <td><a href="https://www.cis.rit.edu/~rsk3900/gaze-in-wild/">https://www.cis.rit.edu/~rsk3900/gaze-in-wild/</a></td>
            <td><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7018838/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7018838/</a></td>
            <td>@article{kothari2020gaze,<br>  title={Gaze-in-wild: A dataset for studying eye and head coordination in everyday activities},<br>  author={Kothari, Rakshit and Yang, Zhizhuo and Kanan, Christopher and Bailey, Reynold and Pelz, Jeff B and Diaz, Gabriel J},<br>  journal={Scientific reports},<br>  volume={10},<br>  number={1},<br>  pages={2539},<br>  year={2020},<br>  publisher={Nature Publishing Group UK London}<br>  }</td>
            <td>2020</td>
            <td>yes</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Characterizing the Performance of Deep Neural Networks for Eye-Tracking</td>
            <td>2021</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3450341.3458491">https://dl.acm.org/doi/10.1145/3450341.3458491</a></td>
            <td> </td>
            <td>no</td>
            <td> binocular Pupil Labs eye tracking glasse</td>
            <td>mobie</td>
            <td>wearable</td>
            <td>120 hz</td>
            <td>-</td>
            <td>real life tasks so no need of a visual angle</td>
            <td>-</td>
            <td>gaze movements</td>
            <td>trained classifier</td>
            <td>fixations, gaze pursuits and saccades</td>
            <td>blink removal</td>
            <td>-</td>
            <td>werable front camera</td>
            <td>lab - semi controlled</td>
            <td>yes</td>
            <td>-</td>
            <td>Indoor navigation, Ball catching, Object search without prior subject-object interaction:, Tea making:</td>
            <td>3 minutes x 4 tasks</td>
            <td>dosnot apply</td>
            <td>walking</td>
            <td>-</td>
            <td>doensot apply</td>
            <td>-</td>
            <td>19</td>
            <td>7 female</td>
            <td>12 males</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> \u03bc\u2009=\u200928, \u03c3\u2009=\u200912.52</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Classification of Alzheimer\u2019s Disease Leveraging Multi-task Machine Learning Analysis of Speech and Eye-Movement Data</td>
            <td>not published due to ethical considerations</td>
            <td><a href="https://www.frontiersin.org/articles/10.3389/fnhum.2021.716670/full#h8">https://www.frontiersin.org/articles/10.3389/fnhum.2021.716670/full#h8</a></td>
            <td>@article{jang2021classification,<br>  title={Classification of Alzheimer\u2019s disease leveraging multi-task machine learning analysis of speech and eye-movement data},<br>  author={Jang, Hyeju and Soroski, Thomas and Rizzo, Matteo and Barral, Oswald and Harisinghani, Anuj and Newton-Mason, Sally and Granby, Saffrin and Stutz da Cunha Vasco, Thiago Monnerat and Lewis, Caitlin and Tutt, Pavan and others},<br>  journal={Frontiers in Human Neuroscience},<br>  volume={15},<br>  pages={716670},<br>  year={2021},<br>  publisher={Frontiers Media SA}<br>  }</td>
            <td>2021</td>
            <td>no</td>
            <td>No</td>
            <td>Long</td>
            <td>Classification of Alzheimer\u2019s using Deep-learning Methods on Webcam-based Gaze Data</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3591126">https://doi.org/10.1145/3591126</a></td>
            <td>@article{harisinghani2023classification,<br>  title={Classification of Alzheimer's using Deep-learning Methods on Webcam-based Gaze Data},<br>  author={Harisinghani, Anuj and Sriram, Harshinee and Conati, Cristina and Carenini, Giuseppe and Field, Thalia and Jang, Hyeju and Murray, Gabriel},<br>  journal={Proceedings of the ACM on Human-Computer Interaction},<br>  volume={7},<br>  number={ETRA},<br>  pages={1--17},<br>  year={2023},<br>  publisher={ACM New York, NY, USA}<br>  }</td>
            <td>did not publish the dataset</td>
            <td>web cam + Tobii Pro X3-120</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> fixations, saccades, and pupil size data.</td>
            <td> </td>
            <td> fixations, saccades, and pupil size data.</td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>pupil calibration, picture description, paragraph reading, and memory recall.</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>79</td>
            <td> </td>
            <td> </td>
            <td>48 with mild to moderate AD, 22 with MCI, nine with SMC) and 83 healthy volunteers </td>
            <td> </td>
            <td>memory clinic patients (</td>
            <td> </td>
            <td>50+</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>patients with  visual abnormalities were excluded</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>EYEGAZE</td>
            <td><a href="https://github.com/cxr-eye-gaze/eye-gaze-dataset">https://github.com/cxr-eye-gaze/eye-gaze-dataset</a></td>
            <td><a href="https://www.nature.com/articles/s41597-021-00863-5#citeas">https://www.nature.com/articles/s41597-021-00863-5#citeas</a></td>
            <td>@misc{Karargyris2020,<br>  author = {Karargyris, Alexandros and Kashyap, Satyananda and Lourentzou, Ismini and Wu, Joy and Tong, Matthew and Sharma, Arjun and Abedin, Shafiq and Beymer, David and Mukherjee, Vandana and Krupinski, Elizabeth and Moradi, Mehdi},<br>  booktitle = {Physionet},<br>  doi = {https://doi.org/10.13026/qfdz-zr67},<br>  title = {{Eye Gaze Data for Chest X-rays (version 1.0.0)}},<br>  url = {https://physionet.org/content/egd-cxr/1.0.0/},<br>  year = {2020}<br>  }</td>
            <td>2021</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Comparing Visual Search Patterns in Chest X-Ray Diagnostics</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588403">https://doi.org/10.1145/3588015.3588403</a></td>
            <td>@inproceedings{moreira2023comparing,<br>  title={Comparing Visual Search Patterns in Chest X-Ray Diagnostics},<br>  author={Moreira, Catarina and Alvito, Diogo Miguel and Sousa, Sandra Costa and Nobre, Isabel Maria Gomes Blanco and Ouyang, Chun and Kopper, Regis and Duchowski, Andrew and Jorge, Joaquim},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--6},<br>  year={2023}<br>  }</td>
            <td>-</td>
            <td>Gazepoint GP3 Eye Tracker</td>
            <td>static</td>
            <td>screen based</td>
            <td>-</td>
            <td> (Dell S2719DGF) set at 1920\u2009\u00d7\u20091080 resolution</td>
            <td>-</td>
            <td>-</td>
            <td>raw data</td>
            <td>-</td>
            <td>fixations and heatmaps</td>
            <td>-</td>
            <td>eye data over viewing 1,083 CXR images.</td>
            <td>-</td>
            <td>-</td>
            <td>9-point calibration</td>
            <td>-</td>
            <td>view Chest X-Ray (CXR) images</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>they only reported teh age of the patients in the pictures</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Dataset for Eye Tracking on a Virtual Reality Platform</td>
            <td>Not sure</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3379155.3391317">https://dl.acm.org/doi/10.1145/3379155.3391317</a></td>
            <td>@inproceedings{10.1145/3379155.3391317,<br>  author = {Garbin, Stephan Joachim and Komogortsev, Oleg and Cavin, Robert and Hughes, Gregory and Shen, Yiru and Schuetz, Immo and Talathi, Sachin S},<br>  title = {Dataset for Eye Tracking on a Virtual Reality Platform},<br>  year = {2020},<br>  isbn = {9781450371339},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3379155.3391317},<br>  doi = {10.1145/3379155.3391317},<br>  booktitle = {ACM Symposium on Eye Tracking Research and Applications},<br>  articleno = {13},<br>  numpages = {10},<br>  keywords = {segmentation, pupil, virtual reality, eye tracking, sclera, iris, appearance-based eye tracking},<br>  location = {Stuttgart, Germany},<br>  series = {ETRA '20 Full Papers}<br>  }</td>
            <td>2020</td>
            <td>no</td>
            <td>Yes</td>
            <td>Long</td>
            <td>Dataset for Eye Tracking on a Virtual Reality Platform</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379155.3391317">https://doi.org/10.1145/3379155.3391317</a></td>
            <td>@inproceedings{garbin2020dataset,<br>  title={Dataset for eye tracking on a virtual reality platform},<br>  author={Garbin, Stephan Joachim and Komogortsev, Oleg and Cavin, Robert and Hughes, Gregory and Shen, Yiru and Schuetz, Immo and Talathi, Sachin S},<br>  booktitle={ACM symposium on eye tracking research and applications},<br>  pages={1--10},<br>  year={2020}<br>  }</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>VR</td>
            <td>200 Hz</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>raw pupil images</td>
            <td>pupil images</td>
            <td>-</td>
            <td>-</td>
            <td>356,649 pupil images</td>
            <td>-</td>
            <td>VR</td>
            <td>-</td>
            <td>-</td>
            <td>a combination of explicit gaze target fixation task, smooth<br>  pursuit task, and free-viewing task in VR, separated by further<br>  breaks</td>
            <td>20 minutes x two sessions</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>yes</td>
            <td>152</td>
            <td>82 female</td>
            <td>70 male</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>19 -65</td>
            <td>-</td>
            <td>6 with glasses</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Audio Visual Eyetracking (AVE)</td>
            <td><a href="https://hrtavakoli.github.io/AVE/">https://hrtavakoli.github.io/AVE/</a></td>
            <td><a href="https://dl.acm.org/doi/10.1145/3379156.3391337">https://dl.acm.org/doi/10.1145/3379156.3391337</a></td>
            <td>@inproceedings{10.1145/3379156.3391337,<br>  author = {Tavakoli, Hamed Rezazadegan and Borji, Ali and Kannala, Juho and Rahtu, Esa},<br>  title = {Deep Audio-Visual Saliency: Baseline Model and Data},<br>  year = {2020},<br>  isbn = {9781450371346},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3379156.3391337},<br>  doi = {10.1145/3379156.3391337},<br>  booktitle = {ACM Symposium on Eye Tracking Research and Applications},<br>  articleno = {23},<br>  numpages = {5},<br>  keywords = {Dynamic Visual Attention, Audio-Visual Saliency, Deep Learning},<br>  location = {Stuttgart, Germany},<br>  series = {ETRA '20 Short Papers}<br>  }</td>
            <td>2020</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Short</td>
            <td>Deep Audio-Visual Saliency: Baseline Model and Data</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391337">https://doi.org/10.1145/3379156.3391337</a></td>
            <td>@inproceedings{tavakoli2020deep,<br>  title={Deep audio-visual saliency: Baseline model and data},<br>  author={Tavakoli, Hamed Rezazadegan and Borji, Ali and Kannala, Juho and Rahtu, Esa},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td>NO</td>
            <td>SR-EyeLink1000</td>
            <td>static</td>
            <td>screenbased</td>
            <td>100 hz</td>
            <td> (21" CRT monitor, screen res=1024 x 768,  screen size=400x300, 75 Hz</td>
            <td>-</td>
            <td>-</td>
            <td>eye movements</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>free-viewing<br>  task</td>
            <td>-</td>
            <td> 57 cm </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>72</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>yes</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>CARPE </td>
            <td><a href="https://thediemproject.wordpress.com/videos-and%c2%a0data/">https://thediemproject.wordpress.com/videos-and%c2%a0data/</a></td>
            <td><a href="https://link.springer.com/article/10.1007/s12559-010-9074-z">https://link.springer.com/article/10.1007/s12559-010-9074-z</a></td>
            <td>@article{mital2011clustering,<br>  title={Clustering of gaze during dynamic scene viewing is predicted by motion},<br>  author={Mital, Parag K and Smith, Tim J and Hill, Robin L and Henderson, John M},<br>  journal={Cognitive computation},<br>  volume={3},<br>  pages={5--24},<br>  year={2011},<br>  publisher={Springer}<br>  }</td>
            <td>2011</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Deep Audio-Visual Saliency: Baseline Model and Data</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391337">https://doi.org/10.1145/3379156.3391337</a></td>
            <td>@inproceedings{tavakoli2020deep,<br>  title={Deep audio-visual saliency: Baseline model and data},<br>  author={Tavakoli, Hamed Rezazadegan and Borji, Ali and Kannala, Juho and Rahtu, Esa},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td>-</td>
            <td>Eyelink 2000</td>
            <td>static</td>
            <td>screen based</td>
            <td>1000 hz</td>
            <td>21\u2033 Viewsonic Monitor with desktop resolution 1,280 \u00d7 960@120 Hz</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>fixation position, heatmaps</td>
            <td>yes</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>yes</td>
            <td>chin and headrest</td>
            <td>video free viewing</td>
            <td>45 mins</td>
            <td>90 cm</td>
            <td>-</td>
            <td>-</td>
            <td>was a study factor</td>
            <td>-</td>
            <td>42</td>
            <td>25 female</td>
            <td>17 males</td>
            <td>-</td>
            <td>yes</td>
            <td>Student and Graduate Employment Service.</td>
            <td>-</td>
            <td> 18 to 36 </td>
            <td>23</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>How saliency, faces, and sound influence gaze in dynamic social scenes</td>
            <td><a href="http://www.gipsa-lab.fr/~antoine.coutrot/">http://www.gipsa-lab.fr/~antoine.coutrot/</a></td>
            <td><a href="https://jov.arvojournals.org/article.aspx?articleid=2243926#87557949">https://jov.arvojournals.org/article.aspx?articleid=2243926#87557949</a></td>
            <td>@article{coutrot2014saliency,<br>  title={How saliency, faces, and sound influence gaze in dynamic social scenes},<br>  author={Coutrot, Antoine and Guyader, Nathalie},<br>  journal={Journal of vision},<br>  volume={14},<br>  number={8},<br>  pages={5--5},<br>  year={2014},<br>  publisher={The Association for Research in Vision and Ophthalmology}<br>  }</td>
            <td>2014</td>
            <td>broken link</td>
            <td>No</td>
            <td>Short</td>
            <td>Deep Audio-Visual Saliency: Baseline Model and Data</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391337">https://doi.org/10.1145/3379156.3391337</a></td>
            <td>@inproceedings{tavakoli2020deep,<br>  title={Deep audio-visual saliency: Baseline model and data},<br>  author={Tavakoli, Hamed Rezazadegan and Borji, Ali and Kannala, Juho and Rahtu, Esa},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td>no</td>
            <td>Eyelink 1000,</td>
            <td>static</td>
            <td>screen based</td>
            <td>1000 hz</td>
            <td>-</td>
            <td>0.01</td>
            <td>-</td>
            <td>Eye positions, fixations, saccades, face labelling</td>
            <td>-</td>
            <td>saccadic amplitudes, fixation duration, Dispersion, temporal analysis, scanpath</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>9 point</td>
            <td>-</td>
            <td>viewed 15 different conversation scenes</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>72</td>
            <td>30 women</td>
            <td>42 men</td>
            <td>-</td>
            <td>-</td>
            <td>french natives</td>
            <td>-</td>
            <td>20 -35</td>
            <td>(M = 23.5; SD = 2.1</td>
            <td>normal to corrected</td>
            <td>-</td>
            <td>-</td>
            <td>left to right</td>
            <td>-</td>
            <td>yes</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>CAT2000</td>
            <td><a href="http://saliency.mit.edu/results_cat2000.html">http://saliency.mit.edu/results_cat2000.html</a></td>
            <td><a href="https://arxiv.org/abs/1505.03581">https://arxiv.org/abs/1505.03581</a></td>
            <td>@article{CAT2000,<br>   title   = {CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research},<br>   author  = {Borji, Ali and Itti, Laurent},<br>   journal = {CVPR 2015 workshop on "Future of Datasets"},<br>   year  = {2015},<br>   note  = {arXiv preprint arXiv:1505.03581}<br>  }</td>
            <td>2012</td>
            <td>yes</td>
            <td>No</td>
            <td>Short Paper</td>
            <td>Deepcomics: saliency estimation for comics</td>
            <td>2018</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3204493.3204560">https://dl.acm.org/doi/10.1145/3204493.3204560</a></td>
            <td>@inproceedings{10.1145/3204493.3204560,<br>  author = {Bannier, K\'{e}vin and Jain, Eakta and Meur, Olivier Le},<br>  title = {Deepcomics: Saliency Estimation for Comics},<br>  year = {2018},<br>  isbn = {9781450357067},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3204493.3204560},<br>  doi = {10.1145/3204493.3204560},<br>  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  articleno = {49},<br>  numpages = {5},<br>  keywords = {saliency, eye-movements, deep learning, comics},<br>  location = {Warsaw, Poland},<br>  series = {ETRA '18}<br>  }</td>
            <td>NO</td>
            <td>Eyelink-1000 </td>
            <td>static</td>
            <td>screenbased</td>
            <td>1000 hz</td>
            <td>42 inch LCD monitor</td>
            <td>45.5<br>  \u25e6 \u00d7<br>  31\u25e6</td>
            <td>0.5</td>
            <td>fixations</td>
            <td>fixations</td>
            <td>-</td>
            <td>-</td>
            <td>4,000<br>  images , 24,148,768 saccades over 240 hours of<br>  viewing time</td>
            <td>-</td>
            <td>-</td>
            <td>yes</td>
            <td>chinrest</td>
            <td>image viewing</td>
            <td>25 minutes followed by 5 minutes rest. </td>
            <td>106 cm</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>120</td>
            <td>800 female</td>
            <td>40 male</td>
            <td>-</td>
            <td> </td>
            <td>underaguate students</td>
            <td>different majors</td>
            <td>18 - 27</td>
            <td> 20.15, std<br>  1.65, median 20)</td>
            <td>normal to corrected</td>
            <td>different ethnicities</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Webpage Saliency</td>
            <td><a href="https://www-users.cse.umn.edu/~qzhao/webpage_saliency.html">https://www-users.cse.umn.edu/~qzhao/webpage_saliency.html</a></td>
            <td><a href="https://link.springer.com/chapter/10.1007/978-3-319-10584-0_3">  </a><a href="https://link.springer.com/chapter/10.1007/978-3-319-10584-0_3">https://link.springer.com/chapter/10.1007/978-3-319-10584-0_3</a></td>
            <td>@inproceedings{shen2014webpage,<br>  title={Webpage Saliency},<br>  author={Shen, Chengyao and Zhao, Qi},<br>  booktitle={ECCV},<br>  year={2014},<br>  organization={IEEE}<br>  }</td>
            <td>2014</td>
            <td>yes</td>
            <td>No</td>
            <td>Short Paper</td>
            <td>Deepcomics: saliency estimation for comics</td>
            <td>2018</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3204493.3204560">https://dl.acm.org/doi/10.1145/3204493.3204560</a></td>
            <td>@inproceedings{10.1145/3204493.3204560,<br>  author = {Bannier, K\'{e}vin and Jain, Eakta and Meur, Olivier Le},<br>  title = {Deepcomics: Saliency Estimation for Comics},<br>  year = {2018},<br>  isbn = {9781450357067},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3204493.3204560},<br>  doi = {10.1145/3204493.3204560},<br>  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  articleno = {49},<br>  numpages = {5},<br>  keywords = {saliency, eye-movements, deep learning, comics},<br>  location = {Warsaw, Poland},<br>  series = {ETRA '18}<br>  }</td>
            <td>NO</td>
            <td> Eyelink 1000</td>
            <td>static</td>
            <td>screenbased</td>
            <td>1000 hz</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>fixations</td>
            <td>fixations</td>
            <td>fixations and heatmaps</td>
            <td>gausian filter</td>
            <td>11 partcipants x 149 webpages</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>chin and headrest</td>
            <td>free webpage viewing</td>
            <td>5 seconds per webpage</td>
            <td>60 cm</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>dark room</td>
            <td>11</td>
            <td>7 females</td>
            <td>4 males</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>21 to 25<br>  </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>ZJU</td>
            <td><a href="https://www.blinkingmatters.com/research">https://www.blinkingmatters.com/research</a></td>
            <td><a href="https://cdn.intechopen.com/pdfs/5896/InTech-Liveness-detection-for-face-recognition.pdf">https://cdn.intechopen.com/pdfs/5896/InTech-Liveness-detection-for-face-recognition.pdf</a></td>
            <td>@article{pan2008liveness,<br>  title={Liveness detection for face recognition},<br>  author={Pan, Gang and Wu, Zhaohui and Sun, Lin},<br>  journal={Recent advances in face recognition},<br>  volume={109},<br>  pages={124},<br>  year={2008},<br>  publisher={InTech Vukovar, Croatia}<br>  }</td>
            <td>2008</td>
            <td>yes</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Detecting Blinks from Wearable Cameras using<br>  Spatial-Temporal-Aware Deep Network Learning</td>
            <td>2023</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3588015.3589668">https://dl.acm.org/doi/10.1145/3588015.3589668</a></td>
            <td> </td>
            <td>-</td>
            <td> Logitech Pro 5000 camera</td>
            <td>static</td>
            <td>screenbased</td>
            <td>30 fps</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>eye images</td>
            <td>eye images</td>
            <td>blinking</td>
            <td>-</td>
            <td>1,016<br>  labeled images of close eyes (positive samples) and 1,200 images of open eyes (negative<br>  samples), 255 blinks</td>
            <td>they used a camera not and eye tracker</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>blinking spontaneously</td>
            <td>5 seconds of natural eye behavior and blinking</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>no</td>
            <td>20</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td><a href="https://github.com/alessandrobruno10/WECVD">WECVD</a></td>
            <td><a href="https://github.com/alessandrobruno10/WECVD">https://github.com/alessandrobruno10/WECVD</a></td>
            <td><a href="https://dl.acm.org/doi/10.1145/3588015.3590133">https://dl.acm.org/doi/10.1145/3588015.3590133</a></td>
            <td>@inproceedings{10.1145/3588015.3590133,<br>  author = {Bruno, Alessandro and Tliba, Marouane and Kerkouri, Mohamed Amine and Chetouani, Aladine and Giunta, Carlo Calogero and \c{C}\"{o}ltekin, Arzu},<br>  title = {Detecting Colour Vision Deficiencies via Webcam-Based Eye-Tracking: A Case Study},<br>  year = {2023},<br>  isbn = {9798400701504},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3588015.3590133},<br>  doi = {10.1145/3588015.3590133},<br>  booktitle = {Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  articleno = {39},<br>  numpages = {2},<br>  location = {Tubingen, Germany},<br>  series = {ETRA '23}<br>  }</td>
            <td>2013</td>
            <td>yes</td>
            <td>Yes</td>
            <td>adjunct</td>
            <td>Detecting colour vision deficiencies via Webcam-based<br>  Eye-tracking: A case study<br>  </td>
            <td>2023</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3588015.3590133">https://dl.acm.org/doi/10.1145/3588015.3590133</a></td>
            <td> </td>
            <td>-</td>
            <td>webcam using realeye</td>
            <td>static</td>
            <td>screenbased</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>fixations</td>
            <td>heatmaps</td>
            <td>fixations</td>
            <td>-</td>
            <td>14 images</td>
            <td>they used a camera not and eye tracker</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> free viewing<br>  task on fourteen images</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>12</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> six dichromatic participants (3 protanopes and 3 deuteranopes) and six normal vision </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> six dichromatic participants (3 protanopes and 3 deuteranopes) and six normal vision </td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>Eve</td>
            <td><a href="https://competitions.codalab.org/competitions/28954">https://competitions.codalab.org/competitions/28954</a></td>
            <td><a href="https://arxiv.org/abs/2007.13120">https://arxiv.org/abs/2007.13120</a></td>
            <td>@inproceedings{Park2020ECCV,<br>  author  = {Seonwook Park and Emre Aksan and Xucong Zhang and Otmar Hilliges},<br>  title   = {Towards End-to-end Video-based Eye-Tracking},<br>  year  = {2020},<br>  booktitle = {European Conference on Computer Vision (ECCV)}<br>  }</td>
            <td>2020</td>
            <td>link broken</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Discussing the importance of calibration in low-cost gaze<br>  estimation solutions</td>
            <td>2023</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3588015.3589539">https://dl.acm.org/doi/10.1145/3588015.3589539</a></td>
            <td> </td>
            <td>-</td>
            <td> Tobii Pro Spectrum eye tracker, Basler Ace acA1920-150uc  camera, three Logitech C922 webcams</td>
            <td>static</td>
            <td>screenbased</td>
            <td>1200, 60, 30hz</td>
            <td> 1920 \u00d7 1080</td>
            <td>-</td>
            <td>-</td>
            <td>eye moevemnts + video of participants' faces </td>
            <td>time-synchronized screen recordings, user-facing camera views, and eye gaze dat</td>
            <td>gaze estimation</td>
            <td>yes</td>
            <td>12.3M frames over a large set of visual stimuli (1004 images, 161 videos, and 162 wikipedia pages)</td>
            <td>yes</td>
            <td>lab</td>
            <td>yes</td>
            <td>no</td>
            <td>watch images, videos and read wikipedia pages</td>
            <td>60 image<br>  stimuli (for three seconds each), at least 12 minutes of video stimuli, and six<br>  minutes of wikipedia stimulus (three 2-minute sessions).</td>
            <td>measured metric</td>
            <td>sitting</td>
            <td>-</td>
            <td>-</td>
            <td>indirect illumination and blocked any bright<br>  or direct sources of light with black tape or tissue paper</td>
            <td>54</td>
            <td>23 female</td>
            <td>30 mles</td>
            <td>1 unspecified</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>I2Head</td>
            <td>could not find a link</td>
            <td><a href="https://dl.acm.org/doi/abs/10.1145/3208031.3208033">https://dl.acm.org/doi/abs/10.1145/3208031.3208033</a></td>
            <td>@inproceedings{10.1145/3208031.3208033,<br>  author = {Martinikorena, Ion and Cabeza, Rafael and Villanueva, Arantxa and Porta, Sonia},<br>  title = {Introducing I2head Database},<br>  year = {2018},<br>  isbn = {9781450357890},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3208031.3208033},<br>  doi = {10.1145/3208031.3208033},<br>  booktitle = {Proceedings of the 7th Workshop on Pervasive Eye Tracking and Mobile Eye-Based Interaction},<br>  articleno = {1},<br>  numpages = {7},<br>  keywords = {low cost gaze estimation, head pose estimation, database, gaze estimation evaluation},<br>  location = {Warsaw, Poland},<br>  series = {PETMEI '18}<br>  }</td>
            <td>2018</td>
            <td>no</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Discussing the importance of calibration in low-cost gaze<br>  estimation solutions</td>
            <td>2023</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3588015.3589540">https://dl.acm.org/doi/10.1145/3588015.3589540</a></td>
            <td> </td>
            <td>no</td>
            <td>Logitech webcam</td>
            <td>static</td>
            <td>screenbased</td>
            <td>30 hz</td>
            <td>1280x720 pixel</td>
            <td>-</td>
            <td>-</td>
            <td>images of users</td>
            <td>-</td>
            <td>gaze and head estimation</td>
            <td>-</td>
            <td> 94 videos</td>
            <td>yes</td>
            <td>-</td>
            <td>yes</td>
            <td>no but they were asked to not move</td>
            <td>point on two grids with different sizes,  17 and 65 points</td>
            <td>-</td>
            <td>60 cm</td>
            <td>sitting</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>DynamicRead</td>
            <td><a href="https://doi.org/10.1145/3591127">https://doi.org/10.1145/3591127</a></td>
            <td><a href="https://zenodo.org/record/7806945">https://zenodo.org/record/7806945</a></td>
            <td>@article{10.1145/3591127,<br>  author = {Lei, Yaxiong and Wang, Yuheng and Caslin, Tyler and Wisowaty, Alexander and Zhu, Xu and Khamis, Mohamed and Ye, Juan},<br>  title = {DynamicRead: Exploring Robust Gaze Interaction Methods for Reading on Handheld Mobile Devices under Dynamic Conditions},<br>  year = {2023},<br>  issue_date = {May 2023},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  volume = {7},<br>  number = {ETRA},<br>  url = {https://doi.org/10.1145/3591127},<br>  doi = {10.1145/3591127},<br>  journal = {Proc. ACM Hum.-Comput. Interact.},<br>  month = {may},<br>  articleno = {158},<br>  numpages = {17},<br>  keywords = {gaze-based interaction, dwell, eye tracking, smartphones, reading, scrolling techniques, mobile devices, gaze gesture, pursuit}<br>  }</td>
            <td>2023</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Long</td>
            <td>DynamicRead: Exploring Robust Gaze Interaction Methods for Reading on Handheld Mobile Devices under Dynamic Conditions</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3591127">https://doi.org/10.1145/3591127</a></td>
            <td>@article{lei2023dynamicread,<br>  title={DynamicRead: Exploring Robust Gaze Interaction Methods for Reading on Handheld Mobile Devices under Dynamic Conditions},<br>  author={Lei, Yaxiong and Wang, Yuheng and Caslin, Tyler and Wisowaty, Alexander and Zhu, Xu and Khamis, Mohamed and Ye, Juan},<br>  journal={Proceedings of the ACM on Human-Computer Interaction},<br>  volume={7},<br>  number={ETRA},<br>  pages={1--17},<br>  year={2023},<br>  publisher={ACM New York, NY, USA}<br>  }</td>
            <td>yes</td>
            <td>iphone front camera with iTracker</td>
            <td>mobile</td>
            <td>phone</td>
            <td>-</td>
            <td>(6.7-inch,</td>
            <td>-</td>
            <td>-</td>
            <td>gaze data</td>
            <td>heatmaps, scanpath, gaze data</td>
            <td>gaze patterns</td>
            <td>-</td>
            <td> </td>
            <td>yes</td>
            <td>lab</td>
            <td>-</td>
            <td>-</td>
            <td> four gaze-based scrolling techniques</td>
            <td>1.5 hours</td>
            <td>-</td>
            <td>sitting and walking</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>20</td>
            <td>7 female</td>
            <td>13 male</td>
            <td>-</td>
            <td>10 pounds</td>
            <td>undergraduate and postgraduate</td>
            <td>-</td>
            <td>20-32</td>
            <td>25.05</td>
            <td>14 with glasses</td>
            <td>-</td>
            <td>yes</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>left and right eye myopia</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td> </td>
              </tr>
              <tr>
            <td>NVGaze</td>
            <td><a href="https://research.nvidia.com/publication/2019-05_nvgaze-anatomically-informed-dataset-low-latency-near-eye-gaze-estimation">https://research.nvidia.com/publication/2019-05_nvgaze-anatomically-informed-dataset-low-latency-near-eye-gaze-estimation</a></td>
            <td><a href="https://dl.acm.org/doi/abs/10.1145/3290605.3300780">https://dl.acm.org/doi/abs/10.1145/3290605.3300780</a></td>
            <td> </td>
            <td>2019</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>EllSeg-Gen, towards Domain Generalization for head-mounted eyetracking</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3530880">https://doi.org/10.1145/3530880</a></td>
            <td>@article{kothari2022ellseg, title={EllSeg-Gen, towards Domain Generalization for head-mounted eyetracking}, author={Kothari, Rakshit S and Bailey, Reynold J and Kanan, Christopher and Pelz, Jeff B and Diaz, Gabriel J}, journal={Proceedings of the ACM on Human-Computer Interaction}, volume={6}, number={ETRA}, pages={1--17}, year={2022}, publisher={ACM New York, NY, USA}}</td>
            <td> </td>
            <td>NIR-camera</td>
            <td>mobile</td>
            <td>VR</td>
            <td>120</td>
            <td>640 x 480</td>
            <td> </td>
            <td> </td>
            <td>gaze direction and pupil size</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>Two emulations in lab: One for VR one for AR</td>
            <td> </td>
            <td>yes</td>
            <td>Identify orientation of letters</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>variable</td>
            <td>30</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gender, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td> </td>
            <td> </td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td>Not specific: "captured images from 30 subjects with variation in gen-<br>  der, ethnicity, age, eye shape, and face shape"</td>
            <td> </td>
              </tr>
              <tr>
            <td>Swirski</td>
            <td><a href="https://www.cl.cam.ac.uk/research/rainbow/projects/pupiltracking/">https://www.cl.cam.ac.uk/research/rainbow/projects/pupiltracking/</a></td>
            <td><a href="https://dl.acm.org/doi/abs/10.1145/2168556.2168585">  https://dl.acm.org/doi/abs/10.1145/2168556.2168585</a><br><a href="https://dl.acm.org/doi/abs/10.1145/2168556.2168585">  </a></td>
            <td> </td>
            <td>2012</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>EllSeg-Gen, towards Domain Generalization for head-mounted eyetracking</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3530880">https://doi.org/10.1145/3530880</a></td>
            <td>@article{kothari2022ellseg, title={EllSeg-Gen, towards Domain Generalization for head-mounted eyetracking}, author={Kothari, Rakshit S and Bailey, Reynold J and Kanan, Christopher and Pelz, Jeff B and Diaz, Gabriel J}, journal={Proceedings of the ACM on Human-Computer Interaction}, volume={6}, number={ETRA}, pages={1--17}, year={2022}, publisher={ACM New York, NY, USA}}</td>
            <td>no</td>
            <td>Head - mounted camera system</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>600 hand labeled eye images</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>630 MB</td>
            <td> </td>
            <td>lab</td>
            <td>not specified</td>
            <td>no</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>2</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>VEDB</td>
            <td><a href="http://visualexperiencedatabase.org/research.html">http://visualexperiencedatabase.org/research.html</a></td>
            <td> </td>
            <td> </td>
            <td>2020</td>
            <td>not yet</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Ergonomic Design Development of the Visual Experience Database Headset</td>
            <td>2021</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3450341.3458487">https://dl.acm.org/doi/10.1145/3450341.3458487</a></td>
            <td> </td>
            <td>yes</td>
            <td>Pupil Core</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>200</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>Exploring the Effects of Scanpath Feature Engineering for Supervised Image Classification Models</td>
            <td><a href="https://osf.io/fhmjy/">https://osf.io/fhmjy/</a></td>
            <td> </td>
            <td> </td>
            <td>2021</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>Exploring the Effects of Scanpath Feature Engineering for Supervised Image Classification Models</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3591130">https://doi.org/10.1145/3591130</a></td>
            <td>@article{byrne2023exploring,<br>  title={Exploring the Effects of Scanpath Feature Engineering for Supervised Image Classification Models},<br>  author={Byrne, Sean Anthony and Maquiling, Virmarie and Reynolds, Adam Peter Frederick and Polonio, Luca and Castner, Nora and Kasneci, Enkelejda},<br>  journal={Proceedings of the ACM on Human-Computer Interaction},<br>  volume={7},<br>  number={ETRA},<br>  pages={1--18},<br>  year={2023},<br>  publisher={ACM New York, NY, USA}<br>  }</td>
            <td>no</td>
            <td>Eyelink 1000</td>
            <td>static</td>
            <td>screen based</td>
            <td>1000</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>saccades, fixations, scanpaths, AOIs</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>two person matrix game</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>243</td>
            <td>162 women</td>
            <td>81 men</td>
            <td> </td>
            <td>16 Euro</td>
            <td> </td>
            <td> </td>
            <td>24</td>
            <td>24</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>VREM-R1</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>2020</td>
            <td>not specified</td>
            <td>Yes</td>
            <td>adjunct</td>
            <td>Eye Movement Biometrics Using a New Dataset Collected in Virtual Reality</td>
            <td>2020</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3379157.3391420">https://dl.acm.org/doi/10.1145/3379157.3391420</a></td>
            <td> </td>
            <td>yes</td>
            <td>SMI + HTC Vive</td>
            <td>mobile</td>
            <td>VR</td>
            <td>250</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>0.67</td>
            <td>Over 1000 features from fixations, saccades,<br>  and post-saccadic oscillations</td>
            <td> </td>
            <td>yes</td>
            <td>low-pass filtering</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>reading</td>
            <td>2 x 30 minutes with 5 minutes break</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>458</td>
            <td>248 women</td>
            <td>206 men</td>
            <td>4 other</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>Data from 66 subjects excluded due to missing samples</td>
              </tr>
              <tr>
            <td>EMIP</td>
            <td><a href="https://osf.io/53kts/">https://osf.io/53kts/</a></td>
            <td> </td>
            <td> </td>
            <td>2018</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Eye Movement Features in response to Comprehension Performance during the Reading of Programs</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391981">https://doi.org/10.1145/3379156.3391981</a></td>
            <td>@inproceedings{nakayama2020eye,<br>  title={Eye movement features in response to comprehension performance during the reading of programs},<br>  author={Nakayama, Minoru and Harada, Hiroto},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td>no</td>
            <td>SMI:RED250 mobile</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>250</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>code comprehension in three languages: ava, Scala and Python</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>216</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>programmers with different level of expertise </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>scanpath dataset</td>
            <td><a href="http://iam-data.cs.manchester.ac.uk/data_files/19">iam-data.cs.manchester.ac.uk/data_files/19</a></td>
            <td> </td>
            <td> </td>
            <td>2016</td>
            <td>no</td>
            <td>Yes</td>
            <td>Full Paper</td>
            <td>Eye tracking scanpath analysis on web pages: how many users?</td>
            <td>2016</td>
            <td><a href="https://dl.acm.org/doi/10.1145/2857491.2857519">https://dl.acm.org/doi/10.1145/2857491.2857519</a></td>
            <td>@inproceedings{10.1145/2857491.2857519,<br> author = {Eraslan, Sukru and Yesilada, Yeliz and Harper, Simon},<br> title = {Eye Tracking Scanpath Analysis on Web Pages: How Many Users?},<br> year = {2016},<br> isbn = {9781450341257},<br> publisher = {Association for Computing Machinery},<br> address = {New York, NY, USA},<br> url = {https://doi.org/10.1145/2857491.2857519},<br> doi = {10.1145/2857491.2857519},<br> booktitle = {Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research \&amp; Applications},<br> pages = {103\u2013110},<br> numpages = {8},<br> keywords = {scanpath, web pages, eye tracking, sample size, users},<br> location = {Charleston, South Carolina},<br> series = {ETRA '16}<br> }</td>
            <td>no</td>
            <td>Tobii T60</td>
            <td>static</td>
            <td>screen based</td>
            <td>60</td>
            <td>1280 x 1024</td>
            <td> </td>
            <td> </td>
            <td>scanpaths</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td> </td>
            <td>searching and browsing</td>
            <td> </td>
            <td> </td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>81</td>
            <td>40 women</td>
            <td>41 men</td>
            <td> </td>
            <td>not specified</td>
            <td>students, academic and administrative staff</td>
            <td> </td>
            <td>The majority of these users (55 users) were aged between 18 and<br>  24, then 25-34 (20 users) and then 35-54 (6 users</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not able to complete tasks or calibration related problems</td>
              </tr>
              <tr>
            <td>EDA</td>
            <td><a href="https://edaproject.visualstudio.com/_git/EDA">https://edaproject.visualstudio.com/_git/EDA</a></td>
            <td> </td>
            <td> </td>
            <td>2019</td>
            <td>yes</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Eye-tracking Data Analyser (EDA): Web Application and Evaluation</td>
            <td>2020</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3379157.3391301">https://dl.acm.org/doi/10.1145/3379157.3391301</a></td>
            <td> </td>
            <td>no</td>
            <td>Tobii T60</td>
            <td>static</td>
            <td>screen based</td>
            <td>60</td>
            <td>1280 x 1024</td>
            <td>not specified</td>
            <td> </td>
            <td>scanpaths</td>
            <td>FixationIndex    Timestamp    FixationDuration    MappedFixationPointX    MappedFixationPointY</td>
            <td>yes</td>
            <td>not specified</td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td> </td>
            <td>searching and browsing</td>
            <td> </td>
            <td> </td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>40</td>
            <td>20 women</td>
            <td>20 men</td>
            <td> </td>
            <td>not specified</td>
            <td>students, academic and administrative staff</td>
            <td> </td>
            <td>Most of these participants (19 participants)<br>  were aged between 18-24, then 25-34 group (15 participants) and then 35-54 group (six<br>  participants).</td>
            <td> </td>
            <td>5</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not able to complete tasks or calibration related problems</td>
              </tr>
              <tr>
            <td>EYEDIAP</td>
            <td><a href="https://www.idiap.ch/en/dataset/eyediap">https://www.idiap.ch/en/dataset/eyediap</a></td>
            <td> </td>
            <td> </td>
            <td>2014</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Full Paper</td>
            <td>EYEDIAP: A Database for the Development and Evaluation of Gaze Estimation Algorithms from RGB and RGB-D Cameras</td>
            <td>2014</td>
            <td><a href="https://dl.acm.org/doi/10.1145/2578153.2578190">https://dl.acm.org/doi/10.1145/2578153.2578190</a></td>
            <td>@inproceedings{10.1145/2578153.2578190,<br> author = {Funes Mora, Kenneth Alberto and Monay, Florent and Odobez, Jean-Marc},<br> title = {EYEDIAP: A Database for the Development and Evaluation of Gaze Estimation Algorithms from RGB and RGB-D Cameras},<br> year = {2014},<br> isbn = {9781450327510},<br> publisher = {Association for Computing Machinery},<br> address = {New York, NY, USA},<br> url = {https://doi.org/10.1145/2578153.2578190},<br> doi = {10.1145/2578153.2578190},<br> booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},<br> pages = {255\u2013258},<br> numpages = {4},<br> keywords = {depth, RGB, head pose, remote sensing, database, gaze estimation, natural-light, RGB-D},<br> location = {Safety Harbor, Florida},<br> series = {ETRA '14}<br> }</td>
            <td>no</td>
            <td>Kinect ((640x480) and 30fps) HD Camera ((1920x1080) at 25fps) 5 LEDs</td>
            <td>static</td>
            <td>screen based</td>
            <td> </td>
            <td> </td>
            <td>not specified</td>
            <td>not clear</td>
            <td>  rgb_vga.mov : the RGB Kinect video. Encoded using MPEG-4.depth.mov : the Depth video. Encoded using ZLIB.rgb_hd.mov: the RGB HD video. Encoded using MPEG-4 (The HD video is not available for a few sessions).head_pose.txt : the frame-by-frame head pose parameters.eye_tracking.txt : the frame-by-frame 2D and 3D eyes position.ball_tracking.txt : the frame-by-frame 2D and 3D position of the ball target (if relevant).screen_coordinates.txt : the frame-by-frame 2D and 3D screen coordinates (if relevant).rgb_vga_calibration.txt: the calibration parameters for the RGB Kinect camera.depth_calibration.txt: the calibration parameters for the Depth camera.rgb_hd_calibration.txt: the calibration parameters for the RGB HD camera.</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td>yes</td>
            <td>lab</td>
            <td>yes</td>
            <td>no</td>
            <td>Gaze at a target (4cm diameter ball)</td>
            <td> </td>
            <td> </td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>Beijing Sentence Corpus (BSC)</td>
            <td><a href="https://osf.io/vr3k8/">https://osf.io/vr3k8/</a></td>
            <td><a href="https://link.springer.com/article/10.3758/s13428-021-01730-2">https://link.springer.com/article/10.3758/s13428-021-01730-2</a></td>
            <td> </td>
            <td>2021</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3591131">https://doi.org/10.1145/3591131</a></td>
            <td>@article{deng2023eyettention,<br>  title={Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading},<br>  author={Deng, Shuwen and Reich, David R and Prasse, Paul and Haller, Patrick and Scheffer, Tobias and J{\"a}ger, Lena A},<br>  journal={Proceedings of the ACM on Human-Computer Interaction},<br>  volume={7},<br>  number={ETRA},<br>  pages={1--24},<br>  year={2023},<br>  publisher={ACM New York, NY, USA}<br>  }</td>
            <td>no</td>
            <td>Eyelink 1000</td>
            <td>static</td>
            <td>screen based</td>
            <td>500</td>
            <td>1024\u2009\u00d7\u2009768</td>
            <td>Each character occupied a 32\u2009\u00d7\u200932 pixel grid and thus subtended approximately 1.5 degrees of visual angle.</td>
            <td> </td>
            <td>fixation, total reading time, </td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>yes</td>
            <td>reading</td>
            <td> </td>
            <td>43 cm</td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>60</td>
            <td>42 women</td>
            <td>18 men</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>22</td>
            <td>22</td>
            <td>not specified</td>
            <td>chinese</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>CELER L1</td>
            <td><a href="https://github.com/berzak/celer">https://github.com/berzak/celer</a></td>
            <td><a href="https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00054/110717/CELER-A-365-Participant-Corpus-of-Eye-Movements-in">https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00054/110717/CELER-A-365-Participant-Corpus-of-Eye-Movements-in</a></td>
            <td>10.1162/opmi_a_00054</td>
            <td>2022</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3591131">https://doi.org/10.1145/3591131</a></td>
            <td>@article{deng2023eyettention,<br>  title={Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading},<br>  author={Deng, Shuwen and Reich, David R and Prasse, Paul and Haller, Patrick and Scheffer, Tobias and J{\"a}ger, Lena A},<br>  journal={Proceedings of the ACM on Human-Computer Interaction},<br>  volume={7},<br>  number={ETRA},<br>  pages={1--24},<br>  year={2023},<br>  publisher={ACM New York, NY, USA}<br>  }</td>
            <td>no</td>
            <td>Eyelink 1000</td>
            <td>static</td>
            <td>screen based</td>
            <td>1000</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td>fixation </td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>not specified</td>
            <td>reading</td>
            <td>45-90 minutes</td>
            <td> </td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>365</td>
            <td>224 women</td>
            <td>138 men</td>
            <td> </td>
            <td>yes 20$ to 30$</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>27.3</td>
            <td>27.3</td>
            <td>not specified</td>
            <td>Native speakers of 23 Arabic, 71 Chinese, 71 Japanese, 68 Spanish, and 63 Portuguese</td>
            <td> </td>
            <td>variable</td>
            <td>not specified</td>
            <td>yes</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not able to complete tasks or calibration related problems</td>
              </tr>
              <tr>
            <td>ZuCo 2.0 NR</td>
            <td><a href="https://osf.io/2urht/">https://osf.io/2urht/</a></td>
            <td><a href="https://arxiv.org/pdf/1912.00903.pdf">https://arxiv.org/pdf/1912.00903.pdf</a></td>
            <td> </td>
            <td>2019</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3591131">https://doi.org/10.1145/3591131</a></td>
            <td>@article{deng2023eyettention,<br>  title={Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading},<br>  author={Deng, Shuwen and Reich, David R and Prasse, Paul and Haller, Patrick and Scheffer, Tobias and J{\"a}ger, Lena A},<br>  journal={Proceedings of the ACM on Human-Computer Interaction},<br>  volume={7},<br>  number={ETRA},<br>  pages={1--24},<br>  year={2023},<br>  publisher={ACM New York, NY, USA}<br>  }</td>
            <td>yes</td>
            <td>EyeLink 1000 Plus</td>
            <td>static</td>
            <td>screen based</td>
            <td>500</td>
            <td>800 x 600</td>
            <td> </td>
            <td> </td>
            <td>fixation</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>yes</td>
            <td>reading</td>
            <td> </td>
            <td>68 cm</td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>18</td>
            <td>10 women</td>
            <td>8 men</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>34</td>
            <td> </td>
            <td> </td>
            <td>native language is English, originating from Australia, Canada, UK, USA or South Africa</td>
            <td> </td>
            <td> </td>
            <td>2 are left handed</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>not able to complete tasks or calibration related problems</td>
              </tr>
              <tr>
            <td>ZuCo NR</td>
            <td><a href="https://osf.io/q3zws/">https://osf.io/q3zws/</a></td>
            <td> </td>
            <td> </td>
            <td>2018</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3591131">https://doi.org/10.1145/3591131</a></td>
            <td>@article{deng2023eyettention,<br>  title={Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading},<br>  author={Deng, Shuwen and Reich, David R and Prasse, Paul and Haller, Patrick and Scheffer, Tobias and J{\"a}ger, Lena A},<br>  journal={Proceedings of the ACM on Human-Computer Interaction},<br>  volume={7},<br>  number={ETRA},<br>  pages={1--24},<br>  year={2023},<br>  publisher={ACM New York, NY, USA}<br>  }</td>
            <td>yes</td>
            <td>EyeLink 1000 Plus</td>
            <td>static</td>
            <td>screen based</td>
            <td>500</td>
            <td> </td>
            <td> </td>
            <td>&lt;0.01\u00b0</td>
            <td>fixation and saccades</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>yes</td>
            <td>reading</td>
            <td>2-3 hours</td>
            <td>68 cm</td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>12</td>
            <td>5 women</td>
            <td>7 men</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>22 to 54 years old</td>
            <td>22 to 54 years old</td>
            <td> </td>
            <td>all native English speakers (originating from Canada, USA, UK or Australia)</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>Fixations that were shorter than 100\u2009ms were excluded from the analyses</td>
              </tr>
              <tr>
            <td>MPIIEgoFixation Dataset</td>
            <td><a href="https://www.mpi-inf.mpg.de/MPIIEgoFixation/">https://www.mpi-inf.mpg.de/MPIIEgoFixation/</a></td>
            <td> </td>
            <td> </td>
            <td>2018</td>
            <td>yes</td>
            <td>No</td>
            <td>Full Paper</td>
            <td>Fixation detection for head-mounted eye tracking based on visual similarity of gaze targets</td>
            <td>2018</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3204493.3204538">https://dl.acm.org/doi/10.1145/3204493.3204538</a></td>
            <td>@inproceedings{10.1145/3204493.3204538,<br>  author = {Steil, Julian and Huang, Michael Xuelin and Bulling, Andreas},<br>  title = {Fixation Detection for Head-Mounted Eye Tracking Based on Visual Similarity of Gaze Targets},<br>  year = {2018},<br>  isbn = {9781450357067},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3204493.3204538},<br>  doi = {10.1145/3204493.3204538},<br>  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  articleno = {23},<br>  numpages = {9},<br>  keywords = {mobile eye tracking, visual focus of attention, egocentric vision},<br>  location = {Warsaw, Poland},<br>  series = {ETRA '18}<br>  }</td>
            <td>yes</td>
            <td>Pupil</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>30</td>
            <td>720p</td>
            <td> </td>
            <td> </td>
            <td>fixation</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td>3.1 MB</td>
            <td>yes</td>
            <td>wild</td>
            <td>yes</td>
            <td>no</td>
            <td>viewing</td>
            <td>5 minutes</td>
            <td>not specified</td>
            <td>walking</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>5</td>
            <td>1 woman</td>
            <td>4 men</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>20 to 33</td>
            <td>20 to 33</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>EHTask</td>
            <td><a href="http://zhiminghu.net/hu22_ehtask.html">http://zhiminghu.net/hu22_ehtask.html</a></td>
            <td><a href="https://ieeexplore.ieee.org/document/9664291">https://ieeexplore.ieee.org/document/9664291</a></td>
            <td><a href="http://dx.doi.org/10.1109/TVCG.2021.3138902">http://dx.doi.org/10.1109/TVCG.2021.3138902</a></td>
            <td>2023</td>
            <td> </td>
            <td>No</td>
            <td>Short</td>
            <td>For Your Eyes Only: Privacy-preserving eye-tracking datasets</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3517031.3529618">https://doi.org/10.1145/3517031.3529618</a></td>
            <td>@inproceedings{david2022your,<br>  title={For your eyes only: Privacy-preserving eye-tracking datasets},<br>  author={David-John, Brendan and Butler, Kevin and Jain, Eakta},<br>  booktitle={2022 Symposium on Eye Tracking Research and Applications},<br>  pages={1--6},<br>  year={2022}<br>  }</td>
            <td>no</td>
            <td>HTC Vive 7invensun VR HMD</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>100</td>
            <td> </td>
            <td> </td>
            <td>0.5\u00b0</td>
            <td>fixation and saccades</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td>no</td>
            <td>lab</td>
            <td>yes</td>
            <td>no</td>
            <td>Free viewing, Visual search, Saliency, and Track</td>
            <td>150seconds</td>
            <td> </td>
            <td>not specified</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>30</td>
            <td>12 women</td>
            <td>18 men</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>24.5</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>ET-DK2</td>
            <td><a href="https://zenodo.org/record/4642612">https://zenodo.org/record/4642612</a></td>
            <td><a href="https://github.com/xucong-zhang/ETH-Xgaze">https://github.com/xucong-zhang/ETH-Xgaze</a></td>
            <td><a href="https://zenodo.org/doi/10.5281/zenodo.4642566">https://zenodo.org/doi/10.5281/zenodo.4642566</a></td>
            <td>2021</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>For Your Eyes Only: Privacy-preserving eye-tracking datasets</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3517031.3529618">https://doi.org/10.1145/3517031.3529619</a></td>
            <td>@inproceedings{david2022your,<br>  title={For your eyes only: Privacy-preserving eye-tracking datasets},<br>  author={David-John, Brendan and Butler, Kevin and Jain, Eakta},<br>  booktitle={2022 Symposium on Eye Tracking Research and Applications},<br>  pages={1--6},<br>  year={2022}<br>  }</td>
            <td>yes</td>
            <td>SMI 60Hz</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>60</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>Eye and head movement data</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>no</td>
            <td>Free viewing </td>
            <td>25 seconds</td>
            <td> </td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>18</td>
            <td>5 women</td>
            <td>13 men</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>23 to 52</td>
            <td>32</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>motion sickness and incomplete data</td>
              </tr>
              <tr>
            <td>OpenEDS</td>
            <td><a href="https://paperswithcode.com/dataset/openeds">https://paperswithcode.com/dataset/openeds</a></td>
            <td><a href="https://doi.org/10.1145/3448018.3457996">https://doi.org/10.1145/3448018.3457996</a></td>
            <td> </td>
            <td>2021</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>For Your Eyes Only: Privacy-preserving eye-tracking datasets</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3517031.3529618">https://doi.org/10.1145/3517031.3529623</a></td>
            <td>@inproceedings{david2022your,<br>  title={For your eyes only: Privacy-preserving eye-tracking datasets},<br>  author={David-John, Brendan and Butler, Kevin and Jain, Eakta},<br>  booktitle={2022 Symposium on Eye Tracking Research and Applications},<br>  pages={1--6},<br>  year={2022}<br>  }</td>
            <td>yes</td>
            <td>Oculus Rift with custom made eye tracker</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>not specified</td>
            <td>2560 x 1440 </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>3D gaze vectors in meters</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>no</td>
            <td>Free Exploration</td>
            <td>10 minutes</td>
            <td> </td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>44</td>
            <td>20 women</td>
            <td>24 men</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>31.7</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>40 right handed</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>62.95mm</td>
            <td> </td>
              </tr>
              <tr>
            <td>VR-EyeTracking</td>
            <td><a href="https://github.com/xuyanyu-shh/VR-EyeTracking">https://github.com/xuyanyu-shh/VR-EyeTracking</a></td>
            <td> </td>
            <td> </td>
            <td>2018</td>
            <td>not clear</td>
            <td>No</td>
            <td>Short</td>
            <td>For Your Eyes Only: Privacy-preserving eye-tracking datasets</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3517031.3529618">https://doi.org/10.1145/3517031.3529622</a></td>
            <td>@inproceedings{david2022your,<br>  title={For your eyes only: Privacy-preserving eye-tracking datasets},<br>  author={David-John, Brendan and Butler, Kevin and Jain, Eakta},<br>  booktitle={2022 Symposium on Eye Tracking Research and Applications},<br>  pages={1--6},<br>  year={2022}<br>  }</td>
            <td> </td>
            <td>HTC Vive 7invensun VR HMD</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>gaze points</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>no</td>
            <td>Free Exploration</td>
            <td> </td>
            <td> </td>
            <td>not specified</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>45</td>
            <td>20 women</td>
            <td>25 men</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>VR-Saliency</td>
            <td><a href="https://github.com/vsitzmann/vr-saliency">https://github.com/vsitzmann/vr-saliency</a></td>
            <td><a href="https://ieeexplore.ieee.org/document/8269807">https://ieeexplore.ieee.org/document/8269807</a></td>
            <td> </td>
            <td>2017</td>
            <td>not clear</td>
            <td>No</td>
            <td>Short</td>
            <td>For Your Eyes Only: Privacy-preserving eye-tracking datasets</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3517031.3529618">https://doi.org/10.1145/3517031.3529620</a></td>
            <td>@inproceedings{david2022your,<br>  title={For your eyes only: Privacy-preserving eye-tracking datasets},<br>  author={David-John, Brendan and Butler, Kevin and Jain, Eakta},<br>  booktitle={2022 Symposium on Eye Tracking Research and Applications},<br>  pages={1--6},<br>  year={2022}<br>  }</td>
            <td> </td>
            <td>Oculus DK2 + PupilLabs eye tracker</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>120</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>fixation</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>no</td>
            <td>Free Exploration</td>
            <td>30 seconds</td>
            <td> </td>
            <td>not specified</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>122</td>
            <td>30 women</td>
            <td>92 men</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>GEAR</td>
            <td><a href="https://github.com/Interactions-HSG/GEAR/tree/main">https://github.com/Interactions-HSG/GEAR/tree/main</a></td>
            <td> </td>
            <td> </td>
            <td>2023</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Short</td>
            <td>GEAR: Gaze-enabled augmented reality for human activity recognition</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588402">https://doi.org/10.1145/3588015.3588402</a></td>
            <td>@inproceedings{bektacs2023gear,<br>  title={GEAR: Gaze-enabled augmented reality for human activity recognition},<br>  author={Bekta{\c{s}}, Kenan and Strecker, Jannis and Mayer, Simon and Garcia, Kimberly and Hermann, Jonas and Jenss, Kay Erik and Antille, Yasmine Sheila and Soler, Marc Elias},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--9},<br>  year={2023}<br>  }</td>
            <td>yes</td>
            <td>Microsoft HoloLens 2</td>
            <td>mobile</td>
            <td>AR wearble</td>
            <td>30</td>
            <td> </td>
            <td> </td>
            <td>&gt;1\u00b0</td>
            <td>fixation and blinks</td>
            <td>timestamped raw data</td>
            <td>yes</td>
            <td>yes</td>
            <td> </td>
            <td>yes</td>
            <td>lab</td>
            <td>yes</td>
            <td>no</td>
            <td>reading, inspection, search</td>
            <td>1 minute for each</td>
            <td>70 cm</td>
            <td>standing and sitting</td>
            <td>variable</td>
            <td> </td>
            <td> </td>
            <td>10</td>
            <td>3 women</td>
            <td>7 men</td>
            <td> </td>
            <td>no</td>
            <td> </td>
            <td>computer science</td>
            <td>30</td>
            <td>30</td>
            <td>4 use prescription glasses</td>
            <td>no</td>
            <td> </td>
            <td>left to right</td>
            <td>now specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>Benchmark data for evaluating visualization and analysis techniques for eye tracking for video stimuli</td>
            <td><a href="https://www.visus.uni-stuttgart.de/publikationen/benchmark-eyetracking">https://www.visus.uni-stuttgart.de/publikationen/benchmark-eyetracking</a></td>
            <td><a href="https://dl.acm.org/doi/abs/10.1145/2669557.2669558">https://dl.acm.org/doi/abs/10.1145/2669557.2669558</a></td>
            <td> </td>
            <td>2014</td>
            <td>no</td>
            <td>No</td>
            <td>Short Paper</td>
            <td>Image-based scanpath comparison with slit-scan visualization</td>
            <td>2018</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3204493.3204581">https://dl.acm.org/doi/10.1145/3204493.3204581</a></td>
            <td>@inproceedings{10.1145/3204493.3204581,<br>  author = {Koch, Maurice and Kurzhals, Kuno and Weiskopf, Daniel},<br>  title = {Image-Based Scanpath Comparison with Slit-Scan Visualization},<br>  year = {2018},<br>  isbn = {9781450357067},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3204493.3204581},<br>  doi = {10.1145/3204493.3204581},<br>  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  articleno = {55},<br>  numpages = {5},<br>  keywords = {visualization, slit-scan, scanpath comparison},<br>  location = {Warsaw, Poland},<br>  series = {ETRA '18}<br>  }</td>
            <td>yes</td>
            <td>Tobii T60 XL </td>
            <td>static</td>
            <td>screen based</td>
            <td>60</td>
            <td>1920 x 1200</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>Eye tracking data: fixation in TSV files<br>  Video stimulus<br>  Dynamic AOIs</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>no</td>
            <td>memory game</td>
            <td>45 minutes</td>
            <td>not specified</td>
            <td>presumably sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>25</td>
            <td>10 women</td>
            <td>15 men</td>
            <td> </td>
            <td>7 EURO</td>
            <td>All technical background except 3</td>
            <td>computer science</td>
            <td>24</td>
            <td> </td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>Benchmark paper Data</td>
            <td><a href="https://github.com/jbfove/BenchmarkPaperData">https://github.com/jbfove/BenchmarkPaperData</a></td>
            <td> </td>
            <td> </td>
            <td>2021</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Long</td>
            <td>Important Considerations of Data Collection and Curation for Reliable Benchmarking of End-User Eye-Tracking Systems</td>
            <td>2021</td>
            <td><a href="https://doi.org/10.1145/3448017.3457383">https://doi.org/10.1145/3448017.3457383</a></td>
            <td>@inproceedings{chernyak2021important,<br>  title={Important considerations of data collection and curation for reliable benchmarking of end-user eye-tracking systems},<br>  author={Chernyak, Iakov and Chernyak, Grigory and Bland, Jeffrey KS and Rahier, Pierre DP},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--9},<br>  year={2021}<br>  }</td>
            <td> </td>
            <td>FOVE eye-tracking<br>  headse</td>
            <td>mobile</td>
            <td>wearable</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>3D Gaze vectors in CSV files</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>no</td>
            <td>Look at 3D positon of points in VR</td>
            <td>250 ms</td>
            <td>2 meters</td>
            <td>not specified</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>157</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>yes</td>
            <td> </td>
            <td>yes</td>
            <td>Asian, Black, Hispanic, Middle East, White</td>
            <td>no</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>yes</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>yes</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>Natural Walking and Eye Tracking in different Virtual Reality Tasks<br>  </td>
            <td><a href="https://osf.io/b43uv/">https://osf.io/b43uv/</a></td>
            <td><a href="https://ieeexplore.ieee.org/document/9756758">https://ieeexplore.ieee.org/document/9756758</a></td>
            <td>DOI 10.17605/OSF.IO/B43UV</td>
            <td>2021</td>
            <td>yes</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Indirect gaze estimation from body movements based on<br>  relationship between gaze and body movements</td>
            <td>2023</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3588015.3590131">https://dl.acm.org/doi/10.1145/3588015.3590131</a></td>
            <td> </td>
            <td>yes</td>
            <td>HTC Vive Pro Eye</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>120</td>
            <td>1440X1600</td>
            <td>110</td>
            <td>0.5\u20131.1</td>
            <td>eye tracking <br>  IMU<br>  position from Vive's infrared tracking</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>no</td>
            <td>natural walking on straight and curved paths, searching for a target and avoiding obstacles.</td>
            <td>15 minutes</td>
            <td> </td>
            <td>walking</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>18</td>
            <td>8 women</td>
            <td>10 men</td>
            <td> </td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>20 to 47</td>
            <td>27</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>data loss</td>
              </tr>
              <tr>
            <td>One algorithm to rule them all? An evaluation and discussion of ten eye movement event-detection algorithms</td>
            <td><a href="https://link.springer.com/article/10.3758/s13428-016-0738-9">  </a><a href="https://link.springer.com/article/10.3758/s13428-016-0738-9">https://link.springer.com/article/10.3758/s13428-016-0738-9</a></td>
            <td> </td>
            <td> </td>
            <td>2017</td>
            <td>not clear</td>
            <td>No</td>
            <td>ETVIS Workshop</td>
            <td>Intuitive visualization technique to support eye tracking data analysis: a user-study </td>
            <td>2018</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3205929.3205939">https://dl.acm.org/doi/10.1145/3205929.3205939</a></td>
            <td>@inproceedings{10.1145/3205929.3205939,<br>  author = {Peysakhovich, Vsevolod and Hurter, Christophe},<br>  title = {Intuitive Visualization Technique to Support Eye Tracking Data Analysis: A User-Study},<br>  year = {2018},<br>  isbn = {9781450357876},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3205929.3205939},<br>  doi = {10.1145/3205929.3205939},<br>  booktitle = {Proceedings of the 3rd Workshop on Eye Tracking and Visualization},<br>  articleno = {1},<br>  numpages = {5},<br>  keywords = {flow directional map, scanpath, oriented line integral convolution, visualization},<br>  location = {Warsaw, Poland},<br>  series = {ETVIS '18}<br>  }</td>
            <td>no</td>
            <td>SMI HiSpeed 1250</td>
            <td>static</td>
            <td>screen based</td>
            <td>500</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>fixations and saccades</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td> </td>
            <td>yes</td>
            <td>freely view the images,<br>  look at the moving objects in the videos, and to follow the<br>  moving dot targets</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>17</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>students</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>Coregistration of eye movements and EEG in natural reading: Analyses and review</td>
            <td><a href="https://psycnet.apa.org/record/2011-14094-001">https://psycnet.apa.org/record/2011-14094-001</a></td>
            <td> </td>
            <td> </td>
            <td>2011</td>
            <td>not clear</td>
            <td>No</td>
            <td>ETRA Doctoral Symposium</td>
            <td>Investigating the Multicausality of Processing Speed Deficits across Developmental Disorders with Eye Tracking and EEG: Extended Abstract</td>
            <td>2018</td>
            <td><a href="https://doi.org/10.1145/3204493.3207417">https://doi.org/10.1145/3204493.3207417</a></td>
            <td>@inproceedings{10.1145/3204493.3207417,<br>  author = {Dziemian, Sabine and Langer, Nicolas},<br>  title = {Investigating the Multicausality of Processing Speed Deficits across Developmental Disorders with Eye Tracking and EEG: Extended Abstract},<br>  year = {2018},<br>  isbn = {9781450357067},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3204493.3207417},<br>  doi = {10.1145/3204493.3207417},<br>  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  articleno = {75},<br>  numpages = {2},<br>  keywords = {processing speed, electroencephalography (EEG), symbol search task, developmental disorders, eye-tracking},<br>  location = {Warsaw, Poland},<br>  series = {ETRA '18}<br>  }</td>
            <td> </td>
            <td>SMI iView-X Red-m</td>
            <td> </td>
            <td> </td>
            <td>120</td>
            <td> </td>
            <td> </td>
            <td>&lt; 0.01\u00b0</td>
            <td>fixations and saccades</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>Symbol<br>  Search Test</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>881</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>children and adolescents </td>
            <td> </td>
            <td>5 to 21</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>76.3 subjects are diagnosed with one or more psychological disprders</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>ETPAD v2</td>
            <td><a href="https://userweb.cs.txstate.edu/~ok11/etpad_v2.html">https://userweb.cs.txstate.edu/~ok11/etpad_v2.html</a></td>
            <td><a href="https://www.sciencedirect.com/science/article/pii/S0167865515001737">https://www.sciencedirect.com/science/article/pii/S0167865515001737</a></td>
            <td> </td>
            <td>2015</td>
            <td>yes</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Iris Print Attack Detection using Eye Movement Signals<br>  </td>
            <td>2022</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3517031.3532521">https://dl.acm.org/doi/10.1145/3517031.3532521</a></td>
            <td> </td>
            <td>yes</td>
            <td>EyeLink 1000</td>
            <td>static</td>
            <td>screen based</td>
            <td>1000</td>
            <td> </td>
            <td> </td>
            <td>0.01\u00b0</td>
            <td>1199 eye movement recordings and 400 iris images</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>yes</td>
            <td>single fixation point</td>
            <td> </td>
            <td>55cm</td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>200</td>
            <td>101 women</td>
            <td>99 men</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>18 to 4</td>
            <td>22</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>Ubiris</td>
            <td><a href="http://iris.di.ubi.pt/">http://iris.di.ubi.pt/</a></td>
            <td><a href="https://ieeexplore.ieee.org/document/4815254">https://ieeexplore.ieee.org/document/4815254</a></td>
            <td> </td>
            <td>2010</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>Label Likelihood Maximisation: Adapting iris segmentation models using domain adaptation</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379155.3391327">https://doi.org/10.1145/3379155.3391330</a></td>
            <td>@inproceedings{eskildsen2020label,<br>  title={Label Likelihood Maximisation: Adapting iris segmentation models using domain adaptation},<br>  author={Eskildsen, Anton M{\o}lbjerg and Hansen, Dan Witzner},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--9},<br>  year={2020}<br>  }</td>
            <td> </td>
            <td>Canon EOS 5D</td>
            <td>mobile</td>
            <td>not applicable</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>Manually cropped tiff images</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>11102 images from 522 irises</td>
            <td> </td>
            <td>lab</td>
            <td> </td>
            <td> </td>
            <td>not specified</td>
            <td> </td>
            <td>3 to 10 meters</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>261</td>
            <td>119 women</td>
            <td>142 men</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td>Latin Caucasian (around 90 percent) and also<br>  black (8 percent) and Asian people (2 percent)</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>iris color</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>Labelled Pupils in the Wild (LPW</td>
            <td><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/labelled-pupils-in-the-wild-lpw">https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/labelled-pupils-in-the-wild-lpw</a></td>
            <td> </td>
            <td><a href="https://dl.acm.org/doi/10.1145/2857491.2857520">https://dl.acm.org/doi/10.1145/2857491.2857520</a></td>
            <td>2016</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Short Paper</td>
            <td>Labelled pupils in the wild: a dataset for studying pupil detection in unconstrained environments</td>
            <td>2016</td>
            <td><a href="https://dl.acm.org/doi/10.1145/2857491.2857520">https://dl.acm.org/doi/10.1145/2857491.2857520</a></td>
            <td>@inproceedings{10.1145/2857491.2857520,<br> author = {Tonsen, Marc and Zhang, Xucong and Sugano, Yusuke and Bulling, Andreas},<br> title = {Labelled Pupils in the Wild: A Dataset for Studying Pupil Detection in Unconstrained Environments},<br> year = {2016},<br> isbn = {9781450341257},<br> publisher = {Association for Computing Machinery},<br> address = {New York, NY, USA},<br> url = {https://doi.org/10.1145/2857491.2857520},<br> doi = {10.1145/2857491.2857520},<br> booktitle = {Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research \&amp; Applications},<br> pages = {139\u2013142},<br> numpages = {4},<br> keywords = {head-mounted eye tracking, high-speed, pupil detection, high-quality},<br> location = {Charleston, South Carolina},<br> series = {ETRA '16}<br> }</td>
            <td> </td>
            <td>Pupil Pro</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>120</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>high-speed eye region videos</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>130,856 images (2.4 GB)</td>
            <td>yes</td>
            <td>34.3% of the recordings were done outdoors, in 84.7% natural light was present and in 33.6% artificial light was present</td>
            <td>yes</td>
            <td>no</td>
            <td>look at<br>  a moving red ball as a fixation target </td>
            <td>20 seconds</td>
            <td>not specified</td>
            <td>standing</td>
            <td>variable</td>
            <td> </td>
            <td> </td>
            <td>22</td>
            <td>9 women</td>
            <td>13 men</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>11 Indian, 6 German, 2 Pakistani, 2 Iranian, and 1 Egyptian</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>eye colors: 12 brown, 5  black,  3  blue-gray,  1  blue-green,  1  green</td>
            <td>yes</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>Columbia (Gaze Locking: Passive Eye Contact Detection<br>  for Human\u2013Object Interaction)</td>
            <td><a href="https://www.cs.columbia.edu/CAVE/databases/columbia_gaze/">https://www.cs.columbia.edu/CAVE/databases/columbia_gaze/</a></td>
            <td><a href="https://dl.acm.org/doi/abs/10.1145/2501988.2501994">https://dl.acm.org/doi/abs/10.1145/2501988.2501994</a></td>
            <td> </td>
            <td>2013</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>ManiGaze: a Dataset for Evaluating Remote Gaze Estimator in Object Manipulation Situations</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391369">https://doi.org/10.1145/3379156.3391369</a></td>
            <td>@inproceedings{siegfried2020manigaze,<br>  title={ManiGaze: A dataset for evaluating remote gaze estimator in object manipulation situations},<br>  author={Siegfried, Remy and Aminian, Bozorgmehr and Odobez, Jean-Marc},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td> </td>
            <td>Canon EOS Rebel T3i<br>  Canon EF-S 18\u2013135 mm IS f/3.5\u20135.6 zoom len</td>
            <td>mobile</td>
            <td>not applicable</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>5,880 high-resolution images </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>yes</td>
            <td>Look at a grid of points</td>
            <td> </td>
            <td>2.5 meters</td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>56</td>
            <td>24 women</td>
            <td>32 men</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>18 to 36 </td>
            <td>not specified</td>
            <td> </td>
            <td>1 of our sub-<br>  jects were Asian, 19 were White, 8 were South Asian, 7 were<br>  Black, and 4 were Hispanic or Latino</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>6 wear glasses</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>ManiGaze</td>
            <td><a href="https://www.idiap.ch/en/dataset/manigaze">https://www.idiap.ch/en/dataset/manigaze</a></td>
            <td> </td>
            <td> </td>
            <td>2020</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Short</td>
            <td>ManiGaze: a Dataset for Evaluating Remote Gaze Estimator in Object Manipulation Situations</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391369">https://doi.org/10.1145/3379156.3391369</a></td>
            <td>@inproceedings{siegfried2020manigaze,<br>  title={ManiGaze: A dataset for evaluating remote gaze estimator in object manipulation situations},<br>  author={Siegfried, Remy and Aminian, Bozorgmehr and Odobez, Jean-Marc},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td> </td>
            <td>RGB-D cameras</td>
            <td>static</td>
            <td>not applicable</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>Videos</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>not specified</td>
            <td>no</td>
            <td>Four different HRI tasks (Wizard of Oz)</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>sitting</td>
            <td>not specified</td>
            <td> </td>
            <td> </td>
            <td>16</td>
            <td>4 women</td>
            <td>12 men</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>24 to 36</td>
            <td> </td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>6 wear glasses</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>UTMultiview (Multi-view Gaze Dataset (CVPR\u201914))</td>
            <td><a href="https://www.ut-vision.org/datasets/">https://www.ut-vision.org/datasets/</a></td>
            <td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Sugano_Learning-by-Synthesis_for_Appearance-based_2014_CVPR_paper.html">https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Sugano_Learning-by-Synthesis_for_Appearance-based_2014_CVPR_paper.html</a></td>
            <td> </td>
            <td>2014</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>ManiGaze: a Dataset for Evaluating Remote Gaze Estimator in Object Manipulation Situations</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391369">https://doi.org/10.1145/3379156.3391369</a></td>
            <td>@inproceedings{siegfried2020manigaze,<br>  title={ManiGaze: A dataset for evaluating remote gaze estimator in object manipulation situations},<br>  author={Siegfried, Remy and Aminian, Bozorgmehr and Odobez, Jean-Marc},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td> </td>
            <td>Eight 1.3 megapixel color cameras, PointGrey Flea3 USB<br>  3.0 with a 8 mm fixed focal-length lens</td>
            <td>static</td>
            <td>screen based</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>Eye images <br>  3D eye shape models<br>  Synthesized Eye images</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>20GB</td>
            <td> </td>
            <td>lab</td>
            <td> </td>
            <td>yes</td>
            <td>Look at a target on monitor</td>
            <td> </td>
            <td>60 cm</td>
            <td>sitting</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>50</td>
            <td>15 women</td>
            <td>35 men</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>20 to 40</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>Multidisciplinary Reading Patterns of Digital Documents</td>
            <td><a href="https://github.com/nirdslab/Multidisciplinary-Reading-Patterns">https://github.com/nirdslab/Multidisciplinary-Reading-Patterns</a></td>
            <td> </td>
            <td> </td>
            <td>2022</td>
            <td>yes</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Multidisciplinary Reading Patterns of Digital Documents</td>
            <td>2022</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3517031.3531630">https://dl.acm.org/doi/10.1145/3517031.3531630</a></td>
            <td> </td>
            <td>no</td>
            <td>PupilLabs Core</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>120</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>Raw pupillary, fixation data</td>
            <td> </td>
            <td>yes</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>lab</td>
            <td>yes</td>
            <td>no</td>
            <td>Reading papers</td>
            <td>not specified</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>7</td>
            <td>6 women</td>
            <td>1 man</td>
            <td> </td>
            <td>not specified</td>
            <td>students</td>
            <td>Computer Science (CS) (2), Mathematics (2), and Physics (3)</td>
            <td>25 to 35</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
              </tr>
              <tr>
            <td>multiscale-trajectories</td>
            <td><a href="https://github.com/Jojo134/multiscale-trajectories">https://github.com/Jojo134/multiscale-trajectories</a></td>
            <td> </td>
            <td> </td>
            <td>2016</td>
            <td>yes</td>
            <td>No</td>
            <td>ETVIS Workshop</td>
            <td>Multiscale scanpath visualization and filtering</td>
            <td>2018</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3205929.3205931">https://dl.acm.org/doi/10.1145/3205929.3205931</a></td>
            <td>@inproceedings{10.1145/3205929.3205931,<br>  author = {Rodrigues, Nils and Netzel, Rudolf and Spalink, Joachim and Weiskopf, Daniel},<br>  title = {Multiscale Scanpath Visualization and Filtering},<br>  year = {2018},<br>  isbn = {9781450357876},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3205929.3205931},<br>  doi = {10.1145/3205929.3205931},<br>  booktitle = {Proceedings of the 3rd Workshop on Eye Tracking and Visualization},<br>  articleno = {2},<br>  numpages = {5},<br>  keywords = {visualization, multimatch, quadtree, scanpath, eye tracking},<br>  location = {Warsaw, Poland},<br>  series = {ETVIS '18}<br>  }</td>
            <td>no</td>
            <td>Tobii T60 XL</td>
            <td>static</td>
            <td>screen based</td>
            <td>60</td>
            <td>1920 x 1200</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>fixations and scanpaths</td>
            <td>csv files: timestamp stimuli fixation index fixation duration </td>
            <td> </td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>Lab</td>
            <td>yes</td>
            <td>no</td>
            <td>Reading metro maps and finding routes </td>
            <td>On average 54 minutes</td>
            <td>60 cm</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>40</td>
            <td>17 women</td>
            <td>23 men</td>
            <td> </td>
            <td>10 Euro</td>
            <td>24 students with 14 computer science or SW engineering background. 15 manufactureres </td>
            <td> </td>
            <td>18 to 39</td>
            <td>25.3</td>
            <td> </td>
            <td>not specified</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>Snellen chart and an Ishihara test to check if people had appropriate visual acuity and color vision</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>low quality of recorded eye tracking data</td>
              </tr>
              <tr>
            <td>DR(eye)VE Dataset</td>
            <td><a href="https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=8">https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=8</a></td>
            <td> </td>
            <td> </td>
            <td>2018</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>On the Use of Distribution-based Metrics for the Evaluation of Drivers\u2019 Fixation Maps Against Spatial Baselines</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3517031.3529629">https://doi.org/10.1145/3517031.3529629</a></td>
            <td>@inproceedings{maldonado2022use,<br>  title={On the Use of Distribution-based Metrics for the Evaluation of Drivers\u2019 Fixation Maps Against Spatial Baselines},<br>  author={Maldonado, Jaime and Giefer, Lino Antoni},<br>  booktitle={2022 Symposium on Eye Tracking Research and Applications},<br>  pages={1--7},<br>  year={2022}<br>  }</td>
            <td> </td>
            <td>SMI ETG 2w</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>60</td>
            <td>720p</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>fixation, saccade, gaze map, pupil dilation</td>
            <td>Driver gaze data and Videos from the car perspective</td>
            <td>yes</td>
            <td>not specified</td>
            <td>555000 frames from 74 videos </td>
            <td>In car cockpit</td>
            <td>Wild</td>
            <td>manual calibration before each sequence</td>
            <td>no</td>
            <td>Driving in different conditions (traffic, weather, hours of the day)</td>
            <td>5 minute secuences (but the whole study lasted more than 2 months)</td>
            <td> </td>
            <td>sitting</td>
            <td>yes </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>8</td>
            <td>7 women</td>
            <td>2 men</td>
            <td> </td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>20 to 40</td>
            <td>30</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
              </tr>
              <tr>
            <td>EEGEyeNet</td>
            <td><a href="https://paperswithcode.com/dataset/eegeyenet">https://paperswithcode.com/dataset/eegeyenet</a></td>
            <td> </td>
            <td> </td>
            <td>2021</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>One step closer to EEG based eye tracking</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588423">https://doi.org/10.1145/3588015.3588423</a></td>
            <td>@article{fuhl2023one,<br>  title={One step closer to EEG based eye tracking},<br>  author={Fuhl, Wolfgang and Zabel, Susanne and Harbig, Theresa and Moldt, Julia Astrid and Wiete, Teresa Festl and Werner, Anne Herrmann and Nieselt, Kay},<br>  journal={arXiv preprint arXiv:2303.06039},<br>  year={2023}<br>  }</td>
            <td>no</td>
            <td>ET EyeLink 1000 Plus </td>
            <td>static</td>
            <td>screen based</td>
            <td>500Hz</td>
            <td>800x600px</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>gaze data</td>
            <td>raw and preprocessed data</td>
            <td>yes</td>
            <td>yes</td>
            <td>not specified</td>
            <td>yes</td>
            <td>Lab</td>
            <td>9 point</td>
            <td>yes</td>
            <td>pro- and anti-saccade paradigm</td>
            <td>not specified</td>
            <td>68cm</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>356</td>
            <td>190 female</td>
            <td>166 male</td>
            <td> </td>
            <td>$50 equivalent</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>18-80</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>OpenNEEDS</td>
            <td><a href="https://research.facebook.com/publications/openneeds-a-dataset-of-gaze-head-hand-and-scene-signals-during-exploration-in-open-ended-vr-environments/">https://research.facebook.com/publications/openneeds-a-dataset-of-gaze-head-hand-and-scene-signals-during-exploration-in-open-ended-vr-environments/</a></td>
            <td><a href="https://www.dropbox.com/work/OpenNEEDS_2020">https://www.dropbox.com/work/OpenNEEDS_2020</a></td>
            <td> </td>
            <td>2021</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Short</td>
            <td>OpenNEEDS: A Dataset of Gaze, Head, Hand, and Scene Signals During Exploration in Open-Ended VR Environments</td>
            <td>2021</td>
            <td><a href="https://doi.org/10.1145/3448018.3457996">https://doi.org/10.1145/3448018.3457996</a></td>
            <td>@inproceedings{10.1145/3448018.3457996,<br>  author = {Emery, Kara J and Zannoli, Marina and Warren, James and Xiao, Lei and Talathi, Sachin S},<br>  title = {OpenNEEDS: A Dataset of Gaze, Head, Hand, and Scene Signals During Exploration in Open-Ended VR Environments},<br>  year = {2021},<br>  isbn = {9781450383455},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi-org.libproxy.abertay.ac.uk/10.1145/3448018.3457996},<br>  doi = {10.1145/3448018.3457996},<br>  booktitle = {ACM Symposium on Eye Tracking Research and Applications},<br>  articleno = {11},<br>  numpages = {7},<br>  keywords = {virtual reality, gaze estimation, datasets, eye tracking},<br>  location = {Virtual Event, Germany},<br>  series = {ETRA '21 Short Papers}<br>  }</td>
            <td>no</td>
            <td>Custom VR-HMD and Oculus Rift sensors</td>
            <td>mobile</td>
            <td>VR</td>
            <td>90Hz</td>
            <td>128 x 71</td>
            <td>not specified</td>
            <td>median gaze error 1.3\u25e6</td>
            <td>gaze, hand, head, scene</td>
            <td>Email for access</td>
            <td>yes</td>
            <td>yes</td>
            <td>2,086,507 samples</td>
            <td>VR</td>
            <td>Lab</td>
            <td>yes</td>
            <td>no</td>
            <td>Explore indoor and outdoor scenes. Complete tasks with graspable objects e.g., making a sandwich.</td>
            <td>Up to 5 mins.</td>
            <td>-</td>
            <td>standing</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>44</td>
            <td>20 women</td>
            <td>24 men</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td>31.7</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>40 right-handed</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>average of 62.95mm</td>
            <td>completion of tasks</td>
              </tr>
              <tr>
            <td>GEETUP</td>
            <td><a href="https://www.allpsych.uni-giessen.de/GEETUP/">https://www.allpsych.uni-giessen.de/GEETUP/</a></td>
            <td>-</td>
            <td>-</td>
            <td>2020</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Short</td>
            <td>Pedestrians Egocentric Vision: Individual and Collective Analysis</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391378">https://doi.org/10.1145/3379156.3391378</a></td>
            <td>@inproceedings{valsecchi2020pedestrians,<br>  title={Pedestrians egocentric vision: individual and collective analysis},<br>  author={Valsecchi, Matteo and Akbarinia, Arash and Gil-Rodriguez, Raquel and Gegenfurtner, Karl R},<br>  booktitle={ACM Symposium on eye tracking research and applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td>no</td>
            <td>Tobii Glasses 2</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>gaze 100Hz, head-fixed scene video 25Hz</td>
            <td>1920\u00d71080</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>gaze, scene</td>
            <td>Email for access</td>
            <td>yes</td>
            <td>yes</td>
            <td>59.9 hours</td>
            <td>Urban paths</td>
            <td>Wild</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>Walk specific urban routes.</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>walking</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>Overcast sky; on clear days participants were instructed to walk on the shaded side of the street.</td>
            <td>43</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>students</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>-</td>
              </tr>
              <tr>
            <td>psog_nn</td>
            <td><a href="https://github.com/pseudowolfvn/psog_nn/tree/etra2019">https://github.com/pseudowolfvn/psog_nn/tree/etra2019</a></td>
            <td> </td>
            <td> </td>
            <td>2019</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Long</td>
            <td>Power-efficient and shift-robust eye-tracking sensor for portable VR headsets</td>
            <td>2019</td>
            <td><a href="https://doi.org/10.1145/3314111.3319821">https://doi.org/10.1145/3314111.3319821</a></td>
            <td>@inproceedings{katrychuk2019power,<br>  title={Power-efficient and shift-robust eye-tracking sensor for portable vr headsets},<br>  author={Katrychuk, Dmytro and Griffith, Henry K and Komogortsev, Oleg V},<br>  booktitle={Proceedings of the 11th ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  pages={1--8},<br>  year={2019}<br>  }</td>
            <td>no</td>
            <td>Custom: 850<br>  nm IR illuminator, ThorLabs DCC1545M camera, and a hot mirror<br>  and chin-rest from EyeLink 1000. </td>
            <td>static</td>
            <td>screen based</td>
            <td>120 fps</td>
            <td>Camera recorded monocular images 348x640 pixels at 120 fps. Stimulus screen: 1280x1024</td>
            <td>target range of \u00b119.1\u00b0 (horizontal) and \u00b116.7\u00b0 (vertical) degrees of visual angle</td>
            <td>not specified</td>
            <td>timestamp, and corresponding horizontal and vertical gaze estimates expressed in degrees of the visual angle</td>
            <td>raw data</td>
            <td>yes</td>
            <td>yes</td>
            <td> </td>
            <td>yes</td>
            <td>Lab</td>
            <td>not specified</td>
            <td>yes</td>
            <td>Random horizontal and vertical step-stimulus saccade task</td>
            <td>not specified</td>
            <td>500mm away from the chin-rest</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>23</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>-</td>
              </tr>
              <tr>
            <td>T\u00fcEyeQ</td>
            <td><a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JGOCKI">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JGOCKI</a></td>
            <td><a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JGOCKI">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JGOCKI</a></td>
            <td><a href="https://doi.org/10.7910/DVN/JGOCKI">https://doi.org/10.7910/DVN/JGOCKI</a></td>
            <td>2020</td>
            <td>yes</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Predicting Decision-Making during an Intelligence Test via<br>  Semantic Scanpath Comparisons</td>
            <td>2022</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3517031.3529240">https://dl.acm.org/doi/10.1145/3517031.3529240</a></td>
            <td> </td>
            <td>no</td>
            <td>SMI RED250 </td>
            <td>static</td>
            <td>screen based</td>
            <td>25Hz</td>
            <td>Screen 1920\u2009\u00d7\u20091080</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>raw gaze data consisting of x and y coordinates, timestamp, pupil diameter in millimeters</td>
            <td>Eye Movement Data and Annotations, Aggregated eye-tracking features per participant and task</td>
            <td>yes</td>
            <td>yes</td>
            <td>17,010 rows (observations) and 79 columns (features)</td>
            <td>yes</td>
            <td>Lab</td>
            <td>yes SMI 9 point</td>
            <td>no</td>
            <td>CFT 20-R test (IQ test)</td>
            <td>three sessions, each lasting up to 4\u2009hours</td>
            <td>50\u201370\u2009cm</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>no effects from sunlight or other outdoor light; 10 to 50 lux</td>
            <td>315</td>
            <td>217 female</td>
            <td>94 male</td>
            <td>4 not stated</td>
            <td>8 EUR per hour and additionally 15 EUR if they participated in all sessions</td>
            <td>university entrance qualification</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>23.272</td>
            <td>no visual impairment above 3 dioptres</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>no  neurological or psychiatric pre-existing conditions, grades, handedness, parental background, programming experience, usage of technology, email, browser and social media usage, games genres preferred, smoking or drinking.</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>low tracking ratios (&lt;80%), errors in the presentation, incomplete data</td>
              </tr>
              <tr>
            <td>FiWi</td>
            <td><a href="https://www-users.cse.umn.edu/~qzhao/webpage_saliency.html">https://www-users.cse.umn.edu/~qzhao/webpage_saliency.html</a></td>
            <td> </td>
            <td> </td>
            <td>2014</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Predicting image influence on visual saliency distribution: the focal and ambient dichotomy</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391362">https://doi.org/10.1145/3379156.3391362</a></td>
            <td>@inproceedings{le2020predicting,<br>  title={Predicting image influence on visual saliency distribution: the focal and ambient dichotomy},<br>  author={Le Meur, Olivier and Fons, Pierre-Adrien},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td>no</td>
            <td>Eyelink 1000</td>
            <td>static</td>
            <td>screen based</td>
            <td>1000Hz</td>
            <td>1360 \u00d7 768px</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>monocular, fixations</td>
            <td>Fixations</td>
            <td>yes</td>
            <td>not specified</td>
            <td>149 web pages, 11 subjects</td>
            <td>yes</td>
            <td>Lab</td>
            <td>9 point grid</td>
            <td>yes</td>
            <td>Free view an web page images for 5 seconds</td>
            <td>not specified</td>
            <td>60cm</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>dark room</td>
            <td>11</td>
            <td>7 female</td>
            <td>4 male</td>
            <td> </td>
            <td>not specified</td>
            <td>students, experienced internet users</td>
            <td>not specified</td>
            <td>21-25</td>
            <td>not specified</td>
            <td>normal vision or corrective visual<br>  apparatus</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>MIT1003 dataset</td>
            <td><a href="http://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">http://people.csail.mit.edu/tjudd/WherePeopleLook/index.html</a></td>
            <td> </td>
            <td>@InProceedings{Judd_2009,<br>  author  = {Tilke Judd and Krista Ehinger and Fr{\'e}do Durand and Antonio Torralba},<br>  title   = {Learning to Predict Where Humans Look},<br>  booktitle = {IEEE International Conference on Computer Vision (ICCV)},<br>  year  = {2009}<br>  }</td>
            <td>2009</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Predicting image influence on visual saliency distribution: the focal and ambient dichotomy</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391362">https://doi.org/10.1145/3379156.3391362</a></td>
            <td>@inproceedings{le2020predicting,<br>  title={Predicting image influence on visual saliency distribution: the focal and ambient dichotomy},<br>  author={Le Meur, Olivier and Fons, Pierre-Adrien},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td>no</td>
            <td>not specified</td>
            <td>static</td>
            <td>screen based</td>
            <td>not specified</td>
            <td>1280x1024px</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>raw data, fixations</td>
            <td>raw x and y pixel locations of the eye, fixation maps</td>
            <td>yes</td>
            <td>not specified</td>
            <td>1003 stimuli images, 15 subjects</td>
            <td>yes</td>
            <td>Lab</td>
            <td>yes</td>
            <td>yes</td>
            <td>Free view images</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>dark room</td>
            <td>15</td>
            <td> </td>
            <td> </td>
            <td>gender not stated</td>
            <td>not specified</td>
            <td>naive viewers, 2 were researchers on the project</td>
            <td>not specified</td>
            <td>18-35</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>OSIE</td>
            <td><a href="https://www-users.cse.umn.edu/~qzhao/predicting.html">https://www-users.cse.umn.edu/~qzhao/predicting.html</a></td>
            <td> </td>
            <td> </td>
            <td>2014</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Predicting image influence on visual saliency distribution: the focal and ambient dichotomy</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391362">https://doi.org/10.1145/3379156.3391362</a></td>
            <td>@inproceedings{le2020predicting,<br>  title={Predicting image influence on visual saliency distribution: the focal and ambient dichotomy},<br>  author={Le Meur, Olivier and Fons, Pierre-Adrien},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td>no</td>
            <td>Eyelink 1000</td>
            <td>static</td>
            <td>screen based</td>
            <td>2000Hz</td>
            <td>1680 x 1050px</td>
            <td>33.78\u00b0 x 25.38\u00b0</td>
            <td>not specified</td>
            <td>gaze data, right eye only</td>
            <td>not explicitly stated, right eye only</td>
            <td>yes</td>
            <td>not specified</td>
            <td>700 images, 15 participants</td>
            <td>yes</td>
            <td>Lab</td>
            <td>yes, 9 point grid</td>
            <td>yes (chinrest and forehead)</td>
            <td>Two sessions: 300 images, then 400 images, viewed for 3s each. Free viewed.</td>
            <td>not specified</td>
            <td>26 inches</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>15</td>
            <td> </td>
            <td> </td>
            <td>gender not stated</td>
            <td>not specified</td>
            <td>undergraduate and graduate students</td>
            <td>not specified</td>
            <td>18-30</td>
            <td>not specified</td>
            <td>ncorrected normal eyesight</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>MPIIDPEye</td>
            <td><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/visual-privacy/privacy-aware-eye-tracking-using-differential-privacy">https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/visual-privacy/privacy-aware-eye-tracking-using-differential-privacy</a></td>
            <td> </td>
            <td> </td>
            <td>2019</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Long</td>
            <td>Privacy-Aware Eye Tracking Using Differential Privacy</td>
            <td>2019</td>
            <td><a href="https://doi.org/10.1145/3314111.3319915">https://doi.org/10.1145/3314111.3319915</a></td>
            <td>@inproceedings{steil2019privacy,<br>  title={Privacy-aware eye tracking using differential privacy},<br>  author={Steil, Julian and Hagestedt, Inken and Huang, Michael Xuelin and Bulling, Andreas},<br>  booktitle={Proceedings of the 11th ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  pages={1--9},<br>  year={2019}<br>  }</td>
            <td>yes</td>
            <td>Oculus DK2 virtual reality headset, Pupil eye tracking add-on</td>
            <td>mobile</td>
            <td>VR</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>eye tracking data and the corresponding eye movement features</td>
            <td>eye tracking data and the corresponding eye movement features</td>
            <td>yes</td>
            <td>not specified</td>
            <td>20 participants, three separate recording sessions, 1 documents each time</td>
            <td>yes</td>
            <td>Lab</td>
            <td>no</td>
            <td>no</td>
            <td>three separate recording sessions, reading 1 document each time (comic, online newspaper, and textbook)</td>
            <td>10 mins per session. 3 session - around 30 mins total</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>20</td>
            <td>10 female</td>
            <td>10 male</td>
            <td> </td>
            <td>not specified</td>
            <td>BSc and MSc students</td>
            <td>Various e.g.,  language science, psychology, business administration, computer science</td>
            <td>21-45</td>
            <td>not specified</td>
            <td>normal or corrected-to-normal vision (contact lenses)</td>
            <td>Different countries e.g. India, Pakistan, Germany, Italy</td>
            <td>no, or only minor experience</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>Reading Strategies in Graph Visualizations that Wrap Around in Torus Topology</td>
            <td><a href="https://osf.io/au3bj/">https://osf.io/au3bj/</a></td>
            <td> </td>
            <td> </td>
            <td>2023</td>
            <td>yes</td>
            <td>Yes</td>
            <td>adjunct</td>
            <td>Reading Strategies for Graph Visualizations that Wrap Around in<br>  Torus Topology</td>
            <td>2023</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3588015.3589841">https://dl.acm.org/doi/10.1145/3588015.3589841</a></td>
            <td> </td>
            <td>no</td>
            <td>Tobii Pro X3-120</td>
            <td>static</td>
            <td>screen</td>
            <td>120Hz</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>gaze data, raw data</td>
            <td>gaze data, raw data</td>
            <td>yes</td>
            <td>yes</td>
            <td>not specified</td>
            <td>yes</td>
            <td>Lab</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>view a series of graphs</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>13</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>threshold cutt-off of 70% gaze samples detected with at least one eye</td>
              </tr>
              <tr>
            <td>Dwell Selection with ML-based Intent Prediction Using Only Gaze Data</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3550301">https://dl.acm.org/doi/10.1145/3550301</a></td>
            <td> </td>
            <td> </td>
            <td>2022</td>
            <td>yes</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Reanalyzing Effective Eye-related Information for Developing<br>  User\u2019s Intent Detection Systems<br>  </td>
            <td>2023</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3588015.3589508">https://dl.acm.org/doi/10.1145/3588015.3589508</a></td>
            <td> </td>
            <td>no</td>
            <td>Various Tobii devices</td>
            <td>static</td>
            <td>screen</td>
            <td>Tobii Pro Spectrum at 1200Hz, Tobii Pro Fusion at 250 Hz, Tobii Pro Spectrum at 120Hz</td>
            <td>1980\u00d71080 px</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>gaze data</td>
            <td>Supplemental movie, appendix, image and software files for, Dwell Selection with ML-based Intent Prediction Using Only Gaze Data</td>
            <td>yes</td>
            <td>not specified</td>
            <td>52MB</td>
            <td>yes</td>
            <td>Lab</td>
            <td>yes</td>
            <td>not specified</td>
            <td>5 tasks: word, sentence, image, move, intent labelling</td>
            <td>approx. 68 mins total</td>
            <td>65cm</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>810 lux</td>
            <td>24</td>
            <td>5 females</td>
            <td>19 males</td>
            <td> </td>
            <td>JPY 5,000 (\u223cUSD 45)</td>
            <td>university students</td>
            <td>not specified</td>
            <td>21-26</td>
            <td>22.9</td>
            <td>non specified</td>
            <td>Japanese</td>
            <td>15 had previously participated in an eye tracking experiment</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>Mentioned that "two different eye-trackers was that some participants could not calibrate the Tobii Pro Fusion, apparently because of the incompatibility of pupil detection for Asians"</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>The CRITT TPR-DB 1.0: </td>
            <td><a href="https://aclanthology.org/2012.amta-wptp.1/">https://aclanthology.org/2012.amta-wptp.1/</a></td>
            <td> </td>
            <td> </td>
            <td>2012</td>
            <td>yes</td>
            <td>No</td>
            <td>Full Paper</td>
            <td>Recognition of translator expertise using sequences of fixations and keystrokes</td>
            <td>2014</td>
            <td><a href="https://dl.acm.org/doi/10.1145/2578153.2578201">https://dl.acm.org/doi/10.1145/2578153.2578201</a></td>
            <td>@inproceedings{10.1145/2578153.2578201,<br> author = {Mart\'{\i}nez-G\'{o}mez, Pascual and Minocha, Akshay and Huang, Jin and Carl, Michael and Bangalore, Srinivas and Aizawa, Akiko},<br> title = {Recognition of Translator Expertise Using Sequences of Fixations and Keystrokes},<br> year = {2014},<br> isbn = {9781450327510},<br> publisher = {Association for Computing Machinery},<br> address = {New York, NY, USA},<br> url = {https://doi.org/10.1145/2578153.2578201},<br> doi = {10.1145/2578153.2578201},<br> booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},<br> pages = {299\u2013302},<br> numpages = {4},<br> keywords = {expertise modeling, translation process research},<br> location = {Safety Harbor, Florida},<br> series = {ETRA '14}<br> }</td>
            <td>no</td>
            <td>Various Tobii devices (brackets related to studies in dataset) - Tobii<br>  eyetracker 1750 (BD08, ACS08, KTHJ09 and<br>  LWB09), Tobii T120 (TPR11, BML12, MS12,<br>  NJ12) and Tobii TX300 for SG12</td>
            <td>static</td>
            <td>screen</td>
            <td>various</td>
            <td>various</td>
            <td>various</td>
            <td>various</td>
            <td>gaze data</td>
            <td>raw logging and aligned data</td>
            <td>yes</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
            <td>various (not stated in paper)</td>
              </tr>
              <tr>
            <td>Steil et al 2018</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3229434.3229439">https://dl.acm.org/doi/10.1145/3229434.3229439</a></td>
            <td> </td>
            <td><a href="https://doi.org/10.18419/darus-3285">https://doi.org/10.18419/darus-3285</a></td>
            <td>2018</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>Reducing Calibration Drift in Mobile Eye Trackers by Exploiting Mobile Phone Usage</td>
            <td>2019</td>
            <td><a href="https://doi.org/10.1145/3314111.3319918">https://doi.org/10.1145/3314111.3319918</a></td>
            <td>@inproceedings{muller2019reducing,<br>  title={Reducing calibration drift in mobile eye trackers by exploiting mobile phone usage},<br>  author={M{\"u}ller, Philipp and Buschek, Daniel and Huang, Michael Xuelin and Bulling, Andreas},<br>  booktitle={Proceedings of the 11th ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  pages={1--9},<br>  year={2019}<br>  }</td>
            <td>no</td>
            <td>PUPIL head-mounted eye tracker, DUO3D MLX stereo camera mounted to the eye tracker headset<br>  eye tracker</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>not specified</td>
            <td>eye tracker - one eye camera 640\u00d7480 pixels right eye 30FPS, scene camera 1280\u00d7720 pixels 24FPS. Stereo camera 752\u00d7480px 30FPS.</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>gaze data</td>
            <td>gaze data, raw data</td>
            <td>yes</td>
            <td>not specified</td>
            <td>90 hours approx</td>
            <td>yes</td>
            <td>Wild</td>
            <td>PUPIL software calibration</td>
            <td>no</td>
            <td>Interact with mobile device and visit 3 places on campus- (a canteen, a library, and a caf\u00e9). Free to roam campus too.</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>various</td>
            <td>various</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>20</td>
            <td>6 females</td>
            <td>14 males</td>
            <td> </td>
            <td>not specified</td>
            <td>university students</td>
            <td>not specified</td>
            <td>22-31</td>
            <td>not specified</td>
            <td>normal or<br>  corrected-to-normal vision</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>FakeNewsPer- ception dataset</td>
            <td><a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/C1UD2A">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/C1UD2A</a></td>
            <td> </td>
            <td><a href="https://doi.org/10.7910/DVN/C1UD2A">https://doi.org/10.7910/DVN/C1UD2A</a><a href="https://doi.org/10.7910/DVN/C1UD2A"> </a></td>
            <td>2020</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Regressive Saccadic Eye Movements on Fake News</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3517031.3529619">https://doi.org/10.1145/3517031.3529619</a></td>
            <td>@inproceedings{bozkir2022regressive,<br>  title={Regressive saccadic eye movements on fake news},<br>  author={Bozkir, Efe and Kasneci, Gjergji and Utz, Sonja and Kasneci, Enkelejda},<br>  booktitle={2022 Symposium on Eye Tracking Research and Applications},<br>  pages={1--7},<br>  year={2022}<br>  }</td>
            <td>no</td>
            <td>Tobii Spectrum</td>
            <td>static</td>
            <td>screen</td>
            <td>600Hz</td>
            <td>1920 \u00d7 1080px</td>
            <td>not specified</td>
            <td>lower than a mean angular error of 0.50\u00b0</td>
            <td>eye movement events, their durations, type of the event, unique event ID, mean pupil diameters during events, mean values of X and Y locations for fixations and starting, and ending X and Y positions of saccades</td>
            <td>eye movement events, their durations, type of the event, unique event ID, mean pupil diameters during events, mean values of X and Y locations for fixations and starting, and ending X and Y positions of saccades</td>
            <td>yes</td>
            <td>yes</td>
            <td>25 participants with 60 news items</td>
            <td>yes</td>
            <td>Lab</td>
            <td>9 point</td>
            <td>no</td>
            <td>Reading and rating news items</td>
            <td>approx 40 mins</td>
            <td>not specified</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>25</td>
            <td>13 women</td>
            <td>12 men</td>
            <td> </td>
            <td>\u20ac10</td>
            <td>university students</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>25.9</td>
            <td>normal or<br>  corrected-to-normal vision</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>sensory issues</td>
              </tr>
              <tr>
            <td>RIT-Eyes</td>
            <td><a href="https://cs.rit.edu/~cgaplab/RIT-Eyes/">https://cs.rit.edu/~cgaplab/RIT-Eyes/</a></td>
            <td> </td>
            <td> </td>
            <td>2020</td>
            <td>yes</td>
            <td>No</td>
            <td>adjunct</td>
            <td>RIT-Eyes: realistically rendered eye images for eye-tracking applications</td>
            <td>2020</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3379157.3391990">https://dl.acm.org/doi/10.1145/3379157.3391990</a></td>
            <td> </td>
            <td>no</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>synthetic eye image generation platform</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not specified</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
              </tr>
              <tr>
            <td>HARMONIC</td>
            <td><a href="https://arxiv.org/abs/1807.11154">https://arxiv.org/abs/1807.11154</a></td>
            <td> </td>
            <td><a href="https://harp.ri.cmu.edu/harmonic/">https://harp.ri.cmu.edu/harmonic/</a></td>
            <td>2018</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>Semantic Gaze Labeling for Human-Robot Shared Manipulation</td>
            <td>2019</td>
            <td><a href="https://doi.org/10.1145/3314111.3319840">https://doi.org/10.1145/3314111.3319840</a></td>
            <td>@inproceedings{aronson2019semantic,<br>  title={Semantic gaze labeling for human-robot shared manipulation},<br>  author={Aronson, Reuben M and Admoni, Henny},<br>  booktitle={Proceedings of the 11th ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  pages={1--9},<br>  year={2019}<br>  }</td>
            <td>yes</td>
            <td>Pupil Labs Pupil</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>120Hz</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>gaze data</td>
            <td>gaze point calculation, Pupil video files</td>
            <td>yes</td>
            <td>not specified</td>
            <td>480 trials, comprising of 20 trials for 24 participants - approx 5 hours data</td>
            <td>yes</td>
            <td>Lab</td>
            <td>yes</td>
            <td>not specified</td>
            <td>Guide a robot arm</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not applicable</td>
            <td>not specified</td>
            <td>24</td>
            <td>13 female</td>
            <td>11 male</td>
            <td> </td>
            <td>$15 for one and a half hours of their time</td>
            <td>Pittsburgh area</td>
            <td>not specified</td>
            <td>18-45</td>
            <td>not specified</td>
            <td>not applicable</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>novices - screened for prior experience using this robot arm in similar studies</td>
              </tr>
              <tr>
            <td>BinOculaR Image Statistics DATABASE (BORIS)</td>
            <td><a href="https://osf.io/t9qg5/#:~:text=BinOculaR%20Image%20Statistics%20DATABASE,pairs%20and%20estimates%20of%20bi%E2%80%A6">https://osf.io/t9qg5/#:~:text=BinOculaR%20Image%20Statistics%20DATABASE,pairs%20and%20estimates%20of%20bi%E2%80%A6</a></td>
            <td> </td>
            <td> </td>
            <td>2021</td>
            <td>yes</td>
            <td>No</td>
            <td>adjunct</td>
            <td>Solving Parallax Error for 3D Eye Tracking</td>
            <td>2021</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3450341.3458494">https://dl.acm.org/doi/10.1145/3450341.3458494</a></td>
            <td> </td>
            <td>no</td>
            <td>SR Research EyeLink II Eye Tracker</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>250Hz</td>
            <td>Sony XCD-MV6 (30Hz at 640 \u00d7 480)</td>
            <td>not specified</td>
            <td>0.71 \u00b10.14 degrees (mean \u00b1 standard<br>  deviation)</td>
            <td>stereoscopic images synchronized with binocular fixations</td>
            <td>stereoscopic pairs and estimates of binocular disparity in the central 20 degrees of the visual field</td>
            <td>yes</td>
            <td>yes</td>
            <td>not specified</td>
            <td>yes</td>
            <td>Wild</td>
            <td>not explicitly mentioned</td>
            <td>not specified</td>
            <td>Completing everyday activities- Make Sandwich, Play Videogame, Text Edit, Order Coffee, Indoor Walk, Outdoor Walk (nature and urban)</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>various</td>
            <td>not specified</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>3</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>normal or corrected-to-normal (contact lenses)</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>MECO</td>
            <td><a href="https://meco-read.com/">https://meco-read.com/</a></td>
            <td> </td>
            <td> </td>
            <td>2022</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Synthetic predictabilities from large language models explain reading eye movements</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588420">https://doi.org/10.1145/3588015.3588420</a></td>
            <td>@inproceedings{chandra2023synthetic,<br>  title={Synthetic predictabilities from large language models explain reading eye movements},<br>  author={Chandra, Johan and Witzig, Nicholas and Laubrock, Jochen},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--7},<br>  year={2023}<br>  }</td>
            <td>no</td>
            <td>EyeLink Portable Duo, 1000 or 1000+ eye tracker</td>
            <td>static</td>
            <td>screen-based</td>
            <td>various</td>
            <td>various</td>
            <td>various</td>
            <td>various</td>
            <td>eye movement data</td>
            <td>eye movement data</td>
            <td>yes</td>
            <td>not specified</td>
            <td>not explicitly stated</td>
            <td>not specified</td>
            <td>lab</td>
            <td>9 targets</td>
            <td>yes</td>
            <td>participants read a set of 12 texts in their first and dominant language (L1)</td>
            <td>not specified</td>
            <td>various</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>580</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>13 languages: Dutch, English, Estonian, Finnish, German, Greek, Hebrew, Italian, Korean, Norwegian, Russian, Spanish, and Turkish.</td>
            <td>not specified</td>
            <td>various (Hebrew is right to left)</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>Novel</td>
            <td><a href="https://osf.io/bmvrx/">https://osf.io/bmvrx/</a></td>
            <td> </td>
            <td> </td>
            <td>2020</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Synthetic predictabilities from large language models explain reading eye movements</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588420">https://doi.org/10.1145/3588015.3588420</a></td>
            <td>@inproceedings{chandra2023synthetic,<br>  title={Synthetic predictabilities from large language models explain reading eye movements},<br>  author={Chandra, Johan and Witzig, Nicholas and Laubrock, Jochen},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--7},<br>  year={2023}<br>  }</td>
            <td>no</td>
            <td>EyeLink 1000 System</td>
            <td>static</td>
            <td>screen-based</td>
            <td>1000Hz</td>
            <td>1280 x 1024px</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>Eye movements from both eyes</td>
            <td>fixation sequences</td>
            <td>yes</td>
            <td>yes</td>
            <td>37 participants, 45-minute lab sessions and six 30-minute training sessions</td>
            <td>not specified</td>
            <td>lab</td>
            <td>not specified</td>
            <td>yes</td>
            <td>read an excerpt from the German version of the novel \u201cThe Adventure of the Empty House", different manipulated text</td>
            <td>four 45-minute lab sessions and six 30-minute training sessions</td>
            <td>70cm</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>37</td>
            <td>27 females</td>
            <td>10 males</td>
            <td> </td>
            <td>\u20ac70</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>16-39</td>
            <td>not specified</td>
            <td>normal or corrected-to-normal</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>PROVO</td>
            <td><a href="https://osf.io/sjefs/">https://osf.io/sjefs/</a></td>
            <td> </td>
            <td> </td>
            <td>2017</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Synthetic predictabilities from large language models explain reading eye movements</td>
            <td>2023</td>
            <td><a href="https://doi.org/10.1145/3588015.3588420">https://doi.org/10.1145/3588015.3588420</a></td>
            <td>@inproceedings{chandra2023synthetic,<br>  title={Synthetic predictabilities from large language models explain reading eye movements},<br>  author={Chandra, Johan and Witzig, Nicholas and Laubrock, Jochen},<br>  booktitle={Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},<br>  pages={1--7},<br>  year={2023}<br>  }</td>
            <td>no</td>
            <td>SR Research EyeLink 1000 Plus</td>
            <td>static</td>
            <td>screen-based</td>
            <td>1000Hz</td>
            <td>1,600\u2009\u00d7\u2009900</td>
            <td>40\u2009\u00d7\u200924 deg</td>
            <td>not specified</td>
            <td>gaze data, eye movements were recorded from the right eye</td>
            <td>raw data, fixation report</td>
            <td>yes</td>
            <td>yes</td>
            <td>84 participants, 55 texts</td>
            <td>not specified</td>
            <td>lab</td>
            <td>not specified</td>
            <td>chin and forehead rest</td>
            <td>Read a number of short texts</td>
            <td>not specified</td>
            <td>60cm</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>84</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td>course credit through the Psychology Department subject pool</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>20/20 corrected or uncorrected vision</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>EOTT The Eye Of The Typer Dataset</td>
            <td><a href="https://webgazer.cs.brown.edu/data/">https://webgazer.cs.brown.edu/data/</a></td>
            <td> </td>
            <td> </td>
            <td>2018</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Full Paper</td>
            <td>The eye of the typer: a benchmark and analysis of gaze behavior during typing</td>
            <td>2018</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3204493.3204552">https://dl.acm.org/doi/10.1145/3204493.3204552</a></td>
            <td>@inproceedings{10.1145/3204493.3204552,<br>  author = {Papoutsaki, Alexandra and Gokaslan, Aaron and Tompkin, James and He, Yuze and Huang, Jeff},<br>  title = {The Eye of the Typer: A Benchmark and Analysis of Gaze Behavior during Typing},<br>  year = {2018},<br>  isbn = {9781450357067},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3204493.3204552},<br>  doi = {10.1145/3204493.3204552},<br>  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \&amp; Applications},<br>  articleno = {16},<br>  numpages = {9},<br>  keywords = {webcams, typing, user behavior, eye tracking},<br>  location = {Warsaw, Poland},<br>  series = {ETRA '18}<br>  }</td>
            <td>no</td>
            <td>Tobii Pro X3-120</td>
            <td>static</td>
            <td>screen-based</td>
            <td>120 Hz</td>
            <td>1440 \u00d7 900 or 1920\u00d71200 pixels.</td>
            <td>not specified</td>
            <td>0.4\u25e6</td>
            <td>user input data, screen recordings, webcam video of the participant's face, and eye tracking positions</td>
            <td>user input data (such as mouse and cursor logs), screen recordings, webcam videos of the participants' faces, eye-gaze locations as predicted by a Tobii Pro X3-120 eye tracker, demographic information, and information about the lighting conditions</td>
            <td>yes</td>
            <td>not specified</td>
            <td>51 participants, Fitts Law study, as well as reading, Web search, and typing tasks</td>
            <td>yes (Logitech Full HD Webcam C92)</td>
            <td>lab</td>
            <td>9 point</td>
            <td>no</td>
            <td>Fitts Law study, as well as reading, Web search, and typing tasks</td>
            <td>21 mins on average</td>
            <td>not specified</td>
            <td>sitting</td>
            <td>various (recorded in dataset)</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>51</td>
            <td>not explicitly stated</td>
            <td>not explicitly stated</td>
            <td> </td>
            <td>$20</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>21-58</td>
            <td>27.04</td>
            <td>26 had normal vision, 19 wore eye glasses, and 6 wore contact lenses</td>
            <td>American Indian or Alaska Native, Asian, Black or African American, White, or Other. Skin color - matched a color bar obtained from Ho and Robinson [2015] to the color of the inside part of their upper arm</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>asked as demographic question</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>Observer noted if they were touch typist or had facial hair</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>issues<br>  with the Tobii Pro X3-120 or the screen recording, or interruptions<br>  throughout the study by the participant.</td>
              </tr>
              <tr>
            <td>TEyeDS</td>
            <td><a href="https://arxiv.org/abs/2102.02115">https://arxiv.org/abs/2102.02115</a></td>
            <td>ftp://nephrit.cs.uni-tuebingen.de)</td>
            <td> </td>
            <td>2021</td>
            <td>yes</td>
            <td>No</td>
            <td>adjunct</td>
            <td>The Tiny Eye Movement Transformer<br>  </td>
            <td>2023</td>
            <td><a href="https://dl.acm.org/doi/10.1145/3588015.3590114">https://dl.acm.org/doi/10.1145/3588015.3590114</a></td>
            <td> </td>
            <td>no</td>
            <td>seven different head-mounted eye trackers.</td>
            <td>various</td>
            <td>various</td>
            <td>25-200Hz (various)</td>
            <td>various</td>
            <td>not explicitly stated in paper</td>
            <td>not specified</td>
            <td>Over 20 million real-world eye images with Pupil, Eyelid, and Iris 2D<br>  and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector, and<br>  Eye Movement Types</td>
            <td>Over 20 million real-world eye images with Pupil, Eyelid, and Iris 2D<br>  and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector, and<br>  Eye Movement Types</td>
            <td>yes</td>
            <td>not explicitly stated in paper</td>
            <td>&gt;20 million data points</td>
            <td>not explicitly stated in paper</td>
            <td>various</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>various</td>
            <td>various</td>
            <td>various</td>
            <td>various</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
            <td>not explicitly stated in paper</td>
              </tr>
              <tr>
            <td>MPIIGaze</td>
            <td><a href="https://paperswithcode.com/dataset/mpiigaze">https://paperswithcode.com/dataset/mpiigaze</a></td>
            <td> </td>
            <td>@InProceedings{Zhang_2015_CVPR,<br>  author = {Zhang, Xucong and Sugano, Yusuke and Fritz, Mario and Bulling, Andreas},<br>  title = {Appearance-Based Gaze Estimation in the Wild},<br>  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>  month = {June},<br>  year = {2015}<br>  }</td>
            <td>2015</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Towards efficient calibration for webcam eye-tracking in online experiments</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3517031.3529645">https://doi.org/10.1145/3517031.3529645</a></td>
            <td>@inproceedings{saxena2022towards,<br>  title={Towards efficient calibration for webcam eye-tracking in online experiments},<br>  author={Saxena, Shreshth and Lange, Elke and Fink, Lauren},<br>  booktitle={2022 Symposium on Eye Tracking Research and Applications},<br>  pages={1--7},<br>  year={2022}<br>  }</td>
            <td>no</td>
            <td>laptop camera</td>
            <td>static</td>
            <td>screen-based</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>not applicable</td>
            <td>images</td>
            <td>images</td>
            <td>yes</td>
            <td>not specified</td>
            <td>213,659 images, 15 participants</td>
            <td>no</td>
            <td>wild</td>
            <td>yes</td>
            <td>no</td>
            <td>use a laptop normally over 3 months</td>
            <td>3 months</td>
            <td>not applicable</td>
            <td>not specified</td>
            <td>various</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>15</td>
            <td>6 female</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>21-35</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not applicable</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>N/A</td>
            <td>Email for access</td>
            <td> </td>
            <td> </td>
            <td>2020</td>
            <td>yes, on request</td>
            <td>Yes</td>
            <td>Long</td>
            <td>Towards inferring cognitive state changes from pupil size variations in real world conditions</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379155.3391319">https://doi.org/10.1145/3379155.3391319</a></td>
            <td>@inproceedings{medathati2020towards,<br>  title={Towards inferring cognitive state changes from pupil size variations in real world conditions},<br>  author={Medathati, Naga Venkata Kartheek and Desai, Ruta and Hillis, James},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--10},<br>  year={2020}<br>  }</td>
            <td>no</td>
            <td>Tobii Pro Glasses 2</td>
            <td>static</td>
            <td>wearable</td>
            <td>100Hz</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>gaze and pupillometric<br>  data</td>
            <td>not specified</td>
            <td>yes</td>
            <td>not explicitly stated in paper</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>lab</td>
            <td>yes</td>
            <td>not specified</td>
            <td>four types of visual search tasks</td>
            <td>not specified</td>
            <td>100-200cm</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>25</td>
            <td>11 females</td>
            <td>13 males</td>
            <td>1 non-binary</td>
            <td>not specified</td>
            <td>no prior<br>  training related to the task</td>
            <td>not specified</td>
            <td>18-55</td>
            <td>not specified</td>
            <td>normal or corrected-to-norma</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>SB-SAT</td>
            <td><a href="https://github.com/ahnchive/SB-SAT">https://github.com/ahnchive/SB-SAT</a></td>
            <td><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0178501">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0178501</a></td>
            <td> </td>
            <td>2020</td>
            <td>yes</td>
            <td>Yes</td>
            <td>Short</td>
            <td>Towards Predicting Reading Comprehension From Gaze Behavior</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379156.3391335">https://doi.org/10.1145/3379156.3391335</a></td>
            <td>@inproceedings{ahn2020towards,<br>  title={Towards predicting reading comprehension from gaze behavior},<br>  author={Ahn, Seoyoung and Kelton, Conor and Balasubramanian, Aruna and Zelinsky, Greg},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--5},<br>  year={2020}<br>  }</td>
            <td>no</td>
            <td>EyeLink 1000</td>
            <td>static</td>
            <td>screen-based</td>
            <td>1000Hz</td>
            <td>1024 \u00d7 768px</td>
            <td>30\u25e6 \u00d722\u25e6</td>
            <td>average error &lt; .9\u25e6, maximum error &lt; 1.5\u25e6</td>
            <td>raw eye-movemenet data</td>
            <td>raw eye-movemenet data (csv)</td>
            <td>yes</td>
            <td>not explicitly stated in paper</td>
            <td>95 participants, 4 passages to read</td>
            <td>not specified</td>
            <td>lab</td>
            <td>yes</td>
            <td>not specified</td>
            <td>reading passages of text</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>95</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>undergraduate students</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>U-HAR</td>
            <td><a href="https://atreus.informatik.uni-tuebingen.de/seafile/d/978f6631b6b34f7c9139/">https://atreus.informatik.uni-tuebingen.de/seafile/d/978f6631b6b34f7c9139/</a></td>
            <td> </td>
            <td>link does not work</td>
            <td>2022</td>
            <td>link does not work</td>
            <td>Yes</td>
            <td>Long</td>
            <td>U-HAR: A Convolutional Approach to Human Activity Recognition Combining Head and Eye Movements for Context-Aware Smart Glasses</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3530884">https://doi.org/10.1145/3530884</a></td>
            <td>@article{meyer2022u,<br>  title={U-har: A convolutional approach to human activity recognition combining head and eye movements for context-aware smart glasses},<br>  author={Meyer, Johannes and Frank, Adrian and Schlebusch, Thomas and Kasneci, Enkelejda},<br>  journal={Proceedings of the ACM on Human-Computer Interaction},<br>  volume={6},<br>  number={ETRA},<br>  pages={1--19},<br>  year={2022},<br>  publisher={ACM New York, NY, USA}<br>  }</td>
            <td>yes</td>
            <td>Pupil Labs Core</td>
            <td>static and mobile</td>
            <td>wearable</td>
            <td>120Hz</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>eye movement data</td>
            <td>not specified</td>
            <td>yes</td>
            <td>not explicitly stated in paper</td>
            <td>20 participants, 2 part experiment</td>
            <td>not specified</td>
            <td>lab and wild</td>
            <td>yes, single point</td>
            <td>not specified</td>
            <td>Multiple: alk, read, solve, watching video and type on the keyboard, walk, cycle</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>various</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>20</td>
            <td>10 female</td>
            <td>10 male</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>27</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>VUI</td>
            <td><a href="https://doi.org/10.1167/14.3.14">https://doi.org/10.1167/14.3.14</a></td>
            <td><a href="https://data.mendeley.com/datasets/8rj98pp6km/1">https://data.mendeley.com/datasets/8rj98pp6km/1</a></td>
            <td> </td>
            <td>2014</td>
            <td>yes</td>
            <td>No</td>
            <td>Long</td>
            <td>Visual Search Target Inference in Natural Interaction Settings with Machine Learning</td>
            <td>2020</td>
            <td><a href="https://doi.org/10.1145/3379155.3391314">https://doi.org/10.1145/3379155.3391314</a></td>
            <td>@inproceedings{barz2020visual,<br>  title={Visual search target inference in natural interaction settings with machine learning},<br>  author={Barz, Michael and Stauden, Sven and Sonntag, Daniel},<br>  booktitle={ACM Symposium on Eye Tracking Research and Applications},<br>  pages={1--8},<br>  year={2020}<br>  }</td>
            <td>no</td>
            <td>Eyelink 1000 system</td>
            <td>static</td>
            <td>screen-based</td>
            <td>250Hz</td>
            <td>800 \u00d7 600</td>
            <td>15\u00b0 \u00d7 15\u00b0</td>
            <td>0.5\u00b0 every 80 trials</td>
            <td>eye movements data</td>
            <td>800 Images, observer fixations for three tasks, explicit judgments of saliency for the 800 images</td>
            <td>yes</td>
            <td>not explicitly stated in paper</td>
            <td>100 observers performed an explicit saliency judgment task, 22 observers performed a free viewing task, 20 observers performed a saliency search task, and 38 observers performed a cued object search task.</td>
            <td>not specified</td>
            <td>lab</td>
            <td>yes</td>
            <td>not specified</td>
            <td>explicit saliency judgment task, free viewing task,  saliency search task, cued object search task</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>100 observers performed an explicit saliency judgment task, 22 observers performed a free viewing task, 20 observers performed a saliency search task, and 38 observers performed a cued object search task.</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td>course credit</td>
            <td>University of California, Santa Barbara - undergraduates</td>
            <td>not specified</td>
            <td>18-23</td>
            <td>not specified</td>
            <td>normal or corrected-to-normal vision</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>Bioeye Competition (TEX) and (RAN)</td>
            <td><a href="https://bioeye.cs.txstate.edu/database.php">https://bioeye.cs.txstate.edu/database.php</a></td>
            <td> </td>
            <td> </td>
            <td>2015</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>Visualizing Prediction Correctness of Eye Tracking Classifiers</td>
            <td>2021</td>
            <td><a href="https://doi.org/10.1145/3448018.3457997">https://doi.org/10.1145/3448018.3457997</a></td>
            <td>@inproceedings{10.1145/3448018.3457997,<br>  author = {Prinzler, Martin H.U. and Schr\"{o}der, Christoph and Al Zaidawi, Sahar Mahdie Klim and Zachmann, Gabriel and Maneth, Sebastian},<br>  title = {Visualizing Prediction Correctness of Eye Tracking Classifiers},<br>  year = {2021},<br>  isbn = {9781450383455},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/3448018.3457997},<br>  doi = {10.1145/3448018.3457997},<br>  booktitle = {ACM Symposium on Eye Tracking Research and Applications},<br>  articleno = {10},<br>  numpages = {7},<br>  keywords = {Eye Movement Biometrics, Explainable Artificial Intelligence, Machine Learning, Gaze Point Visualization, User Identification;, Prediction Visualization, Eye Tracking},<br>  location = {Virtual Event, Germany},<br>  series = {ETRA '21 Short Papers}<br>  }</td>
            <td>no</td>
            <td>EyeLink eye-tracker</td>
            <td>static</td>
            <td>screen-based</td>
            <td>1000Hz</td>
            <td>1680\u00d71050</td>
            <td>not specified</td>
            <td>vendor-reported spatial accuracy of 0.5 degrees</td>
            <td>raw eye movement signals</td>
            <td>eye movement data</td>
            <td>yes</td>
            <td>not specified</td>
            <td>First and second sessions- 306 participants, third session - subset of 74 participants</td>
            <td>not specified</td>
            <td>lab</td>
            <td>yes</td>
            <td>yes</td>
            <td>following a dot on screen, reading some text</td>
            <td>1 minute and 40 second for each dot task, 1 min for each reading task</td>
            <td>550mm</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>First and second sessions- 306 participants, third session - subset of 74 participants</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>18-43</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>Voice activity detection from gaze in video mediated communication</td>
            <td><a href="http://medusa.fit.vutbr.cz/TA2/TA2/">http://medusa.fit.vutbr.cz/TA2/TA2/</a></td>
            <td> </td>
            <td>link does not work</td>
            <td>2012</td>
            <td>no</td>
            <td>Yes</td>
            <td>Full Paper</td>
            <td>Voice activity detection from gaze in video mediated communication</td>
            <td>2012</td>
            <td><a href="https://dl.acm.org/doi/10.1145/2168556.2168628">https://dl.acm.org/doi/10.1145/2168556.2168628</a></td>
            <td>@inproceedings{10.1145/2168556.2168628,<br>  author = {Hradis, Michal and Eivazi, Shahram and Bednarik, Roman},<br>  title = {Voice Activity Detection from Gaze in Video Mediated Communication},<br>  year = {2012},<br>  isbn = {9781450312219},<br>  publisher = {Association for Computing Machinery},<br>  address = {New York, NY, USA},<br>  url = {https://doi.org/10.1145/2168556.2168628},<br>  doi = {10.1145/2168556.2168628},<br>  booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},<br>  pages = {329\u2013332},<br>  numpages = {4},<br>  keywords = {machine learning, support vector machines, voice activity detection, gaze tracking, video-mediated communication},<br>  location = {Santa Barbara, California},<br>  series = {ETRA '12}<br>  }</td>
            <td>no</td>
            <td>Tobii X120</td>
            <td>static</td>
            <td>screen-based</td>
            <td>120Hz</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>eye movement data</td>
            <td>not specified</td>
            <td>yes</td>
            <td>not specified</td>
            <td>12 hours and 30 minutes of recorded video-material from each room and 28 gaze recordings with average length of 24 minutes (total 673 minutes).</td>
            <td>  Two rooms: one with Sony HDR-FX1E and another with a Canon HV30</td>
            <td>lab</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>casual discussion to simple problem-solving games</td>
            <td>not specified</td>
            <td>60cm</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not explicitly stated</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td>mostly master or doctoral students at one of the Brno universities and their friends from the same<br>  age group. The language of these sessions was Czech. Additionally, three researchers from other countries joined the recordings<br>  for one extra session which was in conducted then in English</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>EGTEA Gaze+</td>
            <td><a href="https://paperswithcode.com/dataset/egtea">https://paperswithcode.com/dataset/egtea</a></td>
            <td> </td>
            <td>doesn't mention an eye tracker in the paper - only mentions SMI in dataset readme (no details)</td>
            <td>2018</td>
            <td>yes</td>
            <td>No</td>
            <td>Short</td>
            <td>When do Saccades begin? Prediction of Saccades as a Time-to-Event Problem</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3517031.3529627">https://doi.org/10.1145/3517031.3529627</a></td>
            <td>@inproceedings{rolff2022saccades,<br>  title={When do saccades begin? prediction of saccades as a time-to-event problem},<br>  author={Rolff, Tim and Steinicke, Frank and Frintrop, Simone},<br>  booktitle={2022 Symposium on Eye Tracking Research and Applications},<br>  pages={1--7},<br>  year={2022}<br>  }</td>
            <td>no</td>
            <td>SMI wearable eye-tracker</td>
            <td>mobile</td>
            <td>wearable</td>
            <td>30Hz</td>
            <td>1280x960</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>videos and audio,  action annotations include 10325 instances of fine-grained actions, 5,176 hand masks from 13,847 frames</td>
            <td>videos and audio,  action annotations include 10325 instances of fine-grained actions, 5,176 hand masks from 13,847 frames</td>
            <td>yes</td>
            <td>not specified</td>
            <td>28 hours (de-identified) of cooking activities from 86 unique sessions of 32 subjects.</td>
            <td>not specified</td>
            <td>wild</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>cookie activities</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>32</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td> </td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
              <tr>
            <td>BDD-A</td>
            <td><a href="https://paperswithcode.com/dataset/bdd-a">https://paperswithcode.com/dataset/bdd-a</a></td>
            <td> </td>
            <td> </td>
            <td>2017</td>
            <td> </td>
            <td>No</td>
            <td>Long</td>
            <td>Where and What: Driver Attention-based Object Detection</td>
            <td>2022</td>
            <td><a href="https://doi.org/10.1145/3530887">https://doi.org/10.1145/3530887</a></td>
            <td>@article{kasneci2022and,<br>  title={Where and What: Driver Attention-based Object Detection},<br>  author={Kasneci, Enkelejda and Fuhl, Wolfgang and Kassautzki, Naemi-Rebecca and Rong, Yao},<br>  journal={Proceedings of the ACM on Human-Computer Interaction},<br>  volume={6},<br>  number={ETRA},<br>  year={2022},<br>  publisher={ACM}<br>  }</td>
            <td>no</td>
            <td>EyeLink 1000</td>
            <td>static</td>
            <td>screen-based</td>
            <td>1000Hz</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>eye movements data</td>
            <td>videos</td>
            <td>yes</td>
            <td>yes</td>
            <td>45 participants, 200 video clips</td>
            <td>not specified</td>
            <td>lab</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>watching videos of driving</td>
            <td>200 videos x 10 secs each</td>
            <td>not specified</td>
            <td>sitting</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>45</td>
            <td>not specified</td>
            <td>not specified</td>
            <td> </td>
            <td>not specified</td>
            <td>at least 1 year of driving experience</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
            <td>not specified</td>
              </tr>
            </tbody>
            </table>


          <!--end table-->
          


        </div>
      </div>
    </div>
  </div>
</section>
<!-- End table section -->











<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the<a href="https://nerfies.github.io" target="_blank">Nerfies</a>project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
